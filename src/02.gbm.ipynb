{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0341b7",
   "metadata": {},
   "source": [
    "# Description\n",
    "Create a GBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d3a22",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92870ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neon/Documents/lending-club-analysis/lending-club-analysis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "25/06/18 15:12:41 WARN Utils: Your hostname, pop-os-note resolves to a loopback address: 127.0.0.1; using 192.168.0.4 instead (on interface wlp2s0)\n",
      "25/06/18 15:12:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://mmlspark.azureedge.net/maven added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/neon/Documents/lending-club-analysis/lending-club-analysis/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/neon/.ivy2/cache\n",
      "The jars for the packages stored in: /home/neon/.ivy2/jars\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-055f5406-ebd0-413a-84c0-b91869740a6d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#synapseml_2.12;1.0.11 in central\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;1.0.11 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.2.14 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.15 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.5 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.1.3 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.1.3 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.1.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.10 in central\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in central\n",
      "\tfound com.beust#jcommander;1.27 in central\n",
      "\tfound org.scalanlp#breeze_2.12;2.1.0 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;2.1.0 in central\n",
      "\tfound org.typelevel#spire_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-macros_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#algebra_2.12;2.0.1 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.1.1 in central\n",
      "\tfound org.typelevel#spire-platform_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-util_2.12;0.17.0 in central\n",
      "\tfound dev.ludovic.netlib#blas;3.0.1 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound dev.ludovic.netlib#lapack;3.0.1 in central\n",
      "\tfound dev.ludovic.netlib#arpack;3.0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.wendykierp#JTransforms;3.1 in central\n",
      "\tfound pl.edu.icm#JLargeArrays;1.5 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.7.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;1.0.11 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;1.0.11 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.azure#onnx-protobuf_2.12;0.9.3 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;1.0.11 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-sdk;1.24.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;1.0.11 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;9.3.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;1.0.11 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.3.510 in central\n",
      ":: resolution report :: resolve 11915ms :: artifacts dl 564ms\n",
      "\t:: modules in use:\n",
      "\tcom.beust#jcommander;1.27 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.10 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;9.3.0 from central in [default]\n",
      "\tcom.github.wendykierp#JTransforms;3.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 from central in [default]\n",
      "\tcom.microsoft.azure#onnx-protobuf_2.12;0.9.3 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;1.0.11 from central in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-sdk;1.24.1 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.3.510 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdev.ludovic.netlib#arpack;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#blas;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#lapack;3.0.1 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.5 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.1.3 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.1.3 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.15 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.7.0 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.2.14 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;2.1.0 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;2.1.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.typelevel#algebra_2.12;2.0.1 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.1.1 from central in [default]\n",
      "\torg.typelevel#spire-macros_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-platform_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-util_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire_2.12;0.17.0 from central in [default]\n",
      "\tpl.edu.icm#JLargeArrays;1.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.2.0 by [org.scala-lang.modules#scala-collection-compat_2.12;2.7.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.5 by [org.apache.commons#commons-math3;3.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.25] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   55  |   0   |   0   |   4   ||   51  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-055f5406-ebd0-413a-84c0-b91869740a6d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 51 already retrieved (0kB/180ms)\n",
      "25/06/18 15:12:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sql\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Imputer\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from Utils import addWoeFromSavedDF, spark, output_path, input_path, pysparkGiniPerGroups\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e9d40",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014d9701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.parquet(f\"{output_path}train_df_woe.parquet\")\n",
    "test_data = spark.read.parquet(f\"{output_path}test_df_woe.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d47b0",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f300efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 15:13:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "\n",
    "numeric_list = ['loan_amnt',\n",
    " 'funded_amnt',\n",
    " 'funded_amnt_inv',\n",
    " 'int_rate',\n",
    " 'installment',\n",
    " 'annual_inc',\n",
    " 'dti',\n",
    " 'delinq_2yrs',\n",
    " 'fico_range_low',\n",
    " 'fico_range_high',\n",
    " 'inq_last_6mths',\n",
    " 'open_acc',\n",
    " 'pub_rec',\n",
    " 'revol_bal',\n",
    " 'revol_util',\n",
    " 'total_acc']\n",
    "\n",
    "categ_list = ['term',\n",
    " 'grade',\n",
    " 'sub_grade',\n",
    " 'home_ownership',\n",
    " 'verification_status',\n",
    " 'purpose',\n",
    " 'zip_code',\n",
    " 'addr_state',\n",
    " 'initial_list_status']\n",
    "\n",
    "target_col = \"default_flag\"\n",
    "categ_idx_list = [f\"{i}_idx\" for i in categ_list]\n",
    "categ_dummy_list = [f\"{i}_dummy\" for i in categ_list]\n",
    "\n",
    "imp = Imputer(inputCols=numeric_list, outputCols=numeric_list, strategy='mean')\n",
    "stridx = StringIndexer(inputCols=categ_list, outputCols=categ_idx_list, handleInvalid=\"keep\")\n",
    "data_pipeline = Pipeline(stages=[imp, stridx])\n",
    "\n",
    "data_transformer = data_pipeline.fit(train_data)\n",
    "\n",
    "train_data_treated  =  data_transformer.transform(train_data)\n",
    "test_data_treated  =  data_transformer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73df2c",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075cb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_treated\\\n",
    "# .write\\\n",
    "# .mode(\"overwrite\")\\\n",
    "# .save(f\"{output_path}train_data_treated.parquet\")\n",
    "\n",
    "# test_data_treated\\\n",
    "# .write\\\n",
    "# .mode(\"overwrite\")\\\n",
    "# .save(f\"{output_path}test_data_treated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e964c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_treated = spark.read.parquet(f\"{output_path}train_data_treated.parquet\")\n",
    "test_data_treated = spark.read.parquet(f\"{output_path}test_data_treated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def withTrainTestColumn(df: DataFrame, frac: float):\n",
    "#     w1 = Window.orderBy(F.rand(seed=42))\n",
    "#     result = df\\\n",
    "#             .withColumn(\"rand_rank\", F.rank().over(w1) / lit(df.count()))\\\n",
    "#             .withColumn(\"train_test_split\", when(F.col(\"rand_rank\") <= F.lit(frac), \"train\").otherwise(\"test\"))\\\n",
    "#             .drop(\"rand_rank\")\n",
    "#     return result\n",
    "\n",
    "# train_split = train_data_treated\\\n",
    "# .sample(0.1, seed=42)\\\n",
    "# .transform(withTrainTestColumn, 0.6)\n",
    "\n",
    "# def withStringToCateg(df: pd.DataFrame):\n",
    "#     dtypes_dict = df.dtypes.to_dict()\n",
    "#     astype_dict = dict()\n",
    "#     for col_i, type_i in dtypes_dict.items():\n",
    "#         if type_i == \"object\":\n",
    "#             astype_dict[col_i] = \"category\"\n",
    "#     return df.astype(astype_dict)\n",
    "\n",
    "# train_ = train_split.where(F.col(\"train_test_split\")==\"train\").toPandas().pipe(withStringToCateg)\n",
    "# test_  = train_split.where(F.col(\"train_test_split\")==\"test\").toPandas().pipe(withStringToCateg)\n",
    "\n",
    "# Xtrain = train_[numeric_list + categ_list]\n",
    "# ytrain = train_[target_col]\n",
    "\n",
    "# Xtest = test_[numeric_list + categ_list]\n",
    "# ytest = test_[target_col]\n",
    "\n",
    "\n",
    "# import lightgbm as lgbm\n",
    "# from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "# param_grid = {\n",
    "#     \"boosting\" :[\"gbdt\", \"rf\"],\n",
    "#     \"learning_rate\" : [0.01, 0.05, 0.1],\n",
    "#     \"max_depth\" : [-1, 10, 15],\n",
    "#     \"num_leaves\" : [10, 31, 100],\n",
    "#     \"feature_fraction\" : [0.2, 0.5, 1.0],\n",
    "#     \"min_data_in_leaf\" : [1, 20, 30, 40],\n",
    "#     \"min_sum_hessian_in_leaf\" : [0.001],\n",
    "#     \"min_gain_to_split\" : [0.0],\n",
    "#     \"num_iterations\" : [100],\n",
    "#     \"lambda_l1\" : [0.0, 0.01, 0.05],\n",
    "#     \"lambda_l2\" : [0.0, 0.01, 0.05],\n",
    "#     \"bagging_fraction\" : [0.6, 0.8, 1.0],\n",
    "#     \"bin_sample_count\" : [100, 500, 2000, 200000],\n",
    "#     \"bagging_freq\" : [0],\n",
    "# }\n",
    "                 \n",
    "# param_list = ParameterSampler(param_grid, 10, random_state=42)\n",
    "# results = list()\n",
    "# for i, param_i in tqdm(enumerate(param_list), total=len(param_list)):\n",
    "#     model = lgbm.LGBMClassifier(random_state=42, verbosity=-1)\n",
    "#     model = model.set_params(**param_i)\n",
    "#     model.fit(Xtrain, ytrain, \n",
    "#               eval_metric='auc',\n",
    "#               eval_set=[(Xtrain, ytrain),\n",
    "#                         (Xtest, ytest)],\n",
    "#               eval_names=['train', 'test'])\n",
    "#     result_dict = {\n",
    "#         'model_number' : i,\n",
    "#         'model' : model,\n",
    "#         'params' : model.get_params(),\n",
    "#         'auc_train' : model.best_score_['train']['auc'],\n",
    "#         'auc_test' : model.best_score_['test']['auc'],\n",
    "#         'gini_train' : model.best_score_['train']['auc']*2-1,\n",
    "#         'gini_test' : model.best_score_['test']['auc']*2-1\n",
    "#     }\n",
    "#     results.append(result_dict)\n",
    "\n",
    "# result_df = pd.DataFrame(results)\n",
    "# result_df['overfit'] = np.abs(result_df['gini_train'] - result_df['gini_test'])\n",
    "# result_df['score'] = result_df['gini_test'] - result_df['overfit']\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d5750c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperParametersOpt:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 features: list,\n",
    "                 param_grid: dict,\n",
    "                 output_path: str,\n",
    "                 n_samples: int = 50,\n",
    "                 train_test_split_frac: float = 0.5,\n",
    "                 train_data_sample_frac: float = 1.0,\n",
    "                 batch_counter_cache: int = 20,\n",
    "                 random_state: int = 42):\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.n_samples = n_samples\n",
    "        self.output_path = output_path\n",
    "        self.train_data_sample_frac = train_data_sample_frac\n",
    "        self.random_state = random_state\n",
    "        self.train_test_split_frac = train_test_split_frac\n",
    "        self.features = features\n",
    "        self.batch_counter_cache = batch_counter_cache\n",
    "    \n",
    "    def withTrainTestColumn(self, df: DataFrame, frac: float):\n",
    "        w1 = Window.orderBy(F.rand(seed=self.random_state))\n",
    "        \n",
    "        result = df\\\n",
    "                .withColumn(\"rand_rank\", F.rank().over(w1) / lit(df.count()))\\\n",
    "                .withColumn(\"train_test_split\", when(F.col(\"rand_rank\") <= F.lit(frac), \"train\").otherwise(\"test\"))\\\n",
    "                .drop(\"rand_rank\")\n",
    "        return result\n",
    "    \n",
    "    def getTrainTestSplit(self, df: DataFrame):\n",
    "        df\\\n",
    "        .sample(self.train_data_sample_frac, seed=self.random_state)\\\n",
    "        .transform(self.withTrainTestColumn, self.train_test_split_frac)\\\n",
    "        .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(self.output_data_paths['train_data_sampled'])\n",
    "\n",
    "        self.train_split = spark.read.parquet(self.output_data_paths['train_data_sampled'])\n",
    "        self.train_ = self.train_split.where(F.col(\"train_test_split\")==\"train\").repartition(4).cache()\n",
    "    \n",
    "    def pysparkGiniPerGroups(self, df: DataFrame, group_list: list, pred_col: str, target_col: str, weights:str = None):\n",
    "        \"\"\"\n",
    "        Calculate gini by groups from pyspark dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        def pysparkAucPerGroups(df: DataFrame, group_list: list, pred_col: str, target_col: str, weights:str = None):\n",
    "            \"\"\"\n",
    "            Calculate auc by groups from pyspark dataframe\n",
    "            \"\"\"\n",
    "            \n",
    "            def aucFromDf(df: pd.DataFrame, pred_col: str, target_col: str, weights: str = None):\n",
    "                \"\"\"\n",
    "                Calculate auc from pandas dataframe\n",
    "                \"\"\"\n",
    "                from sklearn.metrics import roc_auc_score\n",
    "                pred = df[pred_col]\n",
    "                target = df[target_col]\n",
    "                if weights is None:\n",
    "                    return roc_auc_score(target, pred, sample_weight = weights)\n",
    "                else:\n",
    "                    return roc_auc_score(target, pred, sample_weight = df[weights])\n",
    "            \n",
    "            def aucPerGroups(df: pd.DataFrame, group_list: list, pred_col: str, target_col: str, weights:str=None):\n",
    "                \"\"\"\n",
    "                Calculate auc by groups from pandas dataframe\n",
    "                \"\"\"\n",
    "                result = pd.DataFrame(df.groupby(group_list).apply(aucFromDf, pred_col, target_col, weights, include_groups=True), columns=['auc']).reset_index()\n",
    "                return result\n",
    "            \n",
    "            group_df_list = group_list + [pred_col, target_col]\n",
    "            if weights is None:\n",
    "                agg_column = count(lit(1))\n",
    "            else:\n",
    "                agg_column = sum(col(weights))\n",
    "            df_grouped = df.groupBy(group_df_list).agg(agg_column.alias('count'))\n",
    "\n",
    "            schema = ''\n",
    "            for col_i in group_list:\n",
    "                schema = schema + f'{col_i} {df_grouped.schema[col_i].dataType.simpleString()}, '\n",
    "            schema = schema + 'auc double'\n",
    "            result = df_grouped.groupBy(group_list).applyInPandas(lambda row: aucPerGroups(row, group_list, pred_col, target_col, 'count'), schema=schema)\n",
    "            return result\n",
    "        \n",
    "        auc_df = pysparkAucPerGroups(df, group_list, pred_col, target_col, weights)\n",
    "        result = auc_df.withColumn('gini', col('auc') * 2 - 1)\n",
    "        return result    \n",
    "    \n",
    "    def fit(self, df: DataFrame):\n",
    "        from sklearn.model_selection import ParameterSampler\n",
    "        from functools import reduce\n",
    "\n",
    "        self.output_data_paths = {\n",
    "            'train_data_sampled' : f\"{self.output_path}train_splited.parquet\",\n",
    "            'pred_data' : f\"{self.output_path}lgbm_hp_opt_preds.parquet\",\n",
    "            'score_data' : f\"{self.output_path}lgbm_hp_opt_scores.parquet\"\n",
    "        }\n",
    "        self.getTrainTestSplit(df)\n",
    "        param_list = ParameterSampler(self.param_grid, self.n_samples, random_state=self.random_state)\n",
    "        self.result_models = dict()\n",
    "        \n",
    "        vectoriser = VectorAssembler(inputCols=self.features, outputCol=\"features\")\n",
    "        self.model_pipeline = Pipeline(stages=[vectoriser, self.model])\n",
    "        \n",
    "        for i, param_i in tqdm(enumerate(param_list), total=len(param_list)):\n",
    "            self.model_pipeline.getStages()[-1].setParams(**param_i)    \n",
    "            lgbm_model = self.model_pipeline.fit(self.train_)\n",
    "            self.result_models[i] = {'model' : lgbm_model, 'params': param_i}\n",
    "            prob_col = lgbm_model.stages[-1].getProbabilityCol()\n",
    "            pred_df_temp = self.train_split\\\n",
    "                                .transform(lgbm_model.transform)\\\n",
    "                                .withColumn(\"pred\", F.round(F.get(vector_to_array(F.col(prob_col)), 1), 2))\\\n",
    "                                .select(\n",
    "                                    \"train_test_split\",\n",
    "                                    lit(i).alias(\"model\"),\n",
    "                                    \"default_flag\",\n",
    "                                    \"pred\"\n",
    "                                )\n",
    "                                \n",
    "            if i == 0:\n",
    "                pred_df_temp\\\n",
    "                    .write.mode(\"overwrite\").save(self.output_data_paths['pred_data'])\n",
    "                batch_counter = 1\n",
    "                pred_dfs = []\n",
    "            else:\n",
    "                if ((batch_counter >= self.batch_counter_cache)\n",
    "                    or (i >= len(param_list) - 1)):\n",
    "                    pred_dfs.append(pred_df_temp)\n",
    "                    pred_df = reduce(DataFrame.unionByName, pred_dfs)\n",
    "                    pred_df\\\n",
    "                    .write.mode(\"append\").save(self.output_data_paths['pred_data'])\n",
    "                    \n",
    "                    batch_counter = 1\n",
    "                    pred_dfs = []\n",
    "                else:\n",
    "                    pred_dfs.append(pred_df_temp)\n",
    "                    batch_counter = batch_counter + 1\n",
    "        \n",
    "        self.pred_df = spark.read.parquet(self.output_data_paths['pred_data'])\n",
    "        self.pred_df\\\n",
    "            .transform(self.pysparkGiniPerGroups, ['train_test_split', 'model'], \"pred\", \"default_flag\")\\\n",
    "            .groupBy(\"model\")\\\n",
    "            .pivot(\"train_test_split\")\\\n",
    "            .agg(sum(\"gini\"))\\\n",
    "            .withColumn(\"score\", F.col(\"test\") - F.abs(F.col(\"train\") - F.col(\"test\")))\\\n",
    "            .withColumn(\"max_score\", max(\"score\").over(Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)))\\\n",
    "            .write.mode(\"overwrite\").save(self.output_data_paths['score_data'])\n",
    "        \n",
    "        self.gini_df = spark.read.parquet(self.output_data_paths['score_data'])\n",
    "        \n",
    "        best_param_i = self.gini_df\\\n",
    "            .where(F.col(\"score\") == F.col(\"max_score\"))\\\n",
    "            .select(collect_list(\"model\"))\\\n",
    "            .first()[0][0]\n",
    "        \n",
    "        self.best_params = self.result_models[best_param_i]['params']\n",
    "        self.model_pipeline.getStages()[-1].setParams(**self.best_params)\n",
    "        \n",
    "        self.best_estimator = self.model_pipeline.fit(df)        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7819a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 11:00:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:00:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:00:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:00:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:00:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Using too small ``bin_construct_sample_cnt`` may encounter unexpected errors and poor accuracy.\n",
      "[LightGBM] [Info] Saving data reference to binary buffer\n",
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:35<05:22, 35.82s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:46<02:47, 20.97s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:54<01:46, 15.26s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:23<01:13, 14.71s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:30<00:48, 12.04s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:37<00:32, 10.67s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:43<00:17,  8.97s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:48<00:07,  7.73s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 11:03:11 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "100%|██████████| 10/10 [03:14<00:00, 19.43s/it]                                 \n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "25/06/18 11:04:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_20539/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "25/06/18 11:04:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 11:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 147:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 809048 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Slgbm = LightGBMClassifier(\n",
    "    objective=\"binary\", \n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_col, \n",
    "    seed=42,\n",
    "    verbosity=-1,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"learningRate\" : [0.01, 0.05, 0.1],\n",
    "    \"maxDepth\" : [5, 10, 15],\n",
    "    \"numLeaves\" : [10, 31, 100],\n",
    "    \"featureFraction\" : [0.2, 0.5, 1.0],\n",
    "    \"minDataInLeaf\" : [1, 20, 30, 40],\n",
    "    \"minSumHessianInLeaf\" : [0.001],\n",
    "    \"minGainToSplit\" : [0.0],\n",
    "    \"numIterations\" : [100],\n",
    "    \"lambdaL1\" : [0.0, 0.01, 0.05],\n",
    "    \"lambdaL2\" : [0.0, 0.01, 0.05],\n",
    "    \"baggingFraction\" : [0.6, 0.8, 1.0],\n",
    "    \"binSampleCount\" : [100, 500, 2000],\n",
    "    \"baggingFreq\" : [0],\n",
    "    \"isUnbalance\": [True, False],\n",
    "}\n",
    "\n",
    "hpopt = hyperParametersOpt(model= Slgbm,\n",
    "                            features= numeric_list + categ_idx_list,\n",
    "                            param_grid=param_grid,\n",
    "                            output_path=output_path,\n",
    "                            n_samples=10,\n",
    "                            train_data_sample_frac=0.1,\n",
    "                            train_test_split_frac=0.6,\n",
    "                            batch_counter_cache=35,\n",
    "                            random_state=42)\n",
    "\n",
    "hpopt = hpopt.fit(train_data_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46269ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|count|\n",
      "+-----+-----+\n",
      "|    1|80665|\n",
      "|    9|80665|\n",
      "|    8|80665|\n",
      "|    3|80665|\n",
      "|    4|80665|\n",
      "|    7|80665|\n",
      "|    6|80665|\n",
      "|    5|80665|\n",
      "|    0|80665|\n",
      "|    2|80665|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hpopt.pred_df.groupBy(\"model\").agg(count(lit(1)).alias(\"count\")).orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0511a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+-------------------+------------------+\n",
      "|model|               test|              train|              score|         max_score|\n",
      "+-----+-------------------+-------------------+-------------------+------------------+\n",
      "|    0| 0.4065421752348233|0.41070614732502486| 0.4023782031446217|0.4023782031446217|\n",
      "|    7| 0.4238337629487665|0.45705438856833025| 0.3906131373292028|0.4023782031446217|\n",
      "|    5| 0.4184148649973487| 0.4495791640553044|  0.387250565939393|0.4023782031446217|\n",
      "|    2| 0.4161594897417187|0.45070340050135504| 0.3816155789820823|0.4023782031446217|\n",
      "|    6|0.41086687556500867|  0.443766415422977|0.37796733570704033|0.4023782031446217|\n",
      "|    8| 0.4199045204569214| 0.5402578033345908|0.29955123757925195|0.4023782031446217|\n",
      "|    3| 0.4168869256785743|  0.542895096415501| 0.2908787549416476|0.4023782031446217|\n",
      "|    9| 0.4224008910539314| 0.5626562209518422|0.28214556115602063|0.4023782031446217|\n",
      "|    4|0.41774840077837583| 0.5548948915316487|0.28060191002510293|0.4023782031446217|\n",
      "|    1|0.41210336690683924| 0.7432476638397383| 0.0809590699739402|0.4023782031446217|\n",
      "+-----+-------------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hpopt.gini_df.orderBy(F.col(\"score\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b5e56f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41210336690683924"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpopt.gini_df.orderBy(F.col(\"score\")).select(collect_list(\"test\")).first()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db25603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': PipelineModel_1a8d4ff06a09,\n",
       " 'params': {'numLeaves': 10,\n",
       "  'numIterations': 100,\n",
       "  'minSumHessianInLeaf': 0.001,\n",
       "  'minGainToSplit': 0.0,\n",
       "  'minDataInLeaf': 20,\n",
       "  'maxDepth': 15,\n",
       "  'learningRate': 0.01,\n",
       "  'lambdaL2': 0.05,\n",
       "  'lambdaL1': 0.0,\n",
       "  'isUnbalance': True,\n",
       "  'featureFraction': 1.0,\n",
       "  'binSampleCount': 2000,\n",
       "  'baggingFreq': 0,\n",
       "  'baggingFraction': 0.6}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpopt.result_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "582a8b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numLeaves': 10,\n",
       " 'numIterations': 100,\n",
       " 'minSumHessianInLeaf': 0.001,\n",
       " 'minGainToSplit': 0.0,\n",
       " 'minDataInLeaf': 20,\n",
       " 'maxDepth': 15,\n",
       " 'learningRate': 0.01,\n",
       " 'lambdaL2': 0.05,\n",
       " 'lambdaL1': 0.0,\n",
       " 'isUnbalance': True,\n",
       " 'featureFraction': 1.0,\n",
       " 'binSampleCount': 2000,\n",
       " 'baggingFreq': 0,\n",
       " 'baggingFraction': 0.6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpopt.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad0a358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Using too small ``bin_construct_sample_cnt`` may encounter unexpected errors and poor accuracy.\n",
      "[LightGBM] [Info] Saving data reference to binary buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1356:>                                                       (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Loaded reference dataset: 25 features, 48399 num_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4218/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_4218/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_4218/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "/tmp/ipykernel_4218/1435170631.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-------------------+\n",
      "|model|              test|              train|\n",
      "+-----+------------------+-------------------+\n",
      "|    1|0.4065421752348233|0.41070614732502486|\n",
      "+-----+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Slgbm = LightGBMClassifier(\n",
    "    objective=\"binary\", \n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_col, \n",
    "    seed=42,\n",
    "    verbosity=-1,\n",
    "    deterministic=True\n",
    ")\n",
    "Slgbm.setParams(**hpopt.best_params)\n",
    "vectoriser = VectorAssembler(inputCols=numeric_list + categ_idx_list, outputCol=\"features\")\n",
    "model_pipeline = Pipeline(stages=[vectoriser, Slgbm])\n",
    "prob_col = Slgbm.getProbabilityCol()\n",
    "lgbm_models = model_pipeline.fit(hpopt.train_)\n",
    "pred_df = hpopt.train_split\\\n",
    "        .transform(lgbm_models.transform)\\\n",
    "        .withColumn(\"pred\", F.round(F.get(vector_to_array(F.col(prob_col)), 1), 2))\\\n",
    "        .select(\"train_test_split\",\n",
    "                lit(1).alias(\"model\"),\n",
    "                \"default_flag\",\n",
    "                \"pred\")\n",
    "pred_df\\\n",
    "    .transform(hpopt.pysparkGiniPerGroups,\n",
    "            ['train_test_split', 'model'],\n",
    "             \"pred\",\n",
    "             \"default_flag\" )\\\n",
    "    .groupBy(\"model\")\\\n",
    "    .pivot(\"train_test_split\")\\\n",
    "    .agg(sum(\"gini\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6e023",
   "metadata": {},
   "source": [
    "## Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4718efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 14:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 14:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/18 14:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 2509:============================>                           (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|col_1|rank|\n",
      "+-----+----+\n",
      "|   30|   1|\n",
      "|   20|   2|\n",
      "|   10|   3|\n",
      "+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w1 = Window.orderBy(F.col(\"col_1\").desc())\n",
    "test_df = spark.createDataFrame(\n",
    "    [\n",
    "        [10],\n",
    "        [20],\n",
    "        [30]\n",
    "    ],\n",
    "    schema = ['col_1']\n",
    ")\\\n",
    ".withColumn(\"rank\", rank().over(w1))\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf05253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequentialFeatureElimination:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 features: list,\n",
    "                 params: dict,\n",
    "                 output_path: str = None,\n",
    "                 train_data_sample_frac: float = 0.1,\n",
    "                 train_test_split_frac: float = 0.5,\n",
    "                 random_state: int = 42,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.prob_col = model.getProbabilityCol()\n",
    "        self.features = features\n",
    "        self.train_data_sample_frac = train_data_sample_frac\n",
    "        self.train_test_split_frac = train_test_split_frac\n",
    "        self.random_state = random_state\n",
    "        self.output_path = output_path\n",
    "        self.params = params\n",
    "               \n",
    "    def withTrainTestColumn(self, df: DataFrame, frac: float):\n",
    "        w1 = Window.orderBy(F.rand(seed=self.random_state))\n",
    "        \n",
    "        result = df\\\n",
    "                .withColumn(\"rand_rank\", F.rank().over(w1) / lit(df.count()))\\\n",
    "                .withColumn(\"train_test_split\", when(F.col(\"rand_rank\") <= F.lit(frac), \"train\").otherwise(\"test\"))\\\n",
    "                .drop(\"rand_rank\")\n",
    "        return result\n",
    "    \n",
    "    def getTrainTestSplit(self, df: DataFrame):\n",
    "        df\\\n",
    "        .sample(self.train_data_sample_frac, seed=self.random_state)\\\n",
    "        .transform(self.withTrainTestColumn, self.train_test_split_frac)\\\n",
    "        .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(self.output_data_paths['train_data_sampled'])\n",
    "\n",
    "        self.train_split = spark.read.parquet(self.output_data_paths['train_data_sampled'])\n",
    "        self.train_ = self.train_split.where(F.col(\"train_test_split\")==\"train\").repartition(4).cache()\n",
    "    \n",
    "    def pysparkGiniPerGroups(self, df: DataFrame, group_list: list, pred_col: str, target_col: str, weights:str = None):\n",
    "        \"\"\"\n",
    "        Calculate gini by groups from pyspark dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        def pysparkAucPerGroups(df: DataFrame, group_list: list, pred_col: str, target_col: str, weights:str = None):\n",
    "            \"\"\"\n",
    "            Calculate auc by groups from pyspark dataframe\n",
    "            \"\"\"\n",
    "            \n",
    "            def aucFromDf(df: pd.DataFrame, pred_col: str, target_col: str, weights: str = None):\n",
    "                \"\"\"\n",
    "                Calculate auc from pandas dataframe\n",
    "                \"\"\"\n",
    "                from sklearn.metrics import roc_auc_score\n",
    "                pred = df[pred_col]\n",
    "                target = df[target_col]\n",
    "                if weights is None:\n",
    "                    return roc_auc_score(target, pred, sample_weight = weights)\n",
    "                else:\n",
    "                    return roc_auc_score(target, pred, sample_weight = df[weights])\n",
    "            \n",
    "            def aucPerGroups(df: pd.DataFrame, group_list: list, pred_col: str, target_col: str, weights:str=None):\n",
    "                \"\"\"\n",
    "                Calculate auc by groups from pandas dataframe\n",
    "                \"\"\"\n",
    "                result = pd.DataFrame(df.groupby(group_list).apply(aucFromDf, pred_col, target_col, weights, include_groups=True), columns=['auc']).reset_index()\n",
    "                return result\n",
    "            \n",
    "            group_df_list = group_list + [pred_col, target_col]\n",
    "            if weights is None:\n",
    "                agg_column = count(lit(1))\n",
    "            else:\n",
    "                agg_column = sum(col(weights))\n",
    "            df_grouped = df.groupBy(group_df_list).agg(agg_column.alias('count'))\n",
    "\n",
    "            schema = ''\n",
    "            for col_i in group_list:\n",
    "                schema = schema + f'{col_i} {df_grouped.schema[col_i].dataType.simpleString()}, '\n",
    "            schema = schema + 'auc double'\n",
    "            result = df_grouped.groupBy(group_list).applyInPandas(lambda row: aucPerGroups(row, group_list, pred_col, target_col, 'count'), schema=schema)\n",
    "            return result\n",
    "        \n",
    "        auc_df = pysparkAucPerGroups(df, group_list, pred_col, target_col, weights)\n",
    "        result = auc_df.withColumn('gini', col('auc') * 2 - 1)\n",
    "        return result\n",
    "    \n",
    "    def withPredictions(self, df: DataFrame, fitted_pipe):\n",
    "        result = df\\\n",
    "                .transform(fitted_pipe.transform)\\\n",
    "                .withColumn(\"pred\", F.round(F.get(vector_to_array(F.col(self.prob_col)), 1), 2))\n",
    "        return result\n",
    "    \n",
    "    def getModelPipeline(self, features):\n",
    "        self.vectoriser = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "        model = self.model.copy()\n",
    "        model.setParams(**self.params)\n",
    "        self.model_pipeline = Pipeline(stages=[self.vectoriser, model])\n",
    "    \n",
    "    def getSelectedCols(self):\n",
    "        w1 = Window.partitionBy(\"round\", \"train_test_split\").orderBy(F.col(\"gini\").desc())\n",
    "        w2 = Window.partitionBy(\"train_test_split\").orderBy(F.col(\"gini\").desc())\n",
    "\n",
    "        self.gini_df\\\n",
    "            .withColumn(\"rank\", rank().over(w1))\\\n",
    "            .where(F.col(\"rank\") == 1)\\\n",
    "            .withColumn(\"gini_\", rank().over(w1))\n",
    "     \n",
    "    def fit(self, df: DataFrame):\n",
    "        from IPython.display import clear_output\n",
    "        import time\n",
    "\n",
    "        self.output_data_paths = {\n",
    "            'train_data_sampled' : f\"{self.output_path}train_splited.parquet\",\n",
    "            'pred_data' : f\"{self.output_path}lgbm_hp_opt_preds.parquet\",\n",
    "            'score_data' : f\"{self.output_path}lgbm_hp_opt_scores.parquet\"\n",
    "        }\n",
    "\n",
    "        self.getTrainTestSplit(df)\n",
    "        self.getModelPipeline(self.features)\n",
    "        lgbm_models = self.model_pipeline.fit(self.train_)\n",
    "        gini_df = self.train_split\\\n",
    "                        .transform(self.withPredictions, lgbm_models)\\\n",
    "                        .select(\"train_test_split\",\n",
    "                                lit(\"None\").alias(\"excluded_col\"),\n",
    "                                \"default_flag\",\n",
    "                                \"pred\")\\\n",
    "                        .transform(self.pysparkGiniPerGroups,\n",
    "                                ['train_test_split', 'excluded_col'],\n",
    "                                \"pred\",\n",
    "                                \"default_flag\" )\\\n",
    "                        .withColumn(\"round\", lit(0))\n",
    "        gini_df.write.mode(\"overwrite\").save(f\"{output_path}lgbm_sfe_opt_gini.parquet\")\n",
    "\n",
    "        round_i = 1\n",
    "        result_pred_df = []\n",
    "        input_cols_list = self.features.copy()\n",
    "\n",
    "        while len(input_cols_list) >= 2:\n",
    "            tested_cols = []\n",
    "            for col_i in input_cols_list:\n",
    "                clear_output()\n",
    "                cols_in = [i for i in input_cols_list if i != col_i]\n",
    "                print(f\"round {round_i}: {len(tested_cols)+1} / {len(cols_in)+1}\")\n",
    "                self.getModelPipeline(cols_in)\n",
    "                lgbm_models = self.model_pipeline.fit(self.train_)\n",
    "                pred_df = self.train_split\\\n",
    "                        .transform(self.withPredictions, lgbm_models)\\\n",
    "                        .select(\"train_test_split\",\n",
    "                                lit(col_i).alias(\"excluded_col\"),\n",
    "                                \"default_flag\",\n",
    "                                \"pred\")\n",
    "                result_pred_df.append(pred_df)\n",
    "                tested_cols.append(col_i)\n",
    "                            \n",
    "            pred_df_result = functools.reduce(DataFrame.unionByName, result_pred_df)\n",
    "            result_pred_df = list()\n",
    "\n",
    "            gini_df = pred_df_result\\\n",
    "                        .transform(self.pysparkGiniPerGroups,\n",
    "                                ['train_test_split', 'excluded_col'],\n",
    "                                \"pred\",\n",
    "                                \"default_flag\" )\\\n",
    "                        .withColumn(\"round\", lit(round_i))\n",
    "                \n",
    "            gini_df.write.mode(\"append\").save(f\"{output_path}lgbm_sfe_opt_gini.parquet\")\n",
    "            self.gini_df = spark.read.parquet(f\"{output_path}lgbm_sfe_opt_gini.parquet\")\n",
    "            \n",
    "            excluded_col = self.gini_df\\\n",
    "                            .where((F.col(\"round\") == round_i)\n",
    "                                 & (F.col(\"train_test_split\") == \"test\"))\\\n",
    "                            .orderBy(F.col(\"gini\").desc()).select(collect_list(\"excluded_col\")).first()[0][0]\n",
    "\n",
    "            print(f\"removed: {excluded_col}\")\n",
    "            input_cols_list = [i for i in input_cols_list if i != excluded_col]\n",
    "            round_i = round_i + 1\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa60571",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slgbm = LightGBMClassifier(\n",
    "    objective=\"binary\", \n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_col, \n",
    "    seed=42,\n",
    "    verbosity=-1,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "params = {'numLeaves': 10,\n",
    "          'numIterations': 100,\n",
    "          'minSumHessianInLeaf': 0.001,\n",
    "          'minGainToSplit': 0.0,\n",
    "          'minDataInLeaf': 20,\n",
    "          'maxDepth': 15,\n",
    "          'learningRate': 0.01,\n",
    "          'lambdaL2': 0.05,\n",
    "          'lambdaL1': 0.0,\n",
    "          'isUnbalance': True,\n",
    "          'featureFraction': 1.0,\n",
    "          'binSampleCount': 2000,\n",
    "          'baggingFreq': 0,\n",
    "          'baggingFraction': 0.6}\n",
    "\n",
    "features = numeric_list\n",
    "\n",
    "sfe = sequentialFeatureElimination(\n",
    "    model = Slgbm,\n",
    "    features = features,\n",
    "    params = params,\n",
    "    output_path = output_path,\n",
    "    train_data_sample_frac = 0.1,\n",
    "    train_test_split_frac = 0.5,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "sfe.fit(train_data_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9114d5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_test_split</th>\n",
       "      <th>excluded_col</th>\n",
       "      <th>auc</th>\n",
       "      <th>gini</th>\n",
       "      <th>round</th>\n",
       "      <th>rank</th>\n",
       "      <th>gini_change</th>\n",
       "      <th>gini_change_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.558944</td>\n",
       "      <td>0.117887</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.003107</td>\n",
       "      <td>0.120994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.560497</td>\n",
       "      <td>0.120994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.120992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>None</td>\n",
       "      <td>0.560496</td>\n",
       "      <td>0.120992</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.120992</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_test_split     excluded_col       auc      gini  round  rank  \\\n",
       "0             test  funded_amnt_inv  0.558944  0.117887      2     1   \n",
       "1             test        loan_amnt  0.560497  0.120994      1     1   \n",
       "2             test             None  0.560496  0.120992      0     1   \n",
       "\n",
       "   gini_change  gini_change_next  \n",
       "0    -0.003107          0.120994  \n",
       "1     0.000002          0.120992  \n",
       "2     0.120992               NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = Window.partitionBy(\"round\", \"train_test_split\").orderBy(F.col(\"gini\").desc())\n",
    "w2 = Window.partitionBy(\"train_test_split\").orderBy(F.col(\"round\"))\n",
    "w3 = Window.partitionBy(\"train_test_split\").orderBy(F.col(\"round\").desc()).rowsBetween(1, 6)\n",
    "\n",
    "sfe.gini_df\\\n",
    "    .withColumn(\"rank\", rank().over(w1))\\\n",
    "    .where(F.col(\"rank\") == 1)\\\n",
    "    .where(F.col(\"train_test_split\") == \"test\")\\\n",
    "    .withColumn(\"gini_change\", coalesce(F.col(\"gini\") - lag(F.col(\"gini\"),1).over(w2), F.col(\"gini\")))\\\n",
    "    .withColumn(\"gini_change_next\", sum(\"gini_change\").over(w3))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6cbe42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+------------+----+\n",
      "|train_test_split|   excluded_col|default_flag|pred|\n",
      "+----------------+---------------+------------+----+\n",
      "|           train|fico_range_high|           0| 0.2|\n",
      "|           train|fico_range_high|           0|0.09|\n",
      "|           train|fico_range_high|           0|0.22|\n",
      "|           train|fico_range_high|           0|0.13|\n",
      "|           train|fico_range_high|           1|0.32|\n",
      "|           train|fico_range_high|           0|0.17|\n",
      "|           train|fico_range_high|           0|0.11|\n",
      "|           train|fico_range_high|           0|0.07|\n",
      "|           train|fico_range_high|           1|0.41|\n",
      "|           train|fico_range_high|           0|0.19|\n",
      "|           train|fico_range_high|           0| 0.4|\n",
      "|           train|fico_range_high|           0|0.19|\n",
      "|           train|fico_range_high|           0|0.22|\n",
      "|           train|fico_range_high|           0|0.09|\n",
      "|           train|fico_range_high|           0| 0.1|\n",
      "|           train|fico_range_high|           0|0.38|\n",
      "|           train|fico_range_high|           0|0.03|\n",
      "|           train|fico_range_high|           1|0.38|\n",
      "|           train|fico_range_high|           0|0.21|\n",
      "|           train|fico_range_high|           0|0.06|\n",
      "+----------------+---------------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gini_df = spark.read.parquet(f\"{output_path}lgbm_sfe_opt_preds.parquet\")\n",
    "gini_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
