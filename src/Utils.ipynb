{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import re\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeClassifierBinner:\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 col_to_bin: str,\n",
    "                 target_col: str,\n",
    "                 min_samples_split: float = 0.3,\n",
    "                 max_leaf_nodes: int = 10):\n",
    "        self.col_to_bin = col_to_bin\n",
    "        self.target_col = target_col\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.clf = DecisionTreeClassifier(max_leaf_nodes=self.max_leaf_nodes, random_state=42)\n",
    "        self.data = data\n",
    "        self.x = data[[self.col_to_bin]]\n",
    "        self.y = data[self.target_col]\n",
    "        self.clf.fit(self.x, self.y)\n",
    "        self.univariate_thresholds = self.uniVariableDecisionTreeThresholds()\n",
    "    \n",
    "    def plot(self):\n",
    "        tree.plot_tree(self.clf)\n",
    "    \n",
    "    def uniVariableDecisionTreeThresholds(self):\n",
    "        self.text_tree = tree.export_text(self.clf, feature_names=[self.col_to_bin])\n",
    "        result = str(self.text_tree)\n",
    "        result = result.replace(f\"{self.col_to_bin}\", \"\")\n",
    "        result = re.sub(\"class: [0-9]\", \"\", result)\n",
    "        result = re.sub(\"[0-9., ]\", \"\", result)\n",
    "        result = re.sub(\" +\", \", \", result.strip()).split(\", \")\n",
    "        result = list(set([float(x) for x in result])) + [float(\"-inf\"), float(\"+inf\")]\n",
    "        result.sort()\n",
    "        return result\n",
    "    \n",
    "    def withBands(self, df):\n",
    "        result = df.copy()\n",
    "        result[f'{self.col_to_bin}'] = result[f'{self.col_to_bin}'].fillna(result[f'{self.col_to_bin}'].mean())\n",
    "        result[f'{self.col_to_bin}_bands'] = pd.cut(result[self.col_to_bin], self.univariate_thresholds, labels=False)\n",
    "        return result\n",
    "\n",
    "    def defaultRatesByGroups(self, df):\n",
    "        result = df.groupby(f'{self.col_to_bin}_bands', observed=False)[self.target_col].agg(['count', 'sum'])\n",
    "        result['total_count'] = result['count'].sum()\n",
    "        result['m_rate'] = result['sum']/result['count']\n",
    "        return result.reset_index()\n",
    "\n",
    "    def calculateWoe(self):\n",
    "        grouped_data = self.data\\\n",
    "            .pipe(self.withBands)\\\n",
    "            .pipe(self.defaultRatesByGroups)\n",
    "        grouped_data['non_defaults'] = grouped_data['count'] - grouped_data['sum']\n",
    "        total_defaults = grouped_data['sum'].sum()\n",
    "        total_non_defaults = grouped_data['non_defaults'].sum()\n",
    "        grouped_data['perc_defaults'] = np.clip(grouped_data['sum']/ total_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['perc_non_defaults'] = np.clip(grouped_data['non_defaults']/ total_non_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['woe'] = np.log(grouped_data['perc_defaults'] / grouped_data['perc_non_defaults'])\n",
    "        grouped_data['iv'] = np.sum(-grouped_data['woe'] * (grouped_data['perc_non_defaults'] - grouped_data['perc_defaults']))\n",
    "        self.woe = grouped_data[[f'{self.col_to_bin}_bands', 'woe', 'iv']]\n",
    "    \n",
    "    def addWoeColumn(self, df, na_values=None):\n",
    "        result = df.pipe(self.withBands)\n",
    "        result = result.merge(\n",
    "            self.woe,\n",
    "            how = 'left',\n",
    "            on = f'{self.col_to_bin}_bands'\n",
    "        )\n",
    "        result = result.drop(columns=['iv'])\n",
    "        result = result.rename(columns={'woe':f'{self.col_to_bin}_woe'})\n",
    "        if na_values:\n",
    "            result[f'{self.col_to_bin}_woe'] = result[f'{self.col_to_bin}_woe'].fillna(na_values)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "class categoryClassifierBinner:\n",
    "    def __init__(self,\n",
    "                    data: pd.DataFrame,\n",
    "                    col_to_bin: str,\n",
    "                    target_col: str,\n",
    "                    n_splits:  int = 10,\n",
    "                    n_samples: int = 10000,\n",
    "                    n_groups:  int = 50):\n",
    "        self.n_splits = n_splits\n",
    "        self.col_to_bin = col_to_bin\n",
    "        self.target_col = target_col\n",
    "        self.data = data\n",
    "        self.n_samples = n_samples\n",
    "        self.n_groups = n_groups\n",
    "        self.applyKMeans()\n",
    "        self.calculateWoe()\n",
    "\n",
    "    def applyKMeans(self):\n",
    "        sample = self.data\\\n",
    "        .pipe(self.withDFResampled, self.n_samples, self.n_groups)\\\n",
    "        .pipe(self.defaultRatesByGroups, [self.col_to_bin, 'groups'])\n",
    "\n",
    "        k_fold_sample = pd.pivot_table(sample, values='m_rate', index=self.col_to_bin, columns='groups').reset_index().set_index(self.col_to_bin)\n",
    "        kmeans = KMeans(n_clusters=self.n_splits, random_state=42).fit(k_fold_sample)\n",
    "        k_fold_sample['k_fold_groups'] = kmeans.predict(k_fold_sample)\n",
    "        k_fold_sample = k_fold_sample.reset_index()\n",
    "        self.groups_dict = k_fold_sample[[self.col_to_bin, 'k_fold_groups']].set_index(self.col_to_bin).to_dict()\n",
    "        self.clusters = {}\n",
    "        for group_i in k_fold_sample['k_fold_groups'].drop_duplicates():\n",
    "            self.clusters[group_i] = k_fold_sample[k_fold_sample['k_fold_groups']==group_i][self.col_to_bin].to_list()\n",
    "    \n",
    "    def withDFResampled(self, df, n_samples, n_groups):\n",
    "        sample = df.copy()\n",
    "        sample[self.col_to_bin] = sample[self.col_to_bin].fillna(\"None\")\n",
    "        sample = sample[[self.col_to_bin, self.target_col]].groupby(self.col_to_bin).sample(n_samples, replace=True, random_state=42).reset_index(drop=True)\n",
    "        sample['count'] = sample.groupby(self.col_to_bin)[self.target].expanding().count().reset_index()[self.target_col]\n",
    "        sample['groups'] = pd.cut(sample['count'], n_groups, labels=False)\n",
    "        return sample\n",
    "\n",
    "    def withBands(self, df):\n",
    "        result = df.copy()\n",
    "        result[f'{self.col_to_bin}'] = result[f'{self.col_to_bin}'].fillna('None')\n",
    "        result[f'{self.col_to_bin}_bands'] = result[self.col_to_bin].map(self.groups_dict['k_fold_groups'])\n",
    "        return result\n",
    "    \n",
    "    def defaultRatesByGroups(self, df, col_list: list):\n",
    "        result = df.groupby(col_list, observed=False)[self.target_col].agg(['count', 'sum'])\n",
    "        result['total_count'] = result['count'].sum()\n",
    "        result['m_rate'] = np.clip(result['sum']/result['count'], 0.00001, None)\n",
    "        return result.reset_index()\n",
    "    \n",
    "    def calculateWoe(self):\n",
    "        grouped_data = self.data\\\n",
    "            .pipe(self.withBands)\\\n",
    "            .pipe(self.defaultRatesByGroups, [f\"{self.col_to_bin}_bands\"])\n",
    "        grouped_data['non_defaults'] = grouped_data['count'] - grouped_data['sum']\n",
    "        total_defaults = grouped_data['sum'].sum()\n",
    "        total_non_defaults = grouped_data['non_defaults'].sum()\n",
    "        grouped_data['perc_defaults'] = np.clip(grouped_data['sum']/ total_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['perc_non_defaults'] = np.clip(grouped_data['non_defaults']/ total_non_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['woe'] = np.log(grouped_data['perc_defaults'] / grouped_data['perc_non_defaults'])\n",
    "        grouped_data['iv'] = np.sum(-grouped_data['woe'] * (grouped_data['perc_non_defaults'] - grouped_data['perc_defaults']))\n",
    "        self.woe = grouped_data[[f'{self.col_to_bin}_bands', 'woe', 'iv']]\n",
    "\n",
    "    def addWoeColumn(self, df, na_values=None):\n",
    "        result = df.pipe(self.withBands)\n",
    "        result = result.merge(\n",
    "            self.woe,\n",
    "            how = 'left',\n",
    "            on = f'{self.col_to_bin}_bands'\n",
    "        )\n",
    "        result = result.drop(columns=['iv'])\n",
    "        result = result.rename(columns={'woe':f'{self.col_to_bin}_woe'})\n",
    "        if na_values:\n",
    "            result[f'{self.col_to_bin}_woe'] = result[f'{self.col_to_bin}_woe'].fillna(na_values)\n",
    "        return result\n",
    "\n",
    "\n",
    "class binnerSelector:\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 cols_to_bin: list,\n",
    "                 target_col: str,\n",
    "                 iv_threshold: float = 0.02):\n",
    "        self.data = data\n",
    "        self.cols_to_bin = cols_to_bin\n",
    "        self.cols_dict = self.data[cols_to_bin].dtypes.to_dict()\n",
    "        self.target_col = target_col\n",
    "        self.removed_cols = {}\n",
    "        self.selected_cols = {}\n",
    "        self.runForAllColl()\n",
    "        self.iv_threshold = iv_threshold\n",
    "        self.removeFeaturesWithLowIV()\n",
    "    \n",
    "    def binCol(self, col_i, n_splits):\n",
    "        if self.cols_dict[col_i] in ['float', 'int']:\n",
    "            cat_binner = TreeClassifierBinner(self.data, col_i, self.target_col, max_leaf_nodes=n_splits, min_samples_split=0.01)\n",
    "        else:\n",
    "            cat_binner = categoryClassifierBinner(self.data, col_i, self.target_col, n_splits=n_splits, n_samples=10000, n_groups=50)\n",
    "        return cat_binner\n",
    "\n",
    "    def selectCategBestBin(self, col_i):\n",
    "        results = {}\n",
    "        n_split = 5\n",
    "        max_splits = len(self.data[col_i].drop_duplicates())\n",
    "        if max_splits == 1:\n",
    "            self.removed_cols[col_i] = 'unique value'\n",
    "            return None\n",
    "        if max_splits < 3:\n",
    "            n_split = max_splits\n",
    "            results[max_splits] = self.binCol(col_i, max_splits)\n",
    "            print(f\"No better adjustment found: bins: {n_split}, iv: {results[n_split].woe['iv'][0]}\")\n",
    "            results['best'] = results[n_split]\n",
    "            return results\n",
    "        elif n_split >= max_splits:\n",
    "            n_split = max_splits - 1\n",
    "        print(f\"starting binning for column: {col_i}\")\n",
    "        results[n_split]   = self.binCol(col_i, n_split)\n",
    "        results[n_split+1] = self.binCol(col_i, n_split+1)\n",
    "        results[n_split-1] = self.binCol(col_i, n_split-1)\n",
    "        while results[n_split+1].woe['iv'][0] > results[n_split].woe['iv'][0]:\n",
    "            if n_split + 2 > max_splits:\n",
    "                n_split = n_split + 1\n",
    "                print(\"reached maximun splits\")\n",
    "                break\n",
    "            print(f\"bins: {n_split}, iv: {results[n_split].woe['iv'][0]}\")\n",
    "            print(f\"bins: {n_split+1}, iv: {results[n_split+1].woe['iv'][0]}\")\n",
    "            n_split = n_split + 1\n",
    "            results[n_split+1] = self.binCol(col_i, n_split+1)\n",
    "            if n_split + 3 > max_splits:\n",
    "                n_split = n_split + 1\n",
    "                print(\"reached maximun splits\")\n",
    "                break\n",
    "        while results[n_split-1].woe['iv'][0] > results[n_split].woe['iv'][0]:\n",
    "            if n_split -1 < 3:\n",
    "                print(\"reached minimum splits\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"bins: {n_split}, iv: {results[n_split].woe['iv'][0]}\")\n",
    "                print(f\"bins: {n_split-1}, iv: {results[n_split-1].woe['iv'][0]}\")\n",
    "                n_split = n_split - 1\n",
    "                results[n_split-1] = self.binCol(col_i, n_split-1)\n",
    "        else:\n",
    "            print(f\"No better adjustment found: bins: {n_split}, iv: {results[n_split].woe['iv'][0]}\")\n",
    "            results['best'] = results[n_split]\n",
    "        return results\n",
    "    \n",
    "    def selectMonotonicBestBin(self, col_i):\n",
    "        results = {}\n",
    "        n_split = 5\n",
    "        max_splits = len(self.data[col_i].drop_duplicates())\n",
    "        print(f\"starting binning for column: {col_i}\")\n",
    "        if max_splits == 1:\n",
    "            self.removed_cols[col_i] = 'unique value'\n",
    "            return None\n",
    "        results[n_split]      =  self.binCol(col_i, n_split)\n",
    "        results[n_split + 1]  =  self.binCol(col_i, n_split + 1)\n",
    "        results[n_split - 1]  =  self.binCol(col_i, n_split - 1)\n",
    "        while results[n_split + 1].woe['iv'][0] > results[n_split].woe['iv'][0]:\n",
    "            print(f'bins: {n_split}, iv: {results[n_split].woe['iv'][0]}')\n",
    "            print(f'bins: {n_split+1}, iv: {results[n_split+1].woe['iv'][0]}')\n",
    "            if results[n_split+1].woe['woe'].is_monotonic_decreasing or results[n_split+1].woe['woe'].is_monotonic_increasing:\n",
    "                n_split = n_split + 1\n",
    "                results[n_split + 1] = self.binCol(col_i, n_split + 1)\n",
    "            else:\n",
    "                print(f\"bins: {n_split + 1} has no monotonic behaviour\")\n",
    "                break\n",
    "        while results[n_split-1].woe['iv'][0] > results[n_split].woe['iv'][0] \\\n",
    "        or (results[n_split].woe['woe'].is_monotonic_increasing == False\\\n",
    "            and results[n_split].woe['woe'].is_monotonic_decreasing == False):\n",
    "            if n_split - 1 <=2:\n",
    "                print(\"reached minimum split\")\n",
    "                n_split = n_split -1\n",
    "                break\n",
    "            else:\n",
    "                print(f\"bins: {n_split}, iv: {results[n_split].woe['iv'][0]}\")\n",
    "                print(f\"bins: {n_split-1}, iv: {results[n_split-1].woe['iv'][0]}\")\n",
    "                n_split = n_split-1\n",
    "                results[n_split-1] = self.binCol(col_i, n_split-1)\n",
    "        \n",
    "        print(f\"No better adjustment found: bins: {n_split}, iv: {results[n_split].woe['iv'][0]}\")\n",
    "        results['best'] = results[n_split]\n",
    "        return results\n",
    "    \n",
    "    def runForAllCols(self):\n",
    "        self.results_per_column = {}\n",
    "        for col_i in self.cols_to_bin:\n",
    "            if self.cols_dict[col_i] in ['float', 'int', 'double']:\n",
    "                result = self.selectMonotonicBestBin(col_i)\n",
    "                if result:\n",
    "                    self.results_per_column[col_i] = result\n",
    "            else:\n",
    "                self.results_per_column[col_i] = self.selectCategBestBin(col_i)\n",
    "    \n",
    "    def removeFeaturesWithLowIV(self):\n",
    "        self.ivs = []\n",
    "        for i, col_i in enumerate(self.results_per_column):\n",
    "            self.ivs.append({'variable': col_i, 'iv': self.results_per_column[col_i]['best'].woe['iv'][0]})\n",
    "            if self.results_per_column[col_i]['best'].woe['iv'][0] > self.iv_threshold:\n",
    "                self.selected_cols[col_i] = self.results_per_column[col_i]['best']\n",
    "            else:\n",
    "                self.removed_cols[col_i] = 'low IV'\n",
    "        self.ivs = pd.DataFrame(self.ivs, columns=['variable', 'iv'])\n",
    "        print(f\"selected cols: {len(self.selected_cols.keys())}\")\n",
    "        print(f\"removed cols: {len(self.removed_cols.keys())}\")\n",
    "    \n",
    "    def addWoeColumn(self, df):\n",
    "        result = df.copy()\n",
    "        for col_i, cat_binner_i in self.selected_cols.items():\n",
    "            result = cat_binner_i.addWoeColumn(result)\n",
    "        return result\n",
    "    \n",
    "    def removeHighlyCorrelatedFeatures(self):\n",
    "        self.woe_df = self.addWoeColumn(self.data)[[f\"{col_i}_woe\" for col_i in self.selected_cols.keys()]]\n",
    "        corr = self.woe_df.corr()\n",
    "        self.corr = corr\n",
    "        corr = corr.reset_index().melt(id_vars='index')\n",
    "        highCorr = corr[(corr['index']!=corr['variable'])\n",
    "                        &(np.abs(corr['value']) > 0.7)]\\\n",
    "                        .reset_index(drop=True)\n",
    "        highCorr = highCorr.merge(\n",
    "            self.ivs,\n",
    "            how='left',\n",
    "            on='variable'\n",
    "        ).merge(\n",
    "            self.ivs,\n",
    "            how='left',\n",
    "            right_on='variable',\n",
    "            left_on='index',\n",
    "            suffixes=['_1', '_2']\n",
    "        )\n",
    "        highCorr['dropped_variables'] = np.where(highCorr['iv_1'] < highCorr['iv_2'], highCorr['variable_1'], highCorr['variable_2'])\n",
    "        self.highCorr = highCorr\n",
    "        dropped_cols = highCorr[::2]['dropped_variables'].drop_duplicates()\n",
    "        for col_i in dropped_cols:\n",
    "            self.removed_cols[col_i] = \"high correl\"\n",
    "            del self.selected_cols[col_i]\n",
    "        print(f\"removed {len(dropped_cols)} cols due to high corr\")\n",
    "        print(f\"selected cols: {len(self.selected_cols)}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
