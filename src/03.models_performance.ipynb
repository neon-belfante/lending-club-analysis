{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb319d76",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook aims to compare the models performance accross different segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ceffd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134ea6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sql\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bce3338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/09 13:26:29 WARN Utils: Your hostname, pop-os-note, resolves to a loopback address: 127.0.0.1; using 192.168.0.4 instead (on interface wlp2s0)\n",
      "25/06/09 13:26:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 13:26:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/09 13:26:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/09 13:26:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"Test\")\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8ead37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/\"\n",
    "output_path = \"../outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a936b80",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea8568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.parquet(f\"{output_path}train_df_woe.parquet\")\n",
    "test_data  = spark.read.parquet(f\"{output_path}test_df_woe.parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587505b8",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb701e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = PipelineModel.load(f\"{output_path}_log_reg.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ee0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class pysparkLogRegWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, pyspark_pipeline: PipelineModel):\n",
    "        self.pipeline_model = pyspark_pipeline\n",
    "        self.feature_name_ = self.pipeline_model.stages[0].getInputCols()\n",
    "        self.feature_name_in_ = self.pipeline_model.stages[0].getInputCols()\n",
    "        self.classes_ = np.array([0,1], dtype=int)\n",
    "        self.params = \n",
    "        \n",
    "\n",
    "log_reg = PipelineModel.load(f\"{output_path}_log_reg.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e2481",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "708b28bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selected_cols = \\\n",
    "['funded_amnt_woe',\n",
    " 'term_woe',\n",
    " 'sub_grade_woe',\n",
    " 'home_ownership_woe',\n",
    " 'annual_inc_woe',\n",
    " 'verification_status_woe',\n",
    " 'zip_code_woe',\n",
    " 'dti_woe',\n",
    " 'earliest_cr_line_woe',\n",
    " 'fico_range_low_woe',\n",
    " 'inq_last_6mths_woe',\n",
    " 'revol_util_woe']\n",
    "\n",
    "target_col = \"default_flag\"\n",
    "\n",
    "train_pd = train_data.select(\n",
    "    selected_cols + [target_col]\n",
    ")\\\n",
    ".toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ac8f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidModelError",
     "evalue": "An unknown model type was passed: <class 'pyspark.ml.regression.GeneralizedLinearRegressionModel'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidModelError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinearExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lending-club-analysis/lending-club-analysis/.venv/lib/python3.10/site-packages/shap/explainers/_linear.py:129\u001b[0m, in \u001b[0;36mLinearExplainer.__init__\u001b[0;34m(self, model, masker, link, nsamples, feature_perturbation, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsamples \u001b[38;5;241m=\u001b[39m nsamples\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# extract what we need from the given model object\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept \u001b[38;5;241m=\u001b[39m \u001b[43mLinearExplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# extract the data\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker), (maskers\u001b[38;5;241m.\u001b[39mIndependent, maskers\u001b[38;5;241m.\u001b[39mPartition)):\n",
      "File \u001b[0;32m~/Documents/lending-club-analysis/lending-club-analysis/.venv/lib/python3.10/site-packages/shap/explainers/_linear.py:311\u001b[0m, in \u001b[0;36mLinearExplainer._parse_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    309\u001b[0m         intercept \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidModelError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unknown model type was passed: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coef, intercept\n",
      "\u001b[0;31mInvalidModelError\u001b[0m: An unknown model type was passed: <class 'pyspark.ml.regression.GeneralizedLinearRegressionModel'>"
     ]
    }
   ],
   "source": [
    "explainer = shap.LinearExplainer(log_reg.stages[1], train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7710246",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cecaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class applyShap:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 data):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "    \n",
    "    def explain(self):\n",
    "        try:\n",
    "            self.explainer = shap.TreeExplainer(self.model, feature_perturbation=\"tree_path_dependent\")\n",
    "        except:\n",
    "            self.explainer = shap.LinearExplainer(self.mode, self.data, feature_names = self.model.features_names_in_)\n",
    "        self.shap_values = self.explainer(self.data)\n",
    "    \n",
    "    def calculate_shap_values(self):\n",
    "        self.shap_values = self.explainer(self.data)\n",
    "    \n",
    "    def beeswarm_plot(self, max_display=20, log_scale=False):\n",
    "        try:\n",
    "            shap.plots.beeswarm(self.shap_values, max_display = max_display, log_scale = log_scale)\n",
    "        except:\n",
    "            shap.plots.beeswarm(self.shap_values[:,:,1], max_display = max_display, log_scale = log_scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
