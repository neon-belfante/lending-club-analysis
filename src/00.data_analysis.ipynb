{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516eba87",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook aims to download data from lending club and do a initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d9aac",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b386153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sql\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b7b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/09 09:28:52 WARN Utils: Your hostname, pop-os-note, resolves to a loopback address: 127.0.0.1; using 192.168.0.4 instead (on interface wlp2s0)\n",
      "25/06/09 09:28:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 09:28:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"Test\")\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf65376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e137855",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/\"\n",
    "output_path = \"../outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e889b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b3991",
   "metadata": {},
   "source": [
    "## raw data input\n",
    "We'll read the csv file and save as parquet, being a more efficient data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c721e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_df = spark.read\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"True\")\\\n",
    ".option(\"quote\", '''\"''')\\\n",
    ".option(\"escape\", '''\"''')\\\n",
    ".option(\"inferSchema\", \"True\")\\\n",
    ".load(\"/home/neon/.cache/kagglehub/datasets/wordsforthewise/lending-club/versions/3/accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8cf1a7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- member_id: string (nullable = true)\n",
      " |-- loan_amnt: double (nullable = true)\n",
      " |-- funded_amnt: double (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: double (nullable = true)\n",
      " |-- delinq_2yrs: double (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- fico_range_low: double (nullable = true)\n",
      " |-- fico_range_high: double (nullable = true)\n",
      " |-- inq_last_6mths: double (nullable = true)\n",
      " |-- mths_since_last_delinq: double (nullable = true)\n",
      " |-- mths_since_last_record: double (nullable = true)\n",
      " |-- open_acc: double (nullable = true)\n",
      " |-- pub_rec: double (nullable = true)\n",
      " |-- revol_bal: double (nullable = true)\n",
      " |-- revol_util: double (nullable = true)\n",
      " |-- total_acc: double (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: double (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: double (nullable = true)\n",
      " |-- next_pymnt_d: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- last_fico_range_high: double (nullable = true)\n",
      " |-- last_fico_range_low: double (nullable = true)\n",
      " |-- collections_12_mths_ex_med: double (nullable = true)\n",
      " |-- mths_since_last_major_derog: double (nullable = true)\n",
      " |-- policy_code: double (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- dti_joint: double (nullable = true)\n",
      " |-- verification_status_joint: string (nullable = true)\n",
      " |-- acc_now_delinq: double (nullable = true)\n",
      " |-- tot_coll_amt: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- open_acc_6m: double (nullable = true)\n",
      " |-- open_act_il: double (nullable = true)\n",
      " |-- open_il_12m: double (nullable = true)\n",
      " |-- open_il_24m: double (nullable = true)\n",
      " |-- mths_since_rcnt_il: double (nullable = true)\n",
      " |-- total_bal_il: double (nullable = true)\n",
      " |-- il_util: double (nullable = true)\n",
      " |-- open_rv_12m: double (nullable = true)\n",
      " |-- open_rv_24m: double (nullable = true)\n",
      " |-- max_bal_bc: double (nullable = true)\n",
      " |-- all_util: double (nullable = true)\n",
      " |-- total_rev_hi_lim: double (nullable = true)\n",
      " |-- inq_fi: double (nullable = true)\n",
      " |-- total_cu_tl: double (nullable = true)\n",
      " |-- inq_last_12m: double (nullable = true)\n",
      " |-- acc_open_past_24mths: double (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- bc_open_to_buy: double (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: double (nullable = true)\n",
      " |-- mo_sin_old_il_acct: double (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: double (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: double (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: double (nullable = true)\n",
      " |-- mort_acc: double (nullable = true)\n",
      " |-- mths_since_recent_bc: double (nullable = true)\n",
      " |-- mths_since_recent_bc_dlq: double (nullable = true)\n",
      " |-- mths_since_recent_inq: double (nullable = true)\n",
      " |-- mths_since_recent_revol_delinq: double (nullable = true)\n",
      " |-- num_accts_ever_120_pd: double (nullable = true)\n",
      " |-- num_actv_bc_tl: double (nullable = true)\n",
      " |-- num_actv_rev_tl: double (nullable = true)\n",
      " |-- num_bc_sats: double (nullable = true)\n",
      " |-- num_bc_tl: double (nullable = true)\n",
      " |-- num_il_tl: double (nullable = true)\n",
      " |-- num_op_rev_tl: double (nullable = true)\n",
      " |-- num_rev_accts: double (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: double (nullable = true)\n",
      " |-- num_sats: double (nullable = true)\n",
      " |-- num_tl_120dpd_2m: double (nullable = true)\n",
      " |-- num_tl_30dpd: double (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: double (nullable = true)\n",
      " |-- num_tl_op_past_12m: double (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: double (nullable = true)\n",
      " |-- tax_liens: double (nullable = true)\n",
      " |-- tot_hi_cred_lim: double (nullable = true)\n",
      " |-- total_bal_ex_mort: double (nullable = true)\n",
      " |-- total_bc_limit: double (nullable = true)\n",
      " |-- total_il_high_credit_limit: double (nullable = true)\n",
      " |-- revol_bal_joint: double (nullable = true)\n",
      " |-- sec_app_fico_range_low: double (nullable = true)\n",
      " |-- sec_app_fico_range_high: double (nullable = true)\n",
      " |-- sec_app_earliest_cr_line: string (nullable = true)\n",
      " |-- sec_app_inq_last_6mths: double (nullable = true)\n",
      " |-- sec_app_mort_acc: double (nullable = true)\n",
      " |-- sec_app_open_acc: double (nullable = true)\n",
      " |-- sec_app_revol_util: double (nullable = true)\n",
      " |-- sec_app_open_act_il: double (nullable = true)\n",
      " |-- sec_app_num_rev_accts: double (nullable = true)\n",
      " |-- sec_app_chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- sec_app_collections_12_mths_ex_med: double (nullable = true)\n",
      " |-- sec_app_mths_since_last_major_derog: double (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- hardship_type: string (nullable = true)\n",
      " |-- hardship_reason: string (nullable = true)\n",
      " |-- hardship_status: string (nullable = true)\n",
      " |-- deferral_term: double (nullable = true)\n",
      " |-- hardship_amount: double (nullable = true)\n",
      " |-- hardship_start_date: string (nullable = true)\n",
      " |-- hardship_end_date: string (nullable = true)\n",
      " |-- payment_plan_start_date: string (nullable = true)\n",
      " |-- hardship_length: double (nullable = true)\n",
      " |-- hardship_dpd: double (nullable = true)\n",
      " |-- hardship_loan_status: string (nullable = true)\n",
      " |-- orig_projected_additional_accrued_interest: double (nullable = true)\n",
      " |-- hardship_payoff_balance_amount: double (nullable = true)\n",
      " |-- hardship_last_payment_amount: double (nullable = true)\n",
      " |-- disbursement_method: string (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      " |-- debt_settlement_flag_date: string (nullable = true)\n",
      " |-- settlement_status: string (nullable = true)\n",
      " |-- settlement_date: string (nullable = true)\n",
      " |-- settlement_amount: double (nullable = true)\n",
      " |-- settlement_percentage: double (nullable = true)\n",
      " |-- settlement_term: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc2ff3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+--------+-----------+----------+--------------------+----+------------------+------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+----------------------+----------------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------+------------------+--------------------+-------------------+--------------------------+---------------------------+-----------+----------------+----------------+---------+-------------------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+------------------+------------+-------+-----------+-----------+----------+--------+----------------+------+-----------+------------+--------------------+-----------+--------------+-------+------------------------+-----------+------------------+--------------------+---------------------+--------------+--------+--------------------+------------------------+---------------------+------------------------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------------+----------------+--------------------+---------+---------------+-----------------+--------------+--------------------------+---------------+----------------------+-----------------------+------------------------+----------------------+----------------+----------------+------------------+-------------------+---------------------+--------------------------------+----------------------------------+-----------------------------------+-------------+-------------+---------------+---------------+-------------+---------------+-------------------+-----------------+-----------------------+---------------+------------+--------------------+------------------------------------------+------------------------------+----------------------------+-------------------+--------------------+-------------------------+-----------------+---------------+-----------------+---------------------+---------------+\n",
      "|      id|member_id|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|           emp_title|emp_length|home_ownership|annual_inc|verification_status| issue_d|loan_status|pymnt_plan|                 url|desc|           purpose|             title|zip_code|addr_state|  dti|delinq_2yrs|earliest_cr_line|fico_range_low|fico_range_high|inq_last_6mths|mths_since_last_delinq|mths_since_last_record|open_acc|pub_rec|revol_bal|revol_util|total_acc|initial_list_status|out_prncp|out_prncp_inv|       total_pymnt|total_pymnt_inv|total_rec_prncp|total_rec_int|total_rec_late_fee|recoveries|collection_recovery_fee|last_pymnt_d|last_pymnt_amnt|next_pymnt_d|last_credit_pull_d|last_fico_range_high|last_fico_range_low|collections_12_mths_ex_med|mths_since_last_major_derog|policy_code|application_type|annual_inc_joint|dti_joint|verification_status_joint|acc_now_delinq|tot_coll_amt|tot_cur_bal|open_acc_6m|open_act_il|open_il_12m|open_il_24m|mths_since_rcnt_il|total_bal_il|il_util|open_rv_12m|open_rv_24m|max_bal_bc|all_util|total_rev_hi_lim|inq_fi|total_cu_tl|inq_last_12m|acc_open_past_24mths|avg_cur_bal|bc_open_to_buy|bc_util|chargeoff_within_12_mths|delinq_amnt|mo_sin_old_il_acct|mo_sin_old_rev_tl_op|mo_sin_rcnt_rev_tl_op|mo_sin_rcnt_tl|mort_acc|mths_since_recent_bc|mths_since_recent_bc_dlq|mths_since_recent_inq|mths_since_recent_revol_delinq|num_accts_ever_120_pd|num_actv_bc_tl|num_actv_rev_tl|num_bc_sats|num_bc_tl|num_il_tl|num_op_rev_tl|num_rev_accts|num_rev_tl_bal_gt_0|num_sats|num_tl_120dpd_2m|num_tl_30dpd|num_tl_90g_dpd_24m|num_tl_op_past_12m|pct_tl_nvr_dlq|percent_bc_gt_75|pub_rec_bankruptcies|tax_liens|tot_hi_cred_lim|total_bal_ex_mort|total_bc_limit|total_il_high_credit_limit|revol_bal_joint|sec_app_fico_range_low|sec_app_fico_range_high|sec_app_earliest_cr_line|sec_app_inq_last_6mths|sec_app_mort_acc|sec_app_open_acc|sec_app_revol_util|sec_app_open_act_il|sec_app_num_rev_accts|sec_app_chargeoff_within_12_mths|sec_app_collections_12_mths_ex_med|sec_app_mths_since_last_major_derog|hardship_flag|hardship_type|hardship_reason|hardship_status|deferral_term|hardship_amount|hardship_start_date|hardship_end_date|payment_plan_start_date|hardship_length|hardship_dpd|hardship_loan_status|orig_projected_additional_accrued_interest|hardship_payoff_balance_amount|hardship_last_payment_amount|disbursement_method|debt_settlement_flag|debt_settlement_flag_date|settlement_status|settlement_date|settlement_amount|settlement_percentage|settlement_term|\n",
      "+--------+---------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+--------+-----------+----------+--------------------+----+------------------+------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+----------------------+----------------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------+------------------+--------------------+-------------------+--------------------------+---------------------------+-----------+----------------+----------------+---------+-------------------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+------------------+------------+-------+-----------+-----------+----------+--------+----------------+------+-----------+------------+--------------------+-----------+--------------+-------+------------------------+-----------+------------------+--------------------+---------------------+--------------+--------+--------------------+------------------------+---------------------+------------------------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------------+----------------+--------------------+---------+---------------+-----------------+--------------+--------------------------+---------------+----------------------+-----------------------+------------------------+----------------------+----------------+----------------+------------------+-------------------+---------------------+--------------------------------+----------------------------------+-----------------------------------+-------------+-------------+---------------+---------------+-------------+---------------+-------------------+-----------------+-----------------------+---------------+------------+--------------------+------------------------------------------+------------------------------+----------------------------+-------------------+--------------------+-------------------------+-----------------+---------------+-----------------+---------------------+---------------+\n",
      "|68407277|     NULL|   3600.0|     3600.0|         3600.0| 36 months|   13.99|     123.03|    C|       C4|             leadman| 10+ years|      MORTGAGE|   55000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|debt_consolidation|Debt consolidation|   190xx|        PA| 5.91|        0.0|        Aug-2003|         675.0|          679.0|           1.0|                  30.0|                  NULL|     7.0|    0.0|   2765.0|      29.7|     13.0|                  w|      0.0|          0.0| 4421.723916800001|        4421.72|         3600.0|       821.72|               0.0|       0.0|                    0.0|    Jan-2019|         122.67|        NULL|          Mar-2019|               564.0|              560.0|                       0.0|                       30.0|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|       722.0|   144904.0|        2.0|        2.0|        0.0|        1.0|              21.0|      4981.0|   36.0|        3.0|        3.0|     722.0|    34.0|          9300.0|   3.0|        1.0|         4.0|                 4.0|    20701.0|        1506.0|   37.2|                     0.0|        0.0|             148.0|               128.0|                  3.0|           3.0|     1.0|                 4.0|                    69.0|                  4.0|                          69.0|                  2.0|           2.0|            4.0|        2.0|      5.0|      3.0|          4.0|          9.0|                4.0|     7.0|             0.0|         0.0|               0.0|               3.0|          76.9|             0.0|                 0.0|      0.0|       178050.0|           7746.0|        2400.0|                   13734.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|\n",
      "|68355089|     NULL|  24700.0|    24700.0|        24700.0| 36 months|   11.99|     820.28|    C|       C1|            Engineer| 10+ years|      MORTGAGE|   65000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|    small_business|          Business|   577xx|        SD|16.06|        1.0|        Dec-1999|         715.0|          719.0|           4.0|                   6.0|                  NULL|    22.0|    0.0|  21470.0|      19.2|     38.0|                  w|      0.0|          0.0|          25679.66|       25679.66|        24700.0|       979.66|               0.0|       0.0|                    0.0|    Jun-2016|         926.35|        NULL|          Mar-2019|               699.0|              695.0|                       0.0|                       NULL|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|         0.0|   204396.0|        1.0|        1.0|        0.0|        1.0|              19.0|     18005.0|   73.0|        2.0|        3.0|    6472.0|    29.0|        111800.0|   0.0|        0.0|         6.0|                 4.0|     9733.0|       57830.0|   27.1|                     0.0|        0.0|             113.0|               192.0|                  2.0|           2.0|     4.0|                 2.0|                    NULL|                  0.0|                           6.0|                  0.0|           5.0|            5.0|       13.0|     17.0|      6.0|         20.0|         27.0|                5.0|    22.0|             0.0|         0.0|               0.0|               2.0|          97.4|             7.7|                 0.0|      0.0|       314017.0|          39475.0|       79300.0|                   24667.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|\n",
      "|68341763|     NULL|  20000.0|    20000.0|        20000.0| 60 months|   10.78|     432.66|    B|       B4|        truck driver| 10+ years|      MORTGAGE|   63000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|  home_improvement|              NULL|   605xx|        IL|10.78|        0.0|        Aug-2000|         695.0|          699.0|           0.0|                  NULL|                  NULL|     6.0|    0.0|   7869.0|      56.2|     18.0|                  w|      0.0|          0.0|22705.924293878397|       22705.92|        20000.0|      2705.92|               0.0|       0.0|                    0.0|    Jun-2017|        15813.3|        NULL|          Mar-2019|               704.0|              700.0|                       0.0|                       NULL|        1.0|       Joint App|         71000.0|    13.85|             Not Verified|           0.0|         0.0|   189699.0|        0.0|        1.0|        0.0|        4.0|              19.0|     10827.0|   73.0|        0.0|        2.0|    2081.0|    65.0|         14000.0|   2.0|        5.0|         1.0|                 6.0|    31617.0|        2737.0|   55.9|                     0.0|        0.0|             125.0|               184.0|                 14.0|          14.0|     5.0|               101.0|                    NULL|                 10.0|                          NULL|                  0.0|           2.0|            3.0|        2.0|      4.0|      6.0|          4.0|          7.0|                3.0|     6.0|             0.0|         0.0|               0.0|               0.0|         100.0|            50.0|                 0.0|      0.0|       218418.0|          18696.0|        6200.0|                   14877.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|\n",
      "|66310712|     NULL|  35000.0|    35000.0|        35000.0| 60 months|   14.85|      829.9|    C|       C5|Information Syste...| 10+ years|      MORTGAGE|  110000.0|    Source Verified|Dec-2015|    Current|         n|https://lendingcl...|NULL|debt_consolidation|Debt consolidation|   076xx|        NJ|17.06|        0.0|        Sep-2008|         785.0|          789.0|           0.0|                  NULL|                  NULL|    13.0|    0.0|   7802.0|      11.6|     17.0|                  w| 15897.65|     15897.65|          31464.01|       31464.01|       19102.35|     12361.66|               0.0|       0.0|                    0.0|    Feb-2019|          829.9|    Apr-2019|          Mar-2019|               679.0|              675.0|                       0.0|                       NULL|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|         0.0|   301500.0|        1.0|        1.0|        0.0|        1.0|              23.0|     12609.0|   70.0|        1.0|        1.0|    6987.0|    45.0|         67300.0|   0.0|        1.0|         0.0|                 2.0|    23192.0|       54962.0|   12.1|                     0.0|        0.0|              36.0|                87.0|                  2.0|           2.0|     1.0|                 2.0|                    NULL|                 NULL|                          NULL|                  0.0|           4.0|            5.0|        8.0|     10.0|      2.0|         10.0|         13.0|                5.0|    13.0|             0.0|         0.0|               0.0|               1.0|         100.0|             0.0|                 0.0|      0.0|       381215.0|          52226.0|       62500.0|                   18000.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|\n",
      "|68476807|     NULL|  10400.0|    10400.0|        10400.0| 60 months|   22.45|     289.91|    F|       F1| Contract Specialist|   3 years|      MORTGAGE|  104433.0|    Source Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|    major_purchase|    Major purchase|   174xx|        PA|25.37|        1.0|        Jun-1998|         695.0|          699.0|           3.0|                  12.0|                  NULL|    12.0|    0.0|  21929.0|      64.5|     35.0|                  w|      0.0|          0.0|           11740.5|        11740.5|        10400.0|       1340.5|               0.0|       0.0|                    0.0|    Jul-2016|       10128.96|        NULL|          Mar-2018|               704.0|              700.0|                       0.0|                       NULL|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|         0.0|   331730.0|        1.0|        3.0|        0.0|        3.0|              14.0|     73839.0|   84.0|        4.0|        7.0|    9702.0|    78.0|         34000.0|   2.0|        1.0|         3.0|                10.0|    27644.0|        4567.0|   77.5|                     0.0|        0.0|             128.0|               210.0|                  4.0|           4.0|     6.0|                 4.0|                    12.0|                  1.0|                          12.0|                  0.0|           4.0|            6.0|        5.0|      9.0|     10.0|          7.0|         19.0|                6.0|    12.0|             0.0|         0.0|               0.0|               4.0|          96.6|            60.0|                 0.0|      0.0|       439570.0|          95768.0|       20300.0|                   88097.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|\n",
      "+--------+---------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+--------+-----------+----------+--------------------+----+------------------+------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+----------------------+----------------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------+------------------+--------------------+-------------------+--------------------------+---------------------------+-----------+----------------+----------------+---------+-------------------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+------------------+------------+-------+-----------+-----------+----------+--------+----------------+------+-----------+------------+--------------------+-----------+--------------+-------+------------------------+-----------+------------------+--------------------+---------------------+--------------+--------+--------------------+------------------------+---------------------+------------------------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------------+----------------+--------------------+---------+---------------+-----------------+--------------+--------------------------+---------------+----------------------+-----------------------+------------------------+----------------------+----------------+----------------+------------------+-------------------+---------------------+--------------------------------+----------------------------------+-----------------------------------+-------------+-------------+---------------+---------------+-------------+---------------+-------------------+-----------------+-----------------------+---------------+------------+--------------------+------------------------------------------+------------------------------+----------------------------+-------------------+--------------------+-------------------------+-----------------+---------------+-----------------+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "118e4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         loan_status|  count|\n",
      "+--------------------+-------+\n",
      "|          Fully Paid|1076751|\n",
      "|             Default|     40|\n",
      "|     In Grace Period|   8436|\n",
      "|         Charged Off| 268559|\n",
      "|  Late (31-120 days)|  21467|\n",
      "|             Current| 878317|\n",
      "|   Late (16-30 days)|   4349|\n",
      "|                NULL|     33|\n",
      "|Does not meet the...|   1988|\n",
      "|Does not meet the...|    761|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_df\\\n",
    ".groupBy(\"loan_status\")\\\n",
    ".agg(count(\"id\").alias(\"count\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2360a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_df\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".format(\"parquet\")\\\n",
    ".save(f\"{input_path}accepted_2007_to_2018Q4_2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4405841",
   "metadata": {},
   "source": [
    "## Read parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f0f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(f\"{input_path}accepted_2007_to_2018Q4_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(f\"{input_path}accepted_2007_to_2018Q4_2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88a734",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c39da2",
   "metadata": {},
   "source": [
    "## Default Rate vs DIfferent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charged Off</td>\n",
       "      <td>268559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Current</td>\n",
       "      <td>878317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Default</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does not meet the credit policy. Status:Charge...</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does not meet the credit policy. Status:Fully ...</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>1076751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In Grace Period</td>\n",
       "      <td>8436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Late (16-30 days)</td>\n",
       "      <td>4349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Late (31-120 days)</td>\n",
       "      <td>21467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         loan_status    count\n",
       "0                                               None       33\n",
       "1                                        Charged Off   268559\n",
       "2                                            Current   878317\n",
       "3                                            Default       40\n",
       "4  Does not meet the credit policy. Status:Charge...      761\n",
       "5  Does not meet the credit policy. Status:Fully ...     1988\n",
       "6                                         Fully Paid  1076751\n",
       "7                                    In Grace Period     8436\n",
       "8                                  Late (16-30 days)     4349\n",
       "9                                 Late (31-120 days)    21467"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loan_status_display = data\\\n",
    ".groupBy(\"loan_status\")\\\n",
    ".agg(count(col(\"id\")).alias(\"count\"))\\\n",
    ".orderBy(\"loan_status\")\\\n",
    ".toPandas()\n",
    "\n",
    "loan_status_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b63f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def withTarget(df: DataFrame):\n",
    "    result = df\\\n",
    "    .withColumn(\"default_flag\", \n",
    "                when(col(\"loan_status\").isin([\"Default\", \n",
    "                                              \"Charged Off\", \n",
    "                                              \"Does not meet the credit policy. Status:Charged Off\"]),\n",
    "                                              1)\n",
    "                .when(col(\"loan_status\").isin([\"Fully Paid\",\n",
    "                                               \"Does not meet the credit policy. Status:Fully Paid\"]),\n",
    "                                              0)\n",
    "                .when(col(\"loan_status\").isin([\"Current\",\n",
    "                                               \"In Grace Period\",\n",
    "                                               \"Late (16-30 days)\",\n",
    "                                               \"Late (31-120 days)\"]),\n",
    "                                             -1)\n",
    "                .otherwise(-1))\n",
    "    return result\n",
    "\n",
    "def filterValidTargetValues(df: DataFrame):\n",
    "    result = df\\\n",
    "    .where(col(\"default_flag\").isin([1,0]))\n",
    "    return result\n",
    "\n",
    "def withDefaultRateGrouped(df: DataFrame, group_col: str):\n",
    "    result = df\n",
    "    if data.schema[group_col].dataType.simpleString() in [\"double\", \"int\"]:\n",
    "        quantDisc = QuantileDiscretizer(inputCol=group_col, \n",
    "                                        outputCol=f'{group_col}_bands', \n",
    "                                        numBuckets=10, \n",
    "                                        handleInvalid=\"keep\")\n",
    "        \n",
    "        w1 = Window.partitionBy(f'{group_col}_bands').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "        \n",
    "        result = quantDisc\\\n",
    "                    .fit(result)\\\n",
    "                    .transform(result)\\\n",
    "                    .withColumn(group_col, \n",
    "                                concat(char(col(f'{group_col}_bands') + 97),\n",
    "                                       lit(\". \"),\n",
    "                                       min(col(group_col)).over(w1),\n",
    "                                       lit(\"-\"),\n",
    "                                       max(col(group_col)).over(w1)))\n",
    "    result = result\\\n",
    "    .groupBy(col(group_col).cast(\"string\").alias(\"value\"))\\\n",
    "    .agg((sum(col(\"default_flag\")) / count(col(\"default_flag\"))).alias(\"default_rate\"),\n",
    "         count(col(\"id\")).alias(\"count\"))\\\n",
    "    .withColumn(\"variable\", lit(group_col))\n",
    "    return result\n",
    "\n",
    "def withDefaultRateGroupedMultipleColumns(df: DataFrame, group_cols: list):\n",
    "    counter = 0\n",
    "    for col_i in group_cols:\n",
    "        if counter == 0:\n",
    "            result = df.transform(withDefaultRateGrouped, col_i)\n",
    "            counter = 1\n",
    "        else:\n",
    "            result = result\\\n",
    "            .unionByName(\n",
    "                df.transform(withDefaultRateGrouped, col_i)\n",
    "            )\n",
    "    return result\n",
    "\n",
    "\n",
    "data_target_filtered = data\\\n",
    ".transform(withTarget)\\\n",
    ".transform(filterValidTargetValues)\n",
    "\n",
    "data_chart = data_target_filtered\\\n",
    ".transform(withDefaultRateGroupedMultipleColumns, \n",
    "           [\"grade\",\n",
    "            \"sub_grade\", \n",
    "            \"fico_range_low\",\n",
    "            \"int_rate\",\n",
    "            \"loan_amnt\",\n",
    "            \"emp_length\",\n",
    "            \"home_ownership\",\n",
    "            'annual_inc',\n",
    "            'installment'])\\\n",
    ".toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c28533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chart = data_chart.sort_values([\"variable\", \"value\"])\n",
    "fig1 = px.line(data_chart,\n",
    "              x='value',\n",
    "              y='default_rate',\n",
    "              facet_col=\"variable\",\n",
    "              facet_col_wrap=3,\n",
    "              facet_col_spacing=0.1,\n",
    "              template='none',\n",
    "              color_discrete_sequence=['red'],\n",
    "              category_orders={'variable':data_chart['variable'].drop_duplicates().sort_values()},\n",
    "              width=700)\n",
    "fig1.update_traces(name=\"default_rate\", legendgroup=\"default_rate\")\n",
    "fig1.update_traces(selector={'xaxis':'x'}, showlegend=True)\n",
    "fig1.update_xaxes(matches=None, showticklabels=True)\n",
    "fig1.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "fig2 = px.bar(data_chart,\n",
    "              x='value',\n",
    "              y='count',\n",
    "              facet_col=\"variable\",\n",
    "              facet_col_wrap=3,\n",
    "              facet_col_spacing=0.1,\n",
    "              facet_row_spacing=0.18,\n",
    "              template='none',\n",
    "              color_discrete_sequence=['grey'],\n",
    "              category_orders={'variable':data_chart['variable'].drop_duplicates().sort_values()},\n",
    "              height=700,\n",
    "              width=1000)\n",
    "fig2.update_traces(name=\"count\", legendgroup=\"count\")\n",
    "fig2.update_traces(selector={'xaxis':'x'}, showlegend=True)\n",
    "\n",
    "for i, data_i in enumerate(fig1['data']):\n",
    "    n = len(fig2['data'])+i+1\n",
    "    y = f'y{n}'\n",
    "    data_i['yaxis'] = y\n",
    "    fig2.add_traces(data_i)\n",
    "    fig2['layout'][f\"yaxis{n}\"] = {\n",
    "        'anchor': fig2['data'][i]['xaxis'],\n",
    "        'overlaying' : fig2['data'][i]['yaxis'],\n",
    "        'side':'right',\n",
    "        'tickformat':'0.1%',\n",
    "        'showgrid':False\n",
    "    }\n",
    "\n",
    "fig2.update_xaxes(matches=None, showticklabels=True, tickangle=40)\n",
    "fig2.update_yaxes(matches=None, showticklabels=True)\n",
    "Image(fig2.to_image(\"png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669fdd95",
   "metadata": {},
   "source": [
    "## Null analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36cbb9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "perc_nulls = data_target_filtered\\\n",
    ".select(\n",
    "    [(count(when(F.col(c).isNull(), c)) / count(lit(1))).alias(c) for c in data_target_filtered.columns]\n",
    ")\\\n",
    ".toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb54a611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#1F77B4",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "member_id",
          "next_pymnt_d",
          "orig_projected_additional_accrued_interest",
          "hardship_payoff_balance_amount",
          "hardship_last_payment_amount",
          "hardship_reason",
          "hardship_type",
          "hardship_status",
          "hardship_amount",
          "deferral_term",
          "hardship_end_date",
          "hardship_dpd",
          "hardship_loan_status",
          "hardship_length",
          "payment_plan_start_date",
          "hardship_start_date",
          "sec_app_mths_since_last_major_derog",
          "sec_app_revol_util",
          "revol_bal_joint",
          "sec_app_inq_last_6mths",
          "sec_app_mort_acc",
          "sec_app_chargeoff_within_12_mths",
          "sec_app_earliest_cr_line",
          "sec_app_open_act_il",
          "sec_app_fico_range_high",
          "sec_app_fico_range_low",
          "sec_app_num_rev_accts",
          "sec_app_collections_12_mths_ex_med",
          "sec_app_open_acc",
          "verification_status_joint",
          "dti_joint",
          "annual_inc_joint",
          "debt_settlement_flag_date",
          "settlement_percentage",
          "settlement_date",
          "settlement_term",
          "settlement_status",
          "settlement_amount",
          "desc",
          "mths_since_last_record",
          "mths_since_recent_bc_dlq",
          "mths_since_last_major_derog",
          "mths_since_recent_revol_delinq",
          "il_util",
          "mths_since_rcnt_il",
          "all_util",
          "open_acc_6m",
          "inq_last_12m",
          "total_cu_tl",
          "open_il_24m",
          "max_bal_bc",
          "total_bal_il",
          "open_act_il",
          "open_il_12m",
          "inq_fi",
          "open_rv_24m",
          "open_rv_12m",
          "mths_since_last_delinq",
          "mths_since_recent_inq",
          "num_tl_120dpd_2m",
          "mo_sin_old_il_acct",
          "emp_title",
          "emp_length",
          "pct_tl_nvr_dlq",
          "avg_cur_bal",
          "mo_sin_rcnt_rev_tl_op",
          "num_rev_accts",
          "mo_sin_old_rev_tl_op",
          "num_tl_30dpd",
          "num_il_tl",
          "num_tl_90g_dpd_24m",
          "mo_sin_rcnt_tl",
          "num_actv_rev_tl",
          "num_actv_bc_tl",
          "num_bc_tl",
          "total_rev_hi_lim",
          "tot_coll_amt",
          "tot_cur_bal",
          "num_rev_tl_bal_gt_0",
          "num_op_rev_tl",
          "num_tl_op_past_12m",
          "tot_hi_cred_lim",
          "num_accts_ever_120_pd",
          "total_il_high_credit_limit",
          "bc_util",
          "percent_bc_gt_75",
          "bc_open_to_buy",
          "mths_since_recent_bc",
          "num_bc_sats",
          "num_sats",
          "mort_acc",
          "total_bal_ex_mort",
          "total_bc_limit",
          "acc_open_past_24mths",
          "title",
          "last_pymnt_d",
          "pub_rec_bankruptcies",
          "revol_util",
          "dti",
          "chargeoff_within_12_mths",
          "collections_12_mths_ex_med",
          "tax_liens",
          "last_credit_pull_d",
          "inq_last_6mths",
          "delinq_amnt",
          "total_acc",
          "open_acc",
          "earliest_cr_line",
          "pub_rec",
          "delinq_2yrs",
          "acc_now_delinq",
          "annual_inc",
          "zip_code",
          "id",
          "loan_amnt",
          "funded_amnt",
          "funded_amnt_inv",
          "term",
          "int_rate",
          "purpose",
          "loan_status",
          "pymnt_plan",
          "url",
          "issue_d",
          "grade",
          "home_ownership",
          "verification_status",
          "installment",
          "sub_grade",
          "revol_bal",
          "policy_code",
          "application_type",
          "last_pymnt_amnt",
          "last_fico_range_low",
          "addr_state",
          "out_prncp",
          "initial_list_status",
          "last_fico_range_high",
          "fico_range_low",
          "fico_range_high",
          "out_prncp_inv",
          "total_pymnt_inv",
          "total_pymnt",
          "collection_recovery_fee",
          "recoveries",
          "total_rec_late_fee",
          "total_rec_int",
          "total_rec_prncp",
          "hardship_flag",
          "disbursement_method",
          "debt_settlement_flag"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAA8D+ROm9UDe/vP+V1S0Al6e8/bhKgwQXd7z9uEqDBBd3vP24SoMEF3e8/bhKgwQXd7z9uEqDBBd3vP24SoMEF3e8/bhKgwQXd7z9uEqDBBd3vP24SoMEF3e8/bhKgwQXd7z9uEqDBBd3vP24SoMEF3e8/bhKgwQXd7z+TkK2SmNfvP8tPN2i/kO8/iyTGtsKO7z8X84cowY7vPxfzhyjBju8/F/OHKMGO7z8X84cowY7vPxfzhyjBju8/F/OHKMGO7z8X84cowY7vPxfzhyjBju8/F/OHKMGO7z8X84cowY7vP2d4cCZuZO8/0nJd6TNj7z923qI+L2PvP5K++L+xNe8/kr74v7E17z+Svvi/sTXvP5K++L+xNe8/kr74v7E17z+Svvi/sTXvP2ddOs6bAu0/LINAp9uN6j9l3mzYWm3oP+81iLNSmuc/meTi6aBR5T/vhIHqC/bkPxxhfU1Wk+M/pTd1REQ94z8SLNNf8zzjPxIs01/zPOM/EizTX/M84z+e+pTR8TzjP576lNHxPOM/nvqU0fE84z+e+pTR8TzjP576lNHxPOM/nvqU0fE84z+e+pTR8TzjP576lNHxPOM/Hcboluoj4D+t6wcG+8nAP8MF1a8G0bY/LF6flRKStD/TpfymV1KwPzkdofEz1a0/CIiXpre/qj+Gin8l4rKqPzKi3nPXsKo/MqLec9ewqj8yot5z17CqP/GK+4++sKo/8Yr7j76wqj/xivuPvrCqP/GK+4++sKo/8Yr7j76wqj/xivuPvrCqP/GK+4++sKo/8Yr7j76wqj/xivuPvrCqP/GK+4++sKo/8Yr7j76wqj/xivuPvrCqP/GK+4++sKo/8Yr7j76wqj/xivuPvrCqP/GK+4++sKo/Ia/W9QGPqD+CQScqTGyoP+3UrWg9RKg/4xTMqJjqpz+OCqTfi0CmP44KpN+LQKY/rH1K5kcAoz+sfUrmRwCjP6x9SuZHAKM/rH1K5kcAoz/9oCFqnU+JP8glRq61QVw/yK9fO+OWUD+DrB6tn81FP+J3/OF2LjI/K51XQCcyHD8rnVdAJzIcP4BOE0nwahQ/M+C1QPYqBj8A7MzlpFX3PiLkEs2FjvY+IuQSzYWO9j4i5BLNhY72PiLkEs2FjvY+IuQSzYWO9j4i5BLNhY72PiLkEs2FjvY+vPtAF+PjyD68+0AX4+OoPgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "5%",
          "x": 1,
          "xanchor": "right",
          "xref": "x domain",
          "y": 0.05,
          "yanchor": "bottom",
          "yref": "y"
         },
         {
          "showarrow": false,
          "text": "40%",
          "x": 1,
          "xanchor": "right",
          "xref": "x domain",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "y"
         }
        ],
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "color": "green",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 0.05,
          "y1": 0.05,
          "yref": "y"
         },
         {
          "line": {
           "color": "red",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 0.4,
          "y1": 0.4,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Percentage of Null"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "variable"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_chart = perc_nulls.melt().sort_values(\"value\", ascending=False)\n",
    "fig = px.bar(\n",
    "    data_chart,\n",
    "    y='value',\n",
    "    x='variable',\n",
    "    template='none',\n",
    "    title=\"Percentage of Null\",\n",
    "    width=700\n",
    ")\n",
    "fig.add_hline(y=0.05, annotation={'text':\"5%\"}, line={'dash':'dash', 'color':'green'})\n",
    "fig.add_hline(y=0.4 , annotation={'text':\"40%\"}, line={'dash':'dash', 'color':'red'})\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce463f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/06 09:23:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 48:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            variable|n_values|\n",
      "+--------------------+--------+\n",
      "|         policy_code|       1|\n",
      "|          pymnt_plan|       1|\n",
      "|       hardship_flag|       1|\n",
      "|    application_type|       2|\n",
      "| initial_list_status|       2|\n",
      "|                term|       2|\n",
      "| disbursement_method|       2|\n",
      "|        default_flag|       2|\n",
      "|debt_settlement_flag|       2|\n",
      "| verification_status|       3|\n",
      "|         loan_status|       5|\n",
      "|      home_ownership|       6|\n",
      "|               grade|       7|\n",
      "|      acc_now_delinq|       8|\n",
      "|chargeoff_within_...|      11|\n",
      "|pub_rec_bankruptcies|      12|\n",
      "|             purpose|      14|\n",
      "|collections_12_mt...|      15|\n",
      "|      inq_last_6mths|      28|\n",
      "|         delinq_2yrs|      31|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def filterOutNullFeatures(df: DataFrame, threshold: float):\n",
    "    columns_selected = df\\\n",
    "        .select(\n",
    "            [(count(when(F.col(c).isNull(), c)) / count(lit(1))).alias(c) for c in df.columns]\n",
    "        )\\\n",
    "        .melt(ids=[], values=[col_i for col_i in df.columns], valueColumnName='values', variableColumnName='variable')\\\n",
    "        .where(col(\"values\")<=threshold)\\\n",
    "        .select(collect_list(\"variable\"))\\\n",
    "        .first()[0]\n",
    "    result = df.select(columns_selected)\n",
    "    return result\n",
    "\n",
    "data_nulls_excluded = data_target_filtered\\\n",
    ".transform(filterOutNullFeatures, 0.05)\n",
    "\n",
    "string_select = []\n",
    "for col in data_nulls_excluded.columns:\n",
    "    string_select.append(f\"CAST({col} AS STRING) AS {col}\")\n",
    "\n",
    "data_nulls_excluded\\\n",
    ".selectExpr(string_select)\\\n",
    ".melt(ids=[], values=[col for col in data_nulls_excluded.columns], variableColumnName=\"variable\", valueColumnName=\"value\")\\\n",
    ".drop_duplicates()\\\n",
    ".groupBy(\"variable\")\\\n",
    ".agg(count(\"value\").alias(\"n_values\"))\\\n",
    ".orderBy(\"n_values\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed164a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|pymnt_plan|  count|\n",
      "+----------+-------+\n",
      "|         n|1348099|\n",
      "+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_nulls_excluded\\\n",
    ".groupBy(\"pymnt_plan\")\\\n",
    ".agg(count(lit(1)).alias(\"count\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a5fc458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_nulls_excluded\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(f\"{output_path}accepted_2007_to2018Q2_treated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0970385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_nulls_excluded = spark.read.parquet(f\"{output_path}accepted_2007_to2018Q2_treated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfe9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_parquet(f\"{output_path}accepted_2007_to2018Q2_treated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2bb42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------+----------+-------------------+--------+-----------+----------+--------------------+------------------+--------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------------+--------------------+-------------------+--------------------------+-----------+----------------+--------------+--------------------+--------------+-------+------------------------+-----------+--------+--------------------+-----------+--------+----------------+--------------------+---------+-----------------+--------------+-------------+-------------------+--------------------+------------+\n",
      "|      id|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|home_ownership|annual_inc|verification_status| issue_d|loan_status|pymnt_plan|                 url|           purpose|               title|zip_code|addr_state|  dti|delinq_2yrs|earliest_cr_line|fico_range_low|fico_range_high|inq_last_6mths|open_acc|pub_rec|revol_bal|revol_util|total_acc|initial_list_status|out_prncp|out_prncp_inv|       total_pymnt|total_pymnt_inv|total_rec_prncp|total_rec_int|total_rec_late_fee|recoveries|collection_recovery_fee|last_pymnt_d|last_pymnt_amnt|last_credit_pull_d|last_fico_range_high|last_fico_range_low|collections_12_mths_ex_med|policy_code|application_type|acc_now_delinq|acc_open_past_24mths|bc_open_to_buy|bc_util|chargeoff_within_12_mths|delinq_amnt|mort_acc|mths_since_recent_bc|num_bc_sats|num_sats|percent_bc_gt_75|pub_rec_bankruptcies|tax_liens|total_bal_ex_mort|total_bc_limit|hardship_flag|disbursement_method|debt_settlement_flag|default_flag|\n",
      "+--------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------+----------+-------------------+--------+-----------+----------+--------------------+------------------+--------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------------+--------------------+-------------------+--------------------------+-----------+----------------+--------------+--------------------+--------------+-------+------------------------+-----------+--------+--------------------+-----------+--------+----------------+--------------------+---------+-----------------+--------------+-------------+-------------------+--------------------+------------+\n",
      "|68407277|   3600.0|     3600.0|         3600.0| 36 months|   13.99|     123.03|    C|       C4|      MORTGAGE|   55000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   190xx|        PA| 5.91|        0.0|        Aug-2003|         675.0|          679.0|           1.0|     7.0|    0.0|   2765.0|      29.7|     13.0|                  w|      0.0|          0.0| 4421.723916800001|        4421.72|         3600.0|       821.72|               0.0|       0.0|                    0.0|    Jan-2019|         122.67|          Mar-2019|               564.0|              560.0|                       0.0|        1.0|      Individual|           0.0|                 4.0|        1506.0|   37.2|                     0.0|        0.0|     1.0|                 4.0|        2.0|     7.0|             0.0|                 0.0|      0.0|           7746.0|        2400.0|            N|               Cash|                   N|           0|\n",
      "|68355089|  24700.0|    24700.0|        24700.0| 36 months|   11.99|     820.28|    C|       C1|      MORTGAGE|   65000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|    small_business|            Business|   577xx|        SD|16.06|        1.0|        Dec-1999|         715.0|          719.0|           4.0|    22.0|    0.0|  21470.0|      19.2|     38.0|                  w|      0.0|          0.0|          25679.66|       25679.66|        24700.0|       979.66|               0.0|       0.0|                    0.0|    Jun-2016|         926.35|          Mar-2019|               699.0|              695.0|                       0.0|        1.0|      Individual|           0.0|                 4.0|       57830.0|   27.1|                     0.0|        0.0|     4.0|                 2.0|       13.0|    22.0|             7.7|                 0.0|      0.0|          39475.0|       79300.0|            N|               Cash|                   N|           0|\n",
      "|68341763|  20000.0|    20000.0|        20000.0| 60 months|   10.78|     432.66|    B|       B4|      MORTGAGE|   63000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|  home_improvement|                NULL|   605xx|        IL|10.78|        0.0|        Aug-2000|         695.0|          699.0|           0.0|     6.0|    0.0|   7869.0|      56.2|     18.0|                  w|      0.0|          0.0|22705.924293878397|       22705.92|        20000.0|      2705.92|               0.0|       0.0|                    0.0|    Jun-2017|        15813.3|          Mar-2019|               704.0|              700.0|                       0.0|        1.0|       Joint App|           0.0|                 6.0|        2737.0|   55.9|                     0.0|        0.0|     5.0|               101.0|        2.0|     6.0|            50.0|                 0.0|      0.0|          18696.0|        6200.0|            N|               Cash|                   N|           0|\n",
      "|68476807|  10400.0|    10400.0|        10400.0| 60 months|   22.45|     289.91|    F|       F1|      MORTGAGE|  104433.0|    Source Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|    major_purchase|      Major purchase|   174xx|        PA|25.37|        1.0|        Jun-1998|         695.0|          699.0|           3.0|    12.0|    0.0|  21929.0|      64.5|     35.0|                  w|      0.0|          0.0|           11740.5|        11740.5|        10400.0|       1340.5|               0.0|       0.0|                    0.0|    Jul-2016|       10128.96|          Mar-2018|               704.0|              700.0|                       0.0|        1.0|      Individual|           0.0|                10.0|        4567.0|   77.5|                     0.0|        0.0|     6.0|                 4.0|        5.0|    12.0|            60.0|                 0.0|      0.0|          95768.0|       20300.0|            N|               Cash|                   N|           0|\n",
      "|68426831|  11950.0|    11950.0|        11950.0| 36 months|   13.44|     405.18|    C|       C3|          RENT|   34000.0|    Source Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   300xx|        GA| 10.2|        0.0|        Oct-1987|         690.0|          694.0|           0.0|     5.0|    0.0|   8822.0|      68.4|      6.0|                  w|      0.0|          0.0|  13708.9485297572|       13708.95|        11950.0|      1758.95|               0.0|       0.0|                    0.0|    May-2017|        7653.56|          May-2017|               759.0|              755.0|                       0.0|        1.0|      Individual|           0.0|                 0.0|         844.0|   91.0|                     0.0|        0.0|     0.0|                36.0|        2.0|     5.0|           100.0|                 0.0|      0.0|          12798.0|        9400.0|            N|               Cash|                   N|           0|\n",
      "|68476668|  20000.0|    20000.0|        20000.0| 36 months|    9.17|     637.58|    B|       B2|      MORTGAGE|  180000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   550xx|        MN|14.67|        0.0|        Jun-1990|         680.0|          684.0|           0.0|    12.0|    0.0|  87329.0|      84.5|     27.0|                  f|      0.0|          0.0|   21393.800000011|        21393.8|        20000.0|       1393.8|               0.0|       0.0|                    0.0|    Nov-2016|       15681.05|          Mar-2019|               654.0|              650.0|                       0.0|        1.0|      Individual|           0.0|                 6.0|           0.0|  102.9|                     0.0|        0.0|     4.0|                12.0|        4.0|    12.0|           100.0|                 0.0|      0.0|         116762.0|       31500.0|            N|               Cash|                   N|           0|\n",
      "|67275481|  20000.0|    20000.0|        20000.0| 36 months|    8.49|     631.26|    B|       B1|      MORTGAGE|   85000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|    major_purchase|      Major purchase|   293xx|        SC|17.61|        1.0|        Feb-1999|         705.0|          709.0|           0.0|     8.0|    0.0|    826.0|       5.7|     15.0|                  w|      0.0|          0.0|   21538.508976797|       21538.51|        20000.0|      1538.51|               0.0|       0.0|                    0.0|    Jan-2017|       14618.23|          Mar-2019|               674.0|              670.0|                       0.0|        1.0|      Individual|           0.0|                 4.0|       13674.0|    5.7|                     0.0|        0.0|     3.0|                32.0|        3.0|     8.0|             0.0|                 0.0|      0.0|          27937.0|       14500.0|            N|               Cash|                   N|           0|\n",
      "|68466926|  10000.0|    10000.0|        10000.0| 36 months|    6.49|     306.45|    A|       A2|          RENT|   85000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|       credit_card|Credit card refin...|   160xx|        PA|13.07|        0.0|        Apr-2002|         685.0|          689.0|           1.0|    14.0|    1.0|  10464.0|      34.5|     23.0|                  w|      0.0|          0.0|  10998.9715749644|       10998.97|        10000.0|       998.97|               0.0|       0.0|                    0.0|    Aug-2018|        1814.48|          Mar-2019|               719.0|              715.0|                       0.0|        1.0|      Individual|           0.0|                 7.0|        8182.0|   50.1|                     0.0|        0.0|     1.0|                 4.0|        7.0|    14.0|            28.6|                 1.0|      0.0|          27957.0|       16400.0|            N|               Cash|                   N|           0|\n",
      "|68616873|   8000.0|     8000.0|         8000.0| 36 months|   11.48|     263.74|    B|       B5|      MORTGAGE|   42000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|       credit_card|Credit card refin...|   029xx|        RI| 34.8|        0.0|        Nov-1994|         700.0|          704.0|           0.0|     8.0|    0.0|   7034.0|      39.1|     18.0|                  w|      0.0|          0.0|   8939.5805031401|        8939.58|         8000.0|       939.58|               0.0|       0.0|                    0.0|    Apr-2017|        4996.24|          Nov-2018|               679.0|              675.0|                       0.0|        1.0|      Individual|           0.0|                 5.0|        9966.0|   41.4|                     0.0|        0.0|     1.0|                50.0|        3.0|     8.0|            33.3|                 0.0|      0.0|         113782.0|       17000.0|            N|               Cash|                   N|           0|\n",
      "|68338832|   1400.0|     1400.0|         1400.0| 36 months|   12.88|       47.1|    C|       C2|      MORTGAGE|   64000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|             other|                NULL|   275xx|        NC|34.95|        0.0|        Jun-1996|         700.0|          704.0|           0.0|    17.0|    0.0|  37828.0|      67.2|     24.0|                  w|      0.0|          0.0|    1575.160697674|        1575.16|         1400.0|       175.16|               0.0|       0.0|                    0.0|    Mar-2017|         965.36|          Sep-2018|               704.0|              700.0|                       0.0|        1.0|      Individual|           0.0|                 4.0|        7940.0|   77.0|                     0.0|        0.0|     4.0|                28.0|        8.0|    17.0|            75.0|                 0.0|      0.0|          75258.0|       34500.0|            N|               Cash|                   N|           0|\n",
      "|66624733|  18000.0|    18000.0|        18000.0| 60 months|   19.48|      471.7|    E|       E2|          RENT|  150000.0|       Not Verified|Dec-2015|Charged Off|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   916xx|        CA| 9.39|        0.0|        Jun-2005|         665.0|          669.0|           1.0|    18.0|    1.0|  14052.0|      40.7|     27.0|                  w|      0.0|          0.0|           9452.74|        9452.74|        3481.86|      4351.98|               0.0|    1618.9|                291.402|    May-2017|          471.7|          Nov-2017|               584.0|              580.0|                       0.0|        1.0|      Individual|           0.0|                 7.0|        5128.0|   51.3|                     0.0|        0.0|     2.0|                 9.0|        7.0|    18.0|            14.3|                 1.0|      0.0|          36247.0|       10300.0|            N|               Cash|                   N|           1|\n",
      "|68466961|  28000.0|    28000.0|        28000.0| 36 months|    6.49|     858.05|    A|       A2|      MORTGAGE|   92000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   275xx|        NC| 21.6|        0.0|        May-1984|         720.0|          724.0|           0.0|    16.0|    0.0|  51507.0|      64.5|     24.0|                  w|      0.0|          0.0|29939.017729337003|       29939.02|        28000.0|      1939.02|               0.0|       0.0|                    0.0|    May-2017|       17093.51|          Mar-2019|               764.0|              760.0|                       0.0|        1.0|      Individual|           0.0|                 1.0|       16623.0|   59.9|                     0.0|        0.0|     2.0|                19.0|        9.0|    16.0|            22.2|                 0.0|      0.0|          74920.0|       41500.0|            N|               Cash|                   N|           0|\n",
      "|68354783|   9600.0|     9600.0|         9600.0| 36 months|    7.49|     298.58|    A|       A4|      MORTGAGE|   60000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|       credit_card|Credit card refin...|   299xx|        SC|22.44|        0.0|        Jun-1996|         695.0|          699.0|           0.0|     7.0|    0.0|   7722.0|      59.4|      9.0|                  w|      0.0|          0.0|10636.098427692801|        10636.1|         9600.0|       1036.1|               0.0|       0.0|                    0.0|    Feb-2018|        3480.17|          Aug-2018|               779.0|              775.0|                       0.0|        1.0|      Individual|           0.0|                 2.0|        4778.0|   61.8|                     0.0|        0.0|     0.0|                11.0|        3.0|     7.0|            66.7|                 0.0|      0.0|          55387.0|       12500.0|            N|               Cash|                   N|           0|\n",
      "|68466916|  25000.0|    25000.0|        25000.0| 36 months|    7.49|     777.55|    A|       A4|      MORTGAGE|  109000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   226xx|        VA|26.02|        0.0|        Dec-2001|         745.0|          749.0|           1.0|     9.0|    0.0|  20862.0|      54.3|     19.0|                  w|      0.0|          0.0|          26224.23|       26224.23|        25000.0|      1224.23|               0.0|       0.0|                    0.0|    Sep-2016|       20807.39|          Apr-2017|               724.0|              720.0|                       0.0|        1.0|      Individual|           0.0|                 2.0|       17538.0|   54.3|                     0.0|        0.0|     3.0|                13.0|        5.0|     9.0|            20.0|                 0.0|      0.0|          68056.0|       38400.0|            N|               Cash|                   N|           0|\n",
      "|68577849|  18000.0|    18000.0|        18000.0| 60 months|   11.99|     400.31|    C|       C1|      MORTGAGE|  112000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   856xx|        AZ| 8.68|        0.0|        Nov-1993|         800.0|          804.0|           0.0|    17.0|    0.0|  10711.0|      15.5|     27.0|                  w|      0.0|          0.0|          18387.22|       18387.22|        18000.0|       387.22|               0.0|       0.0|                    0.0|    Mar-2016|        18004.9|          Mar-2019|               794.0|              790.0|                       0.0|        1.0|      Individual|           0.0|                 5.0|       60336.0|   16.7|                     0.0|        0.0|     4.0|                 1.0|       12.0|    16.0|             0.0|                 0.0|      0.0|          36127.0|       69800.0|            N|               Cash|                   N|           0|\n",
      "|68495092|   8650.0|     8650.0|         8650.0| 36 months|   19.89|     320.99|    E|       E3|          RENT|   55000.0|           Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   462xx|        IN|25.49|        0.0|        Mar-2005|         675.0|          679.0|           4.0|    18.0|    1.0|   9568.0|      46.0|     19.0|                  w|      0.0|          0.0|           9190.49|        9190.49|         8650.0|       540.49|               0.0|       0.0|                    0.0|    May-2016|        8251.42|          Jun-2016|               639.0|              635.0|                       0.0|        1.0|      Individual|           0.0|                17.0|        1375.0|   45.0|                     0.0|        0.0|     0.0|                 8.0|        2.0|    18.0|            50.0|                 1.0|      0.0|          18926.0|        2750.0|            N|               Cash|                   N|           0|\n",
      "|68566886|  29900.0|    29900.0|        29900.0| 60 months|   12.88|     678.49|    C|       C2|      MORTGAGE|   65000.0|           Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   219xx|        MD|21.77|        0.0|        May-2004|         715.0|          719.0|           0.0|    15.0|    0.0|  31682.0|      46.7|     31.0|                  w|      0.0|          0.0|  34033.2141762065|       34033.21|        29900.0|      4133.21|               0.0|       0.0|                    0.0|    Mar-2017|       25234.24|          Jan-2018|               719.0|              715.0|                       0.0|        1.0|      Individual|           0.0|                 7.0|       18383.0|   45.9|                     0.0|        0.0|     2.0|                 5.0|        4.0|    15.0|            25.0|                 0.0|      0.0|          42497.0|       34000.0|            N|               Cash|                   N|           0|\n",
      "|68009401|  16000.0|    16000.0|        16000.0| 60 months|   14.85|     379.39|    C|       C5|      MORTGAGE|   48000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|       credit_card|Credit card refin...|   297xx|        SC|33.18|        0.0|        Jun-1991|         685.0|          689.0|           0.0|    11.0|    2.0|  19108.0|      29.6|     19.0|                  w|      0.0|          0.0|  19498.0921090548|       19498.09|        16000.0|      3498.09|               0.0|       0.0|                    0.0|    Sep-2017|       12322.68|          Sep-2017|               664.0|              660.0|                       0.0|        1.0|      Individual|           0.0|                 6.0|      263953.0|   17.6|                     0.0|        0.0|     2.0|                11.0|        6.0|    11.0|             0.0|                 0.0|      2.0|          31329.0|      281300.0|            N|               Cash|                   N|           0|\n",
      "|68476702|  28000.0|    28000.0|        28000.0| 36 months|   13.44|     949.38|    C|       C3|           OWN|  125000.0|    Source Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|debt_consolidation|  Debt consolidation|   100xx|        NY| 7.49|        0.0|        May-2000|         695.0|          699.0|           0.0|     6.0|    0.0|  22780.0|      63.1|     22.0|                  w|      0.0|          0.0|   33320.393323799|       33320.39|        28000.0|      5320.39|               0.0|       0.0|                    0.0|    Dec-2017|        12486.3|          Mar-2019|               744.0|              740.0|                       0.0|        1.0|      Individual|           0.0|                 1.0|       10607.0|   63.4|                     0.0|        0.0|     0.0|                22.0|        5.0|     6.0|            40.0|                 0.0|      0.0|          22780.0|       29000.0|            N|               Cash|                   N|           0|\n",
      "|68436666|   5000.0|     5000.0|         5000.0| 36 months|   13.44|     169.54|    C|       C3|          RENT|   79000.0|           Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|       credit_card|Credit card refin...|   214xx|        MD| 12.7|        0.0|        Oct-2011|         665.0|          669.0|           0.0|     3.0|    0.0|   9441.0|     101.5|      5.0|                  w|      0.0|          0.0|   6117.4531971568|        6117.45|         5000.0|      1117.45|               0.0|       0.0|                    0.0|    Jan-2019|         169.17|          Mar-2019|               659.0|              655.0|                       0.0|        1.0|      Individual|           0.0|                 1.0|           0.0|  101.4|                     0.0|        0.0|     0.0|                36.0|        1.0|     3.0|           100.0|                 0.0|      0.0|          35653.0|        4300.0|            N|               Cash|                   N|           0|\n",
      "+--------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------+----------+-------------------+--------+-----------+----------+--------------------+------------------+--------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------------+--------------------+-------------------+--------------------------+-----------+----------------+--------------+--------------------+--------------+-------+------------------------+-----------+--------+--------------------+-----------+--------+----------------+--------------------+---------+-----------------+--------------+-------------+-------------------+--------------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "data_nulls_excluded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdec7069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- loan_amnt: double (nullable = true)\n",
      " |-- funded_amnt: double (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: double (nullable = true)\n",
      " |-- delinq_2yrs: double (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- fico_range_low: double (nullable = true)\n",
      " |-- fico_range_high: double (nullable = true)\n",
      " |-- inq_last_6mths: double (nullable = true)\n",
      " |-- open_acc: double (nullable = true)\n",
      " |-- pub_rec: double (nullable = true)\n",
      " |-- revol_bal: double (nullable = true)\n",
      " |-- revol_util: double (nullable = true)\n",
      " |-- total_acc: double (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: double (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: double (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- last_fico_range_high: double (nullable = true)\n",
      " |-- last_fico_range_low: double (nullable = true)\n",
      " |-- collections_12_mths_ex_med: double (nullable = true)\n",
      " |-- policy_code: double (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- acc_now_delinq: double (nullable = true)\n",
      " |-- acc_open_past_24mths: double (nullable = true)\n",
      " |-- bc_open_to_buy: double (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: double (nullable = true)\n",
      " |-- mort_acc: double (nullable = true)\n",
      " |-- mths_since_recent_bc: double (nullable = true)\n",
      " |-- num_bc_sats: double (nullable = true)\n",
      " |-- num_sats: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: double (nullable = true)\n",
      " |-- tax_liens: double (nullable = true)\n",
      " |-- total_bal_ex_mort: double (nullable = true)\n",
      " |-- total_bc_limit: double (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- disbursement_method: string (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      " |-- default_flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_nulls_excluded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9677d3",
   "metadata": {},
   "source": [
    "## train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f860b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df = data_nulls_excluded.randomSplit([0.6, 0.4], seed=42)\n",
    "\n",
    "train_df\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(f\"{output_path}raw_train.parquet\")\n",
    "\n",
    "test_df\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(f\"{output_path}raw_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc1a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.parquet(f\"{output_path}raw_train.parquet\")\n",
    "test_df = spark.read.parquet(f\"{output_path}raw_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc95ae",
   "metadata": {},
   "source": [
    "## binning and WoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1652a",
   "metadata": {},
   "source": [
    "### Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ddd12f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeClassifierBinner:\n",
    "    def __init__(self,\n",
    "                 min_samples_split: float = 0.3,\n",
    "                 max_leaf_nodes: int = 10):\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.avg_value = None\n",
    "        self.clf = DecisionTreeClassifier(max_leaf_nodes=self.max_leaf_nodes, random_state=42)\n",
    "        self.woe_schema = \\\n",
    "                StructType(\n",
    "                            [StructField('type', StringType(), True), \n",
    "                            StructField('column', StringType(), True), \n",
    "                            StructField('splits', ArrayType(DoubleType(), True), True), \n",
    "                            StructField('min', DoubleType(), True), \n",
    "                            StructField('max', DoubleType(), True), \n",
    "                            StructField('clusters', ArrayType(StringType(), True), True), \n",
    "                            StructField('null_coalesce_col_value', StringType(), True), \n",
    "                            StructField('null_coalesce_woe', DoubleType(), True), \n",
    "                            StructField('bands', DoubleType(), True), \n",
    "                            StructField('woe', DoubleType(), True), \n",
    "                            StructField('iv', DoubleType(), True)]\n",
    "                            )\n",
    "    \n",
    "    def fit(self, data: DataFrame, col_to_bin: str, target_col: str):\n",
    "        self.col_to_bin = col_to_bin\n",
    "        self.target_col = target_col\n",
    "        if isinstance(data, DataFrame):\n",
    "            pd_data = data.select(col_to_bin, target_col).toPandas()\n",
    "            x = pd_data[[self.col_to_bin]]\n",
    "            y = pd_data[self.target_col]\n",
    "            self.clf.fit(x, y)\n",
    "            self.univariate_thresholds = self.uniVariableDecisionTreeThresholds()\n",
    "            self.calculateWoe(data)\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            pd_data = data.copy()\n",
    "            x = pd_data[[self.col_to_bin]]\n",
    "            y = pd_data[self.target_col]\n",
    "            self.clf.fit(x, y)\n",
    "            self.univariate_thresholds = self.uniVariableDecisionTreeThresholds()\n",
    "            self.calculateWoePD(pd_data)\n",
    "        return self\n",
    "\n",
    "    def plot(self):\n",
    "        tree.plot_tree(self.clf)\n",
    "    \n",
    "    def uniVariableDecisionTreeThresholds(self):\n",
    "        self.text_tree = tree.export_text(self.clf, feature_names=[self.col_to_bin])\n",
    "        result = str(self.text_tree)\n",
    "        result = result.replace(f\"{self.col_to_bin}\", \"\")\n",
    "        result = re.sub(\"class: [0-9]\", \"\", result)\n",
    "        result = re.sub(\"[^0-9., ]\", \"\", result)\n",
    "        result = re.sub(\" +\", \", \", result.strip()).split(\", \")\n",
    "        result = list(set([float(x) for x in result])) + [float(\"-inf\"), float(\"+inf\")]\n",
    "        result.sort()\n",
    "        return result\n",
    "    \n",
    "    def withBandsPD(self, df: pd.DataFrame):\n",
    "        result = df.copy()\n",
    "        if self.avg_value is None:\n",
    "            self.avg_value = float(result[f'{self.col_to_bin}'].mean())\n",
    "        result[f'{self.col_to_bin}_bands'] = pd.cut(result[self.col_to_bin].fillna(self.avg_value), self.univariate_thresholds, labels=False)\n",
    "        return result\n",
    "    \n",
    "    def withBands(self, df: DataFrame):\n",
    "        from pyspark.ml.feature import Bucketizer\n",
    "        if self.avg_value is None:\n",
    "            self.avg_value = df.agg(avg(self.col_to_bin).alias(\"avg\")).select(collect_list(\"avg\")).first()[0][0]\n",
    "        result = df.withColumn(f\"{self.col_to_bin}_temp\", coalesce(col(self.col_to_bin), lit(self.avg_value)))\n",
    "        \n",
    "        # pd.cut have a left closed interval while bucketizer have a right closed interval, this adjusts the difference\n",
    "        splits = [i + 0.000000001 for i in self.univariate_thresholds]\n",
    "\n",
    "        bct = Bucketizer(splits=splits, \n",
    "                         inputCol=f\"{self.col_to_bin}_temp\", \n",
    "                         outputCol=f\"{self.col_to_bin}_bands\")\n",
    "        result = bct.transform(result)\\\n",
    "                    .drop(f\"{self.col_to_bin}_temp\")\n",
    "        return result\n",
    "\n",
    "    def defaultRatesByGroupsPD(self, df):\n",
    "        result = df.groupby([f'{self.col_to_bin}_bands'], observed=False)[self.target_col].agg(['count', 'sum'])\n",
    "        result['total_count'] = result['count'].sum()\n",
    "        result['m_rate'] = result['sum']/result['count']\n",
    "        return result.reset_index()\n",
    "    \n",
    "    def defaultRatesByGroups(self, df: DataFrame):\n",
    "        w1 = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "        result = df\\\n",
    "            .groupBy(f'{self.col_to_bin}_bands')\\\n",
    "            .agg(count(F.col(self.target_col)).alias(\"count\"),\n",
    "                 sum(F.col(self.target_col)).alias(\"sum\"))\\\n",
    "            .withColumn(\"total_count\", sum(F.col(\"count\")).over(w1))\\\n",
    "            .withColumn(\"m_rate\", F.col(\"sum\") / F.col(\"count\"))\n",
    "        return result\n",
    "\n",
    "    def calculateWoe(self, data: DataFrame):\n",
    "        w1 = Window.orderBy('bands').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "        self.spark_woe  = data\\\n",
    "            .transform(self.withBands)\\\n",
    "            .transform(self.defaultRatesByGroups)\\\n",
    "            .withColumnRenamed(f'{self.col_to_bin}_bands','bands')\\\n",
    "            .withColumn(\"partition\", lit(1))\\\n",
    "            .withColumn(\"non_defaults\", F.col(\"count\") - F.col(\"sum\"))\\\n",
    "            .withColumn(\"total_defaults\", sum(F.col(\"sum\")).over(w1))\\\n",
    "            .withColumn(\"total_non_defaults\", sum(F.col(\"non_defaults\")).over(w1))\\\n",
    "            .withColumn(\"perc_defaults\", greatest(least(F.col(\"sum\") / F.col(\"total_defaults\"), lit(0.99999)), lit(0.00001)))\\\n",
    "            .withColumn(\"perc_non_defaults\", greatest(least(F.col(\"non_defaults\") / F.col(\"total_non_defaults\"), lit(0.99999)), lit(0.00001)))\\\n",
    "            .withColumn(\"woe\", log(F.col(\"perc_defaults\") / F.col(\"perc_non_defaults\")))\\\n",
    "            .withColumn(\"iv\", sum(-1*F.col(\"woe\") * (F.col(\"perc_non_defaults\") - F.col(\"perc_defaults\"))).over(w1))\\\n",
    "            .withColumn(\"splits\", lit(self.univariate_thresholds))\\\n",
    "            .withColumn(\"min\", expr(f\"GET(splits, CAST(bands AS INT))\"))\\\n",
    "            .withColumn(\"max\", expr(f\"GET(splits, CAST(bands + 1 AS INT))\"))\\\n",
    "            .withColumn(\"null_coalesce_col_value\", lit(self.avg_value))\\\n",
    "            .withColumn(\"null_coalesce_woe\", max(\n",
    "                                                when(\n",
    "                                                    (F.col(\"null_coalesce_col_value\") >  F.col(\"min\"))\n",
    "                                                    & (F.col(\"null_coalesce_col_value\") <=  F.col(\"max\")),\n",
    "                                                    F.col(\"woe\"))).over(w1))\\\n",
    "            .withColumn(\"column\", lit(self.col_to_bin))\\\n",
    "            .withColumn(\"type\", lit(\"numeric\"))\\\n",
    "            .withColumn(\"clusters\", lit(None))\\\n",
    "            .select(\n",
    "                \"type\",\n",
    "                \"column\",\n",
    "                \"splits\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"clusters\",\n",
    "                'null_coalesce_col_value',\n",
    "                'null_coalesce_woe',\n",
    "                'bands', \n",
    "                'woe', \n",
    "                'iv'\n",
    "            )\n",
    "        self.woe = self.spark_woe.toPandas()\n",
    "        self.woe = self.woe.sort_values(\"bands\").astype({\"bands\":\"float\"})\n",
    "        self.is_monotonic = self.woe['woe'].is_monotonic_decreasing or self.woe['woe'].is_monotonic_increasing\n",
    "        self.spark_woe = spark.createDataFrame(self.woe, schema = self.woe_schema)\n",
    "        self.iv = self.woe['iv'][0]\n",
    "\n",
    "    def calculateWoePD(self, data: pd.DataFrame):\n",
    "        grouped_data = data\\\n",
    "            .pipe(self.withBandsPD)\\\n",
    "            .pipe(self.defaultRatesByGroupsPD)\\\n",
    "            .rename({f'{self.col_to_bin}_bands':'bands'}, axis='columns')\n",
    "        grouped_data['non_defaults'] = grouped_data['count'] - grouped_data['sum']\n",
    "        total_defaults = grouped_data['sum'].sum()\n",
    "        total_non_defaults = grouped_data['non_defaults'].sum()\n",
    "        grouped_data['perc_defaults'] = np.clip(grouped_data['sum']/ total_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['perc_non_defaults'] = np.clip(grouped_data['non_defaults']/ total_non_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['woe'] = np.log(grouped_data['perc_defaults'] / grouped_data['perc_non_defaults'])\n",
    "        grouped_data['iv'] = np.sum(-grouped_data['woe'] * (grouped_data['perc_non_defaults'] - grouped_data['perc_defaults']))\n",
    "        grouped_data = grouped_data.merge(pd.DataFrame({'splits':[self.univariate_thresholds]}), how='cross') \n",
    "        grouped_data['min'] = grouped_data['bands'].apply(lambda row: self.univariate_thresholds[row])\n",
    "        grouped_data['max'] = grouped_data['bands'].apply(lambda row: self.univariate_thresholds[row+1])\n",
    "        grouped_data['null_coalesce_col_value'] = self.avg_value\n",
    "        grouped_data['null_coalesce_woe'] = pd.Series(np.where(\n",
    "                                                          (grouped_data['null_coalesce_col_value'] > grouped_data['min'])\n",
    "                                                        & (grouped_data['null_coalesce_col_value'] <= grouped_data['max']),\n",
    "                                                        grouped_data['woe'], None\n",
    "                                                        )\n",
    "                                                    ).max()\n",
    "        grouped_data['column'] = self.col_to_bin\n",
    "        grouped_data['type'] = 'numeric'\n",
    "        grouped_data['clusters'] = None\n",
    "        self.woe = grouped_data[[\n",
    "            \"type\",\n",
    "            \"column\",\n",
    "            \"splits\",\n",
    "            \"min\",\n",
    "            \"max\",\n",
    "            \"clusters\",\n",
    "            'null_coalesce_col_value',\n",
    "            'null_coalesce_woe',\n",
    "            'bands', \n",
    "            'woe', \n",
    "            'iv']]\n",
    "        self.iv = self.woe['iv'][0]\n",
    "        self.woe = self.woe.sort_values(\"bands\").astype({\"bands\":\"float\"})\n",
    "        self.is_monotonic = self.woe['woe'].is_monotonic_decreasing or self.woe['woe'].is_monotonic_increasing\n",
    "        self.spark_woe = spark.createDataFrame(self.woe, schema = self.woe_schema)\n",
    "        \n",
    "    def addWoeColumnPD(self, df: pd.DataFrame, na_values=None):\n",
    "        if self.woe is None:\n",
    "            self.woe = self.spark_woe.toPandas()\n",
    "\n",
    "        result = df.pipe(self.withBandsPD)\n",
    "        result = result.merge(\n",
    "            self.woe[['bands', 'woe']].rename({\"bands\":f'{self.col_to_bin}_bands'}, axis='columns'),\n",
    "            how = 'left',\n",
    "            on = f'{self.col_to_bin}_bands'\n",
    "        )\n",
    "        result = result.rename(columns={'woe':f'{self.col_to_bin}_woe'})\n",
    "        if na_values:\n",
    "            result[f'{self.col_to_bin}_woe'] = result[f'{self.col_to_bin}_woe'].fillna(na_values)\n",
    "        return result\n",
    "    \n",
    "    def addWoeColumn(self, df: DataFrame, na_values=None):\n",
    "        if self.spark_woe is None:\n",
    "            spark_woe = spark.createDataFrame(self.woe)\\\n",
    "                                .withColumnRenamed(\"bands\", f\"{self.col_to_bin}_bands\")\n",
    "        else:   \n",
    "            spark_woe = self.spark_woe\\\n",
    "                                .withColumnRenamed(\"bands\", f\"{self.col_to_bin}_bands\")\n",
    "            \n",
    "        avg_woe = spark_woe.select(collect_list(\"null_coalesce_woe\")).first()[0][0]\n",
    "        result = df\\\n",
    "                .alias(\"a\")\\\n",
    "                .join(broadcast(spark_woe.alias(\"b\")), \n",
    "                      on=((F.col(f\"a.{self.col_to_bin}\")  > F.col(\"b.min\"))\n",
    "                         &(F.col(f\"a.{self.col_to_bin}\") <= F.col(\"b.max\"))), \n",
    "                      how=\"left\")\\\n",
    "                .withColumn(f\"{self.col_to_bin}_woe\", coalesce(F.col(\"woe\"), lit(avg_woe)))\\\n",
    "                .drop(\"iv\", \n",
    "                      'splits', \n",
    "                      'min', \n",
    "                      'max', \n",
    "                      'clusters',\n",
    "                      'type',\n",
    "                      'woe', \n",
    "                      'column', \n",
    "                      'null_coalesce_col_value', \n",
    "                      'null_coalesce_woe')\n",
    "        return result\n",
    "    \n",
    "    def addWoeColumn2(self, df: DataFrame, na_values=None):\n",
    "        from pyspark.ml.feature import Bucketizer\n",
    "        if self.spark_woe is None:\n",
    "            spark_woe = spark.createDataFrame(self.woe)\\\n",
    "                                .withColumnRenamed(\"bands\", f\"{self.col_to_bin}_bands\")\n",
    "        else:   \n",
    "            spark_woe = self.spark_woe\\\n",
    "                                .withColumnRenamed(\"bands\", f\"{self.col_to_bin}_bands\")\n",
    "        \n",
    "        splits = spark_woe.select(collect_list(\"splits\")).first()[0][0]\n",
    "        avg_col = spark_woe.select(collect_list(\"null_coalesce_col_value\")).first()[0][0]\n",
    "\n",
    "        result = df.withColumn(f\"{self.col_to_bin}_temp\", coalesce(col(self.col_to_bin), lit(avg_col)))\n",
    "        \n",
    "        # pd.cut have a left closed interval while bucketizer have a right closed interval, this adjusts the difference\n",
    "        splits = [i + 0.000000001 for i in splits]\n",
    "\n",
    "        bct = Bucketizer(splits=splits, \n",
    "                         inputCol=f\"{self.col_to_bin}_temp\", \n",
    "                         outputCol=f\"{self.col_to_bin}_bands\")\n",
    "        \n",
    "        result = bct.transform(result)\\\n",
    "                    .drop(f\"{self.col_to_bin}_temp\")\n",
    "        \n",
    "        result = result\\\n",
    "                .join(broadcast(spark_woe), \n",
    "                      on=[f\"{self.col_to_bin}_bands\"], \n",
    "                      how=\"left\")\\\n",
    "                .drop(\"iv\", \n",
    "                      'splits', \n",
    "                      'min', \n",
    "                      'max', \n",
    "                      'clusters',\n",
    "                      'type',\n",
    "                      'column', \n",
    "                      'null_coalesce_col_value', \n",
    "                      'null_coalesce_woe')\\\n",
    "                .withColumnRenamed(\"woe\", f\"{self.col_to_bin}_woe\")\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16b546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:02:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TreeClassifierBinner at 0x776aa56bce20>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_amnt_woe = TreeClassifierBinner(max_leaf_nodes=5)\n",
    "loan_amnt_woe.fit(train_df, \"loan_amnt\", \"default_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0879740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+---------+--------+--------+-----------------------+-------------------+-----+--------------------+--------------------+\n",
      "|   type|   column|              splits|      min|     max|clusters|null_coalesce_col_value|  null_coalesce_woe|bands|                 woe|                  iv|\n",
      "+-------+---------+--------------------+---------+--------+--------+-----------------------+-------------------+-----+--------------------+--------------------+\n",
      "|numeric|loan_amnt|[-Infinity, 9012....|-Infinity|  9012.5|    NULL|     14412.163431588731|0.05079318623589163|  0.0| -0.2589324649009251|0.034774280532417455|\n",
      "|numeric|loan_amnt|[-Infinity, 9012....|   9012.5| 10012.5|    NULL|     14412.163431588731|0.05079318623589163|  1.0|-0.07557455533314127|0.034774280532417455|\n",
      "|numeric|loan_amnt|[-Infinity, 9012....|  10012.5| 15012.5|    NULL|     14412.163431588731|0.05079318623589163|  2.0| 0.05079318623589163|0.034774280532417455|\n",
      "|numeric|loan_amnt|[-Infinity, 9012....|  15012.5| 28012.5|    NULL|     14412.163431588731|0.05079318623589163|  3.0| 0.15489798948451336|0.034774280532417455|\n",
      "|numeric|loan_amnt|[-Infinity, 9012....|  28012.5|Infinity|    NULL|     14412.163431588731|0.05079318623589163|  4.0| 0.26693797950823805|0.034774280532417455|\n",
      "+-------+---------+--------------------+---------+--------+--------+-----------------------+-------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan_amnt_woe.spark_woe.orderBy(\"bands\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe0ea977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_amnt_woe.is_monotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c758f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.205574989318848 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.463113069534302 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "train_df\\\n",
    ".transform(loan_amnt_woe.addWoeColumn)\\\n",
    ".select(\"loan_amnt\", \"loan_amnt_bands\", \"loan_amnt_woe\")\\\n",
    ".toPandas()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_df\\\n",
    ".transform(loan_amnt_woe.addWoeColumn2)\\\n",
    ".select(\"loan_amnt\", \"loan_amnt_bands\", \"loan_amnt_woe\")\\\n",
    ".toPandas()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d2e93",
   "metadata": {},
   "source": [
    "### category classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e0cb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class categoryClassifierBinner:\n",
    "    def __init__(self,\n",
    "                    n_splits:  int = 10,\n",
    "                    n_samples: int = 10000,\n",
    "                    n_groups:  int = 50):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_samples = n_samples\n",
    "        self.n_groups = n_groups\n",
    "        self.woe_schema = \\\n",
    "                StructType(\n",
    "                            [StructField('type', StringType(), True), \n",
    "                            StructField('column', StringType(), True), \n",
    "                            StructField('splits', ArrayType(DoubleType(), True), True), \n",
    "                            StructField('min', DoubleType(), True), \n",
    "                            StructField('max', DoubleType(), True), \n",
    "                            StructField('clusters', ArrayType(StringType(), True), True), \n",
    "                            StructField('null_coalesce_col_value', StringType(), True), \n",
    "                            StructField('null_coalesce_woe', DoubleType(), True), \n",
    "                            StructField('bands', DoubleType(), True), \n",
    "                            StructField('woe', DoubleType(), True), \n",
    "                            StructField('iv', DoubleType(), True)]\n",
    "                            )\n",
    "\n",
    "    def fit(self, data: DataFrame, col_to_bin: str, target_col: str):\n",
    "        self.col_to_bin = col_to_bin\n",
    "        self.target_col = target_col\n",
    "        if isinstance(data, DataFrame):\n",
    "            pd_data = data.select(col_to_bin, target_col).toPandas()\n",
    "            self.applyKMeans(pd_data)\n",
    "            self.calculateWoe(data)\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            pd_data = data[[col_to_bin, target_col]]\n",
    "            self.applyKMeans(pd_data)\n",
    "            self.calculateWoePD(pd_data)\n",
    "        return self\n",
    "    \n",
    "    def applyKMeans(self, df: pd.DataFrame):\n",
    "        from sklearn.cluster import KMeans\n",
    "        sample = df\\\n",
    "        .pipe(self.withDFResampled, self.n_samples, self.n_groups)\\\n",
    "        .pipe(self.defaultRatesByGroupsPD, [self.col_to_bin, 'groups'])\n",
    "\n",
    "        k_fold_sample = pd.pivot_table(sample, values='m_rate', index=self.col_to_bin, columns='groups').reset_index().set_index(self.col_to_bin)\n",
    "        kmeans = KMeans(n_clusters=self.n_splits, random_state=42).fit(k_fold_sample)\n",
    "        k_fold_sample['k_fold_groups'] = kmeans.predict(k_fold_sample)\n",
    "        k_fold_sample = k_fold_sample.reset_index()\n",
    "        self.groups_df = spark.createDataFrame(\n",
    "            k_fold_sample[[self.col_to_bin, 'k_fold_groups']]\\\n",
    "            .rename({\"k_fold_groups\" : f\"{self.col_to_bin}_bands\"}, axis='columns')\n",
    "            )\n",
    "        self.groups_dict = k_fold_sample[[self.col_to_bin, 'k_fold_groups']].set_index(self.col_to_bin).to_dict()\n",
    "        clusters = {}\n",
    "        for group_i in k_fold_sample['k_fold_groups'].drop_duplicates():\n",
    "            clusters[group_i] = k_fold_sample[k_fold_sample['k_fold_groups']==group_i][self.col_to_bin].to_list()\n",
    "        self.clusters = pd.DataFrame([clusters]).melt(var_name=\"bands\", value_name=\"clusters\")\n",
    "    \n",
    "    def withDFResampled(self, df, n_samples, n_groups):\n",
    "        sample = df.copy()\n",
    "        sample[self.col_to_bin] = sample[self.col_to_bin].fillna(\"None\")\n",
    "        sample = sample[[self.col_to_bin, self.target_col]].groupby(self.col_to_bin).sample(n_samples, replace=True, random_state=42).reset_index(drop=True)\n",
    "        sample['count'] = sample.groupby(self.col_to_bin)[self.target_col].expanding().count().reset_index()[self.target_col]\n",
    "        sample['groups'] = pd.cut(sample['count'], n_groups, labels=False)\n",
    "        return sample\n",
    "\n",
    "    def withBandsPD(self, df: pd.DataFrame):\n",
    "        result = df.copy()\n",
    "        result[f'{self.col_to_bin}'] = result[f'{self.col_to_bin}'].fillna('None')\n",
    "        result[f'{self.col_to_bin}_bands'] = result[self.col_to_bin].map(self.groups_dict['k_fold_groups'])\n",
    "        return result\n",
    "    \n",
    "    def withBands(self, df: DataFrame):\n",
    "        result = df\\\n",
    "                .withColumn(self.col_to_bin, coalesce(col(self.col_to_bin), lit(\"None\")))\\\n",
    "                .join(broadcast(self.groups_df), [self.col_to_bin], \"left\")\n",
    "        return result\n",
    "    \n",
    "    def defaultRatesByGroupsPD(self, df: pd.DataFrame, col_list: list):\n",
    "        result = df.groupby(col_list, observed=False)[self.target_col].agg(['count', 'sum'])\n",
    "        result['total_count'] = result['count'].sum()\n",
    "        result['m_rate'] = np.clip(result['sum']/result['count'], 0.00001, None)\n",
    "        return result.reset_index()\n",
    "    \n",
    "    def defaultRatesByGroups(self, df: DataFrame, col_list: list):\n",
    "        result = df\\\n",
    "            .groupBy(col_list)\\\n",
    "            .agg(count(F.col(self.target_col)).alias(\"count\"),\n",
    "                 sum(F.col(self.target_col)).alias(\"sum\"))\\\n",
    "            .withColumn(\"m_rate\", F.col(\"sum\") / F.col(\"count\"))\n",
    "        return result\n",
    "    \n",
    "    def calculateWoe(self, data: DataFrame):\n",
    "        w1 = Window.orderBy('bands').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "        mode = data.agg(F.mode(F.col(self.col_to_bin)).alias(\"mode\")).select(collect_list(\"mode\")).first()[0][0]\n",
    "        clusters = spark.createDataFrame(self.clusters)\n",
    "        self.spark_woe  = data\\\n",
    "            .transform(self.withBands)\\\n",
    "            .transform(self.defaultRatesByGroups, [f\"{self.col_to_bin}_bands\"])\\\n",
    "            .withColumnRenamed(f'{self.col_to_bin}_bands','bands')\\\n",
    "            .withColumn(\"partition\", lit(1))\\\n",
    "            .withColumn(\"non_defaults\", F.col(\"count\") - F.col(\"sum\"))\\\n",
    "            .withColumn(\"total_defaults\", sum(F.col(\"sum\")).over(w1))\\\n",
    "            .withColumn(\"total_non_defaults\", sum(F.col(\"non_defaults\")).over(w1))\\\n",
    "            .withColumn(\"perc_defaults\", greatest(least(F.col(\"sum\") / F.col(\"total_defaults\"), lit(0.99999)), lit(0.00001)))\\\n",
    "            .withColumn(\"perc_non_defaults\", greatest(least(F.col(\"non_defaults\") / F.col(\"total_non_defaults\"), lit(0.99999)), lit(0.00001)))\\\n",
    "            .withColumn(\"woe\", log(F.col(\"perc_defaults\") / F.col(\"perc_non_defaults\")))\\\n",
    "            .withColumn(\"iv\", sum(-1*F.col(\"woe\") * (F.col(\"perc_non_defaults\") - F.col(\"perc_defaults\"))).over(w1))\\\n",
    "            .withColumn(\"splits\", lit(None))\\\n",
    "            .withColumn(\"min\", lit(None))\\\n",
    "            .withColumn(\"max\", lit(None))\\\n",
    "            .join(clusters.select(\"bands\", \"clusters\"), ['bands'], \"left\")\\\n",
    "            .withColumn(\"null_coalesce_col_value\", lit(mode))\\\n",
    "            .withColumn(\"null_coalesce_woe\", max(\n",
    "                                                when(F.array_contains(F.col(\"clusters\"), F.col(\"null_coalesce_col_value\")),\n",
    "                                                    F.col(\"woe\"))).over(w1))\\\n",
    "            .withColumn(\"column\", lit(self.col_to_bin))\\\n",
    "            .withColumn(\"type\", lit(\"category\"))\\\n",
    "            .select(\n",
    "                \"type\",\n",
    "                \"column\",\n",
    "                \"splits\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"clusters\",\n",
    "                'null_coalesce_col_value',\n",
    "                'null_coalesce_woe',\n",
    "                'bands', \n",
    "                'woe', \n",
    "                'iv'\n",
    "            )\n",
    "        self.woe = self.spark_woe.toPandas()\n",
    "        self.woe = self.woe.sort_values(\"bands\").astype({\"bands\":\"float\"})\n",
    "        self.spark_woe = spark.createDataFrame(self.woe, schema = self.woe_schema)\n",
    "        self.iv = self.woe['iv'][0]\n",
    "\n",
    "    def calculateWoePD(self, df: pd.DataFrame):\n",
    "        grouped_data = df\\\n",
    "            .pipe(self.withBandsPD)\\\n",
    "            .pipe(self.defaultRatesByGroupsPD, [f\"{self.col_to_bin}_bands\"])\\\n",
    "            .rename({f\"{self.col_to_bin}_bands\" : 'bands'}, axis='columns')\n",
    "        grouped_data['non_defaults'] = grouped_data['count'] - grouped_data['sum']\n",
    "        total_defaults = grouped_data['sum'].sum()\n",
    "        total_non_defaults = grouped_data['non_defaults'].sum()\n",
    "        grouped_data['perc_defaults'] = np.clip(grouped_data['sum']/ total_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['perc_non_defaults'] = np.clip(grouped_data['non_defaults']/ total_non_defaults, 0.00001, 0.99999)\n",
    "        grouped_data['woe'] = np.log(grouped_data['perc_defaults'] / grouped_data['perc_non_defaults'])\n",
    "        grouped_data['iv'] = np.sum(-grouped_data['woe'] * (grouped_data['perc_non_defaults'] - grouped_data['perc_defaults']))\n",
    "        grouped_data['null_coalesce_col_value'] = df[self.col_to_bin].mode()[0]\n",
    "        grouped_data = grouped_data\\\n",
    "                .merge(self.clusters,\n",
    "                        on=\"bands\",\n",
    "                        how=\"left\")\n",
    "        grouped_data['null_coalesce_woe'] =  grouped_data[['clusters', 'null_coalesce_col_value', 'woe']]\\\n",
    "                                                    .apply(lambda row: np.where(\n",
    "                                                            row['null_coalesce_col_value'] in row['clusters'],\n",
    "                                                            row['woe'], -np.inf\n",
    "                                                            ), axis=1\n",
    "                                                        ).max()\n",
    "        grouped_data['column'] = self.col_to_bin\n",
    "        grouped_data['type'] = 'category'\n",
    "        grouped_data['splits'] = None\n",
    "        grouped_data['min'] = None\n",
    "        grouped_data['max'] = None\n",
    "        self.woe = grouped_data[['type',\n",
    "                                 'column',\n",
    "                                 'splits',\n",
    "                                 'min',\n",
    "                                 'max',\n",
    "                                 'clusters', \n",
    "                                 'null_coalesce_col_value', \n",
    "                                 'null_coalesce_woe',\n",
    "                                 'bands',\n",
    "                                 'woe', \n",
    "                                 'iv']]\n",
    "        self.iv = self.woe['iv'][0]\n",
    "        self.woe = self.woe.sort_values(\"bands\").astype({\"bands\":\"float\"})\n",
    "        self.spark_woe = spark.createDataFrame(self.woe, schema = self.woe_schema)\n",
    "\n",
    "    def addWoeColumnPD(self, df, na_values=None):\n",
    "        if self.woe is None:\n",
    "            self.woe = self.spark_woe.toPandas()\n",
    "        result = df.pipe(self.withBandsPD)\n",
    "        result = result.merge(\n",
    "            self.woe[['bands', 'woe']]\\\n",
    "                .rename({\"bands\":f'{self.col_to_bin}_bands'}, axis=\"columns\"),\n",
    "            how = 'left',\n",
    "            on = f'{self.col_to_bin}_bands'\n",
    "        )\n",
    "        result = result.rename(columns={'woe':f'{self.col_to_bin}_woe'})\n",
    "        if na_values:\n",
    "            result[f'{self.col_to_bin}_woe'] = result[f'{self.col_to_bin}_woe'].fillna(na_values)\n",
    "        return result\n",
    "    \n",
    "    def addWoeColumn(self, df: DataFrame):\n",
    "        if self.spark_woe is None:\n",
    "            spark_woe = spark.createDataFrame(self.woe)\\\n",
    "                            .withColumn(self.col_to_bin, explode(F.col(\"clusters\")))\\\n",
    "                            .withColumnRenamed(\"bands\", f\"{self.col_to_bin}_bands\")\n",
    "        else:\n",
    "            spark_woe = self.spark_woe\\\n",
    "                            .withColumn(self.col_to_bin, explode(F.col(\"clusters\")))\\\n",
    "                            .withColumnRenamed(\"bands\", f\"{self.col_to_bin}_bands\")\n",
    "\n",
    "        avg_woe = spark_woe.select(collect_list(\"null_coalesce_woe\")).first()[0][0]\n",
    "\n",
    "        result = df\\\n",
    "                .withColumn(self.col_to_bin, coalesce(F.col(self.col_to_bin), lit(\"None\")))\\\n",
    "                .join(broadcast(spark_woe), [f\"{self.col_to_bin}\"], \"left\")\\\n",
    "                .withColumn(f\"{self.col_to_bin}_woe\", coalesce(F.col(\"woe\"), lit(avg_woe)))\\\n",
    "                .drop(\"iv\", 'clusters', 'min', 'max', 'splits', 'type', 'woe', 'column', 'null_coalesce_col_value', 'null_coalesce_woe')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5ebaf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/08 14:05:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.categoryClassifierBinner at 0x776a93da2290>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_grade_woe = categoryClassifierBinner()\n",
    "sub_grade_woe.fit(train_df, \"sub_grade\", \"default_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa9c9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+----+--------------------+-----------------------+--------------------+-----+--------------------+------------------+\n",
      "|    type|   column|splits| min| max|            clusters|null_coalesce_col_value|   null_coalesce_woe|bands|                 woe|                iv|\n",
      "+--------+---------+------+----+----+--------------------+-----------------------+--------------------+-----+--------------------+------------------+\n",
      "|category|sub_grade|  NULL|NULL|NULL|    [C4, C5, D1, D2]|                     C1|-0.13473159450735475|  0.0| 0.37830397593868553|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|[F2, F4, G2, G3, G4]|                     C1|-0.13473159450735475|  1.0|   1.312123349860452|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|    [B1, B2, B3, B4]|                     C1|-0.13473159450735475|  2.0| -0.5620475987247346|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|[E2, E3, E4, E5, ...|                     C1|-0.13473159450735475|  3.0|  0.9815945964983779|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|[A1, A2, A3, A4, A5]|                     C1|-0.13473159450735475|  4.0|  -1.343977625483243|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|    [D3, D4, D5, E1]|                     C1|-0.13473159450735475|  5.0|  0.6590491063967714|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|            [C2, C3]|                     C1|-0.13473159450735475|  6.0| 0.10191037894968166|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|            [F5, G1]|                     C1|-0.13473159450735475|  7.0|  1.3321603568509026|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|            [B5, C1]|                     C1|-0.13473159450735475|  8.0|-0.13473159450735475|0.4693049581933368|\n",
      "|category|sub_grade|  NULL|NULL|NULL|                [G5]|                     C1|-0.13473159450735475|  9.0|  1.5130572854219158|0.4693049581933368|\n",
      "+--------+---------+------+----+----+--------------------+-----------------------+--------------------+-----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_grade_woe.spark_woe.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "781e6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+\n",
      "|sub_grade|sub_grade_bands|       sub_grade_woe|\n",
      "+---------+---------------+--------------------+\n",
      "|       B5|            8.0|-0.13473159450735475|\n",
      "|       D2|            0.0| 0.37830397593868553|\n",
      "|       C5|            0.0| 0.37830397593868553|\n",
      "|       A1|            4.0|  -1.343977625483243|\n",
      "|       B3|            2.0| -0.5620475987247346|\n",
      "|       C3|            6.0| 0.10191037894968166|\n",
      "|       E2|            3.0|  0.9815945964983779|\n",
      "|       D3|            5.0|  0.6590491063967714|\n",
      "|       A4|            4.0|  -1.343977625483243|\n",
      "|       C2|            6.0| 0.10191037894968166|\n",
      "|       B2|            2.0| -0.5620475987247346|\n",
      "|       B2|            2.0| -0.5620475987247346|\n",
      "|       B2|            2.0| -0.5620475987247346|\n",
      "|       A3|            4.0|  -1.343977625483243|\n",
      "|       A3|            4.0|  -1.343977625483243|\n",
      "|       D1|            0.0| 0.37830397593868553|\n",
      "|       B1|            2.0| -0.5620475987247346|\n",
      "|       A3|            4.0|  -1.343977625483243|\n",
      "|       C1|            8.0|-0.13473159450735475|\n",
      "|       C2|            6.0| 0.10191037894968166|\n",
      "+---------+---------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_df\\\n",
    ".transform(sub_grade_woe.addWoeColumn)\\\n",
    ".select(\"sub_grade\", \"sub_grade_bands\", \"sub_grade_woe\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0ea9c",
   "metadata": {},
   "source": [
    "### binner selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class binnerSelector:\n",
    "    def __init__(self,\n",
    "                 iv_threshold: float = 0.02):\n",
    "        self.removed_cols = {}\n",
    "        self.selected_cols = {}\n",
    "        self.iv_threshold = iv_threshold\n",
    "    \n",
    "    def fit(self, \n",
    "            data: DataFrame,\n",
    "            cols_to_bin: list,\n",
    "            target_col: str):\n",
    "        self.cols_to_bin = cols_to_bin\n",
    "        self.target_col = target_col\n",
    "        self.runForAllCols(data)\n",
    "        self.removeFeaturesWithLowIV()\n",
    "        self.buildWoEData()\n",
    "        return self\n",
    "\n",
    "    def buildWoEData(self):\n",
    "        result = None\n",
    "        if self.selected_cols:\n",
    "            for col_i, results_dict in self.results_per_column.items():\n",
    "                if result is None:\n",
    "                    result = results_dict['best'].spark_woe\n",
    "                else:\n",
    "                    result = result.unionByName(results_dict['best'].spark_woe)\n",
    "            self.woe = result\n",
    "        else:\n",
    "            print(\"Binner Not fitted, run fit\")\n",
    "\n",
    "    def binCol(self, df, col_i, n_splits, dtype):\n",
    "        if dtype in ['float', 'int', 'double']:\n",
    "            cat_binner = TreeClassifierBinner(max_leaf_nodes=n_splits, min_samples_split=0.01)\n",
    "            cat_binner.fit(df, col_i, self.target_col)\n",
    "        else:\n",
    "            cat_binner = categoryClassifierBinner(n_splits=n_splits, n_samples=10000, n_groups=50)\n",
    "            cat_binner.fit(df, col_i, self.target_col)\n",
    "        return cat_binner\n",
    "\n",
    "    def selectCategBestBin(self, col_i, df, max_splits, dtype):\n",
    "        results = {}\n",
    "        n_split = 5\n",
    "        pd_data = df\n",
    "        if max_splits == 1:\n",
    "            self.removed_cols[col_i] = 'unique value'\n",
    "            return None\n",
    "        if max_splits < 3:\n",
    "            n_split = max_splits\n",
    "            results[max_splits] = self.binCol(pd_data, col_i, max_splits, dtype)\n",
    "            print(f\"No better adjustment found: bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "            results['best'] = results[n_split]\n",
    "            return results\n",
    "        elif n_split >= max_splits:\n",
    "            n_split = max_splits - 1\n",
    "        print(f\"starting binning for column: {col_i}\")\n",
    "        results[n_split]   = self.binCol(pd_data, col_i, n_split, dtype)\n",
    "        results[n_split+1] = self.binCol(pd_data, col_i, n_split+1, dtype)\n",
    "        results[n_split-1] = self.binCol(pd_data, col_i, n_split-1, dtype)\n",
    "        while results[n_split+1].iv > results[n_split].iv:\n",
    "            if n_split + 2 > max_splits:\n",
    "                n_split = n_split + 1\n",
    "                print(\"reached maximun splits\")\n",
    "                break\n",
    "            n_split = n_split + 1\n",
    "            results[n_split+1] = self.binCol(pd_data, col_i, n_split+1, dtype)\n",
    "            print(f\"bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "            print(f\"bins: {n_split+1}, iv: {results[n_split+1].iv}\")\n",
    "            if n_split + 3 > max_splits:\n",
    "                n_split = n_split + 1\n",
    "                print(\"reached maximun splits\")\n",
    "                break\n",
    "        while results[n_split-1].iv > results[n_split].iv:\n",
    "            if n_split -1 < 3:\n",
    "                print(\"reached minimum splits\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "                print(f\"bins: {n_split-1}, iv: {results[n_split-1].iv}\")\n",
    "                n_split = n_split - 1\n",
    "                results[n_split-1] = self.binCol(pd_data, col_i, n_split-1, dtype)\n",
    "        else:\n",
    "            print(f\"No better adjustment found: bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "            results['best'] = results[n_split]\n",
    "        return results\n",
    "    \n",
    "    def selectMonotonicBestBin(self, col_i, df, max_splits, dtype):\n",
    "        results = {}\n",
    "        n_split = 5\n",
    "        pd_data = df\n",
    "        print(f\"starting binning for column: {col_i}\")\n",
    "        if max_splits == 1:\n",
    "            self.removed_cols[col_i] = 'unique value'\n",
    "            return None\n",
    "        results[n_split]      =  self.binCol(pd_data, col_i, n_split, dtype)\n",
    "        results[n_split + 1]  =  self.binCol(pd_data, col_i, n_split + 1, dtype)\n",
    "        results[n_split - 1]  =  self.binCol(pd_data, col_i, n_split - 1, dtype)\n",
    "        while results[n_split + 1].iv > results[n_split].iv:\n",
    "            print(f\"bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "            print(f\"bins: {n_split+1}, iv: {results[n_split+1].iv}\")\n",
    "            if results[n_split+1].is_monotonic:\n",
    "                n_split = n_split + 1\n",
    "                results[n_split + 1] = self.binCol(pd_data, col_i, n_split + 1, dtype)\n",
    "            else:\n",
    "                print(f\"bins: {n_split + 1} has no monotonic behaviour\")\n",
    "                break\n",
    "        while results[n_split-1].iv > results[n_split].iv \\\n",
    "        or (results[n_split].is_monotonic == False):\n",
    "            print(f\"bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "            print(f\"bins: {n_split-1}, iv: {results[n_split-1].iv}\")\n",
    "            if n_split - 1 <=2:\n",
    "                print(\"reached minimum split\")\n",
    "                n_split = n_split -1\n",
    "                break\n",
    "            else:\n",
    "                n_split = n_split-1\n",
    "                results[n_split-1] = self.binCol(pd_data, col_i, n_split-1, dtype)\n",
    "        \n",
    "        print(f\"No better adjustment found: bins: {n_split}, iv: {results[n_split].iv}\")\n",
    "        results['best'] = results[n_split]\n",
    "        return results\n",
    "    \n",
    "    def runForAllCols(self, df):\n",
    "        self.results_per_column = {}\n",
    "        for col_i in self.cols_to_bin:\n",
    "            if isinstance(df, DataFrame):\n",
    "                max_splits = df.select(col_i).distinct().count()\n",
    "                pd_data = df.select(col_i, self.target_col).toPandas().sort_values([col_i, self.target_col])\n",
    "                dtype = df.schema[col_i].dataType.simpleString()\n",
    "            elif isinstance(df, pd.DataFrame):\n",
    "                max_splits = len(df[col_i].drop_duplicates())\n",
    "                pd_data = df.sort_values([col_i, self.target_col])\n",
    "                dtype = df.dtypes.to_dict()[col_i]\n",
    "            if dtype in ['float', 'int', 'double']:\n",
    "                result = self.selectMonotonicBestBin(col_i, pd_data, max_splits, dtype)\n",
    "                if result:\n",
    "                    self.results_per_column[col_i] = result\n",
    "            else:\n",
    "                result = self.selectCategBestBin(col_i, pd_data, max_splits, dtype)\n",
    "                if result:\n",
    "                    self.results_per_column[col_i] = result\n",
    "    \n",
    "    def removeFeaturesWithLowIV(self):\n",
    "        self.ivs = []\n",
    "        for i, col_i in enumerate(self.results_per_column):\n",
    "            self.ivs.append({'variable': col_i, 'iv': self.results_per_column[col_i]['best'].iv})\n",
    "            if self.results_per_column[col_i]['best'].iv > self.iv_threshold:\n",
    "                self.selected_cols[col_i] = self.results_per_column[col_i]['best']\n",
    "            else:\n",
    "                self.removed_cols[col_i] = 'low IV'\n",
    "        self.ivs = pd.DataFrame(self.ivs, columns=['variable', 'iv'])\n",
    "        print(f\"selected cols: {len(self.selected_cols.keys())}\")\n",
    "        print(f\"removed cols: {len(self.removed_cols.keys())}\")\n",
    "    \n",
    "    def addWoeColumn(self, df):\n",
    "        result = df\n",
    "        for col_i, cat_binner_i in self.selected_cols.items():\n",
    "            result = cat_binner_i.addWoeColumn(result)\n",
    "        return result\n",
    "    \n",
    "    def addWoeColumnPD(self, df):\n",
    "        result = df.copy()\n",
    "        for col_i, cat_binner_i in self.selected_cols.items():\n",
    "            result = cat_binner_i.addWoeColumnPD(result)\n",
    "        return result\n",
    "    \n",
    "    def pysparkCorr(self, df: DataFrame):\n",
    "        num_cols = [f.name for f in df.schema.fields if not isinstance(f.dataType, StringType)]\n",
    "        df_selected = df.select(num_cols)\n",
    "        vector_col = \"corr_features\"\n",
    "        assembler = VectorAssembler(inputCols=df_selected.columns, outputCol=vector_col)\n",
    "        df_vector = assembler.transform(df_selected).select(vector_col)\n",
    "        matrix = Correlation.corr(df_vector, vector_col).collect()[0][0]\n",
    "        corr_matrix = matrix.toArray().tolist()\n",
    "        corr_matrix_df = pd.DataFrame(data=corr_matrix, columns = num_cols, index=num_cols)\n",
    "        return corr_matrix_df\n",
    "\n",
    "    def removeHighlyCorrelatedFeatures(self, df):\n",
    "        woe_col_list = [f\"{col_i}_woe\" for col_i in self.selected_cols.keys()]\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            woe_df = self.addWoeColumnPD(df)[woe_col_list]\n",
    "            corr = woe_df.corr()\n",
    "        elif isinstance(df, DataFrame):\n",
    "            woe_df = self.addWoeColumn(df).select(woe_col_list)\n",
    "            corr =  self.pysparkCorr(woe_df)\n",
    "        self.corr = corr\n",
    "        corr = corr.reset_index().melt(id_vars='index')\n",
    "        highCorr = corr[(corr['index']!=corr['variable'])\n",
    "                        &(np.abs(corr['value']) > 0.7)]\\\n",
    "                        .reset_index(drop=True)\n",
    "        highCorr['variable'] = highCorr['variable'].str.replace(\"_woe\", \"\")\n",
    "        highCorr['index'] = highCorr['index'].str.replace(\"_woe\", \"\")\n",
    "        highCorr = highCorr.merge(\n",
    "            self.ivs,\n",
    "            how='left',\n",
    "            on='variable'\n",
    "        ).merge(\n",
    "            self.ivs,\n",
    "            how='left',\n",
    "            right_on='variable',\n",
    "            left_on='index',\n",
    "            suffixes=['_1', '_2']\n",
    "        )\n",
    "        highCorr['dropped_variables'] = np.where(highCorr['iv_1'] < highCorr['iv_2'], highCorr['variable_1'], highCorr['variable_2'])\n",
    "        self.highCorr = highCorr\n",
    "        dropped_cols = highCorr.sort_values(\"value\")[::2]['dropped_variables'].drop_duplicates()\n",
    "        for col_i in dropped_cols:\n",
    "            self.removed_cols[col_i] = \"high correl\"\n",
    "            del self.selected_cols[col_i]\n",
    "        print(f\"removed {len(dropped_cols)} cols due to high corr\")\n",
    "        print(f\"selected cols: {len(self.selected_cols)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48776b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: loan_amnt\n",
      "bins: 5, iv: 0.034774280532417455\n",
      "bins: 6, iv: 0.035871238420881205\n",
      "bins: 6 has no monotonic behaviour\n",
      "No better adjustment found: bins: 5, iv: 0.034774280532417455\n",
      "starting binning for column: sub_grade\n",
      "bins: 6, iv: 0.464520061691294\n",
      "bins: 7, iv: 0.4720355134924964\n",
      "bins: 7, iv: 0.4720355134924964\n",
      "bins: 8, iv: 0.4639101818876816\n",
      "No better adjustment found: bins: 7, iv: 0.4720355134924964\n",
      "selected cols: 2\n",
      "removed cols: 0\n"
     ]
    }
   ],
   "source": [
    "cols_to_bin = [\n",
    "    \"loan_amnt\",\n",
    "    \"sub_grade\"\n",
    "]\n",
    "\n",
    "train_pd = train_df.select(\"loan_amnt\", \"sub_grade\", \"default_flag\").toPandas()\n",
    "binner_pd = binnerSelector().fit(train_pd, \n",
    "                                 cols_to_bin = cols_to_bin, \n",
    "                                 target_col = \"default_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5b68df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+---------+--------+--------------------+-----------------------+--------------------+-----+--------------------+--------------------+\n",
      "|    type|   column|              splits|      min|     max|            clusters|null_coalesce_col_value|   null_coalesce_woe|bands|                 woe|                  iv|\n",
      "+--------+---------+--------------------+---------+--------+--------------------+-----------------------+--------------------+-----+--------------------+--------------------+\n",
      "| numeric|loan_amnt|[-Infinity, 9012....|-Infinity|  9012.5|                NULL|     14412.163431588731| 0.05079318623589163|  0.0| -0.2589324649009251|0.034774280532417455|\n",
      "| numeric|loan_amnt|[-Infinity, 9012....|   9012.5| 10012.5|                NULL|     14412.163431588731| 0.05079318623589163|  1.0|-0.07557455533314127|0.034774280532417455|\n",
      "| numeric|loan_amnt|[-Infinity, 9012....|  10012.5| 15012.5|                NULL|     14412.163431588731| 0.05079318623589163|  2.0| 0.05079318623589163|0.034774280532417455|\n",
      "| numeric|loan_amnt|[-Infinity, 9012....|  15012.5| 28012.5|                NULL|     14412.163431588731| 0.05079318623589163|  3.0| 0.15489798948451336|0.034774280532417455|\n",
      "| numeric|loan_amnt|[-Infinity, 9012....|  28012.5|Infinity|                NULL|     14412.163431588731| 0.05079318623589163|  4.0| 0.26693797950823805|0.034774280532417455|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|[C2, C3, C4, C5, D1]|                     C1|-0.20859572688986106|  0.0|   0.240086810426605|  0.4720355134924964|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|[F2, F3, F4, F5, ...|                     C1|-0.20859572688986106|  1.0|   1.299023331497523|  0.4720355134924964|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|    [A1, A2, A3, A4]|                     C1|-0.20859572688986106|  2.0| -1.5202257425547134|  0.4720355134924964|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|[E1, E2, E3, E4, ...|                     C1|-0.20859572688986106|  3.0|  0.9274807772107007|  0.4720355134924964|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|    [A5, B1, B2, B3]|                     C1|-0.20859572688986106|  4.0|  -0.706767613735917|  0.4720355134924964|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|        [B4, B5, C1]|                     C1|-0.20859572688986106|  5.0|-0.20859572688986106|  0.4720355134924964|\n",
      "|category|sub_grade|                NULL|     NULL|    NULL|    [D2, D3, D4, D5]|                     C1|-0.20859572688986106|  6.0|  0.5968131743902927|  0.4720355134924964|\n",
      "+--------+---------+--------------------+---------+--------+--------------------+-----------------------+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "binner_pd.woe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c77b1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>column</th>\n",
       "      <th>splits</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>clusters</th>\n",
       "      <th>null_coalesce_col_value</th>\n",
       "      <th>null_coalesce_woe</th>\n",
       "      <th>bands</th>\n",
       "      <th>woe</th>\n",
       "      <th>iv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>numeric</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>[-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]</td>\n",
       "      <td>-inf</td>\n",
       "      <td>9012.5</td>\n",
       "      <td>None</td>\n",
       "      <td>14412.163432</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.258932</td>\n",
       "      <td>0.034774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>numeric</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>[-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]</td>\n",
       "      <td>9012.5</td>\n",
       "      <td>10012.5</td>\n",
       "      <td>None</td>\n",
       "      <td>14412.163432</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.075575</td>\n",
       "      <td>0.034774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>numeric</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>[-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]</td>\n",
       "      <td>10012.5</td>\n",
       "      <td>15012.5</td>\n",
       "      <td>None</td>\n",
       "      <td>14412.163432</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>0.034774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>numeric</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>[-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]</td>\n",
       "      <td>15012.5</td>\n",
       "      <td>28012.5</td>\n",
       "      <td>None</td>\n",
       "      <td>14412.163432</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.154898</td>\n",
       "      <td>0.034774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>numeric</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>[-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]</td>\n",
       "      <td>28012.5</td>\n",
       "      <td>inf</td>\n",
       "      <td>None</td>\n",
       "      <td>14412.163432</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.266938</td>\n",
       "      <td>0.034774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type     column                                          splits  \\\n",
       "0  numeric  loan_amnt  [-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]   \n",
       "1  numeric  loan_amnt  [-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]   \n",
       "2  numeric  loan_amnt  [-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]   \n",
       "3  numeric  loan_amnt  [-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]   \n",
       "4  numeric  loan_amnt  [-inf, 9012.5, 10012.5, 15012.5, 28012.5, inf]   \n",
       "\n",
       "       min      max clusters  null_coalesce_col_value  null_coalesce_woe  \\\n",
       "0     -inf   9012.5     None             14412.163432           0.050793   \n",
       "1   9012.5  10012.5     None             14412.163432           0.050793   \n",
       "2  10012.5  15012.5     None             14412.163432           0.050793   \n",
       "3  15012.5  28012.5     None             14412.163432           0.050793   \n",
       "4  28012.5      inf     None             14412.163432           0.050793   \n",
       "\n",
       "   bands       woe        iv  \n",
       "0    0.0 -0.258932  0.034774  \n",
       "1    1.0 -0.075575  0.034774  \n",
       "2    2.0  0.050793  0.034774  \n",
       "3    3.0  0.154898  0.034774  \n",
       "4    4.0  0.266938  0.034774  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner_pd.results_per_column['loan_amnt']['best'].woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ead7dfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: loan_amnt\n",
      "bins: 5, iv: 0.034774280532417455\n",
      "bins: 6, iv: 0.035871238420881205\n",
      "bins: 6 has no monotonic behaviour\n",
      "No better adjustment found: bins: 5, iv: 0.034774280532417455\n",
      "starting binning for column: sub_grade\n",
      "bins: 6, iv: 0.464520061691294\n",
      "bins: 7, iv: 0.4720355134924964\n",
      "bins: 7, iv: 0.4720355134924964\n",
      "bins: 8, iv: 0.4639101818876816\n",
      "No better adjustment found: bins: 7, iv: 0.4720355134924964\n",
      "selected cols: 2\n",
      "removed cols: 0\n"
     ]
    }
   ],
   "source": [
    "cols_to_bin = [\n",
    "    \"loan_amnt\",\n",
    "    \"sub_grade\"\n",
    "]\n",
    "\n",
    "binner = binnerSelector().fit(train_df, \n",
    "                              cols_to_bin = cols_to_bin, \n",
    "                              target_col = \"default_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "22c6de9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bands                              clusters\n",
      "0      2                      [A1, A2, A3, A4]\n",
      "1      4                      [A5, B1, B2, B3]\n",
      "2      5                          [B4, B5, C1]\n",
      "3      0                  [C2, C3, C4, C5, D1]\n",
      "4      6                      [D2, D3, D4, D5]\n",
      "5      3              [E1, E2, E3, E4, E5, F1]\n",
      "6      1  [F2, F3, F4, F5, G1, G2, G3, G4, G5]\n",
      "   bands                              clusters\n",
      "0      2                      [A1, A2, A3, A4]\n",
      "1      4                      [A5, B1, B2, B3]\n",
      "2      5                          [B4, B5, C1]\n",
      "3      0                  [C2, C3, C4, C5, D1]\n",
      "4      6                      [D2, D3, D4, D5]\n",
      "5      3              [E1, E2, E3, E4, E5, F1]\n",
      "6      1  [F2, F3, F4, F5, G1, G2, G3, G4, G5]\n"
     ]
    }
   ],
   "source": [
    "print(binner_pd.results_per_column['sub_grade'][7].clusters)\n",
    "print(binner.results_per_column['sub_grade'][7].clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4fea3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 419:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 0 cols due to high corr\n",
      "selected cols: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "binner_pd.removeHighlyCorrelatedFeatures(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72405a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 0 cols due to high corr\n",
      "selected cols: 2\n"
     ]
    }
   ],
   "source": [
    "binner.removeHighlyCorrelatedFeatures(train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "270f6d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt_woe</th>\n",
       "      <th>sub_grade_woe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loan_amnt_woe</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_grade_woe</th>\n",
       "      <td>0.119967</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loan_amnt_woe  sub_grade_woe\n",
       "loan_amnt_woe       1.000000       0.119967\n",
       "sub_grade_woe       0.119967       1.000000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner_pd.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7a56907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt_woe</th>\n",
       "      <th>sub_grade_woe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loan_amnt_woe</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_grade_woe</th>\n",
       "      <td>0.119967</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loan_amnt_woe  sub_grade_woe\n",
       "loan_amnt_woe       1.000000       0.119967\n",
       "sub_grade_woe       0.119967       1.000000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "27b5462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: loan_amnt\n",
      "bins: 5, iv: 0.034774280532417455\n",
      "bins: 6, iv: 0.035871238420881205\n",
      "bins: 6 has no monotonic behaviour\n",
      "No better adjustment found: bins: 5, iv: 0.034774280532417455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: funded_amnt\n",
      "bins: 5, iv: 0.03480799081438687\n",
      "bins: 6, iv: 0.035906745113525956\n",
      "bins: 6 has no monotonic behaviour\n",
      "No better adjustment found: bins: 5, iv: 0.03480799081438687\n",
      "starting binning for column: funded_amnt_inv\n",
      "bins: 5, iv: 0.034355240759782704\n",
      "bins: 6, iv: 0.03540250432670741\n",
      "bins: 6 has no monotonic behaviour\n",
      "No better adjustment found: bins: 5, iv: 0.034355240759782704\n",
      "No better adjustment found: bins: 2, iv: 0.17535650315575307\n",
      "starting binning for column: int_rate\n",
      "bins: 5, iv: 0.4204389077978888\n",
      "bins: 6, iv: 0.4266387664168921\n",
      "bins: 6, iv: 0.4266387664168921\n",
      "bins: 7, iv: 0.4502940428313355\n",
      "bins: 7, iv: 0.4502940428313355\n",
      "bins: 8, iv: 0.45619098775983274\n",
      "bins: 8, iv: 0.45619098775983274\n",
      "bins: 9, iv: 0.45781772769774914\n",
      "bins: 9, iv: 0.45781772769774914\n",
      "bins: 10, iv: 0.45944179298738597\n",
      "bins: 10, iv: 0.45944179298738597\n",
      "bins: 11, iv: 0.46135870534467854\n",
      "bins: 11 has no monotonic behaviour\n",
      "No better adjustment found: bins: 10, iv: 0.45944179298738597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: installment\n",
      "bins: 5, iv: 0.03162033019987264\n",
      "bins: 6, iv: 0.0336209036050959\n",
      "bins: 6 has no monotonic behaviour\n",
      "bins: 5, iv: 0.03162033019987264\n",
      "bins: 4, iv: 0.03024862512906655\n",
      "bins: 4, iv: 0.03024862512906655\n",
      "bins: 3, iv: 0.02833415660576514\n",
      "No better adjustment found: bins: 3, iv: 0.02833415660576514\n",
      "starting binning for column: grade\n",
      "bins: 6, iv: 0.45794286361549646\n",
      "bins: 7, iv: 0.45822148346686353\n",
      "reached maximun splits\n",
      "No better adjustment found: bins: 7, iv: 0.45822148346686353\n",
      "starting binning for column: sub_grade\n",
      "bins: 6, iv: 0.464520061691294\n",
      "bins: 7, iv: 0.4720355134924964\n",
      "bins: 7, iv: 0.4720355134924964\n",
      "bins: 8, iv: 0.4639101818876816\n",
      "No better adjustment found: bins: 7, iv: 0.4720355134924964\n",
      "starting binning for column: home_ownership\n",
      "reached maximun splits\n",
      "No better adjustment found: bins: 6, iv: 0.03076742858949011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: annual_inc\n",
      "bins: 5, iv: 0.02771649187765858\n",
      "bins: 6, iv: 0.028287281714438686\n",
      "bins: 6, iv: 0.028287281714438686\n",
      "bins: 7, iv: 0.028608078587412107\n",
      "bins: 7, iv: 0.028608078587412107\n",
      "bins: 8, iv: 0.02886088112507889\n",
      "bins: 8, iv: 0.02886088112507889\n",
      "bins: 9, iv: 0.02909883900444218\n",
      "bins: 9, iv: 0.02909883900444218\n",
      "bins: 10, iv: 0.029186270688916393\n",
      "bins: 10, iv: 0.029186270688916393\n",
      "bins: 11, iv: 0.02930310593303591\n",
      "bins: 11 has no monotonic behaviour\n",
      "No better adjustment found: bins: 10, iv: 0.029186270688916393\n",
      "starting binning for column: verification_status\n",
      "reached maximun splits\n",
      "No better adjustment found: bins: 3, iv: 0.05348822161973471\n",
      "starting binning for column: purpose\n",
      "bins: 6, iv: 0.018441574791887183\n",
      "bins: 7, iv: 0.01845826412194573\n",
      "bins: 7, iv: 0.01845826412194573\n",
      "bins: 8, iv: 0.018540637143451572\n",
      "bins: 8, iv: 0.018540637143451572\n",
      "bins: 9, iv: 0.018585895623473857\n",
      "bins: 9, iv: 0.018585895623473857\n",
      "bins: 10, iv: 0.018625449548820158\n",
      "bins: 10, iv: 0.018625449548820158\n",
      "bins: 11, iv: 0.01864209912237946\n",
      "bins: 11, iv: 0.01864209912237946\n",
      "bins: 12, iv: 0.018645705538871692\n",
      "bins: 12, iv: 0.018645705538871692\n",
      "bins: 13, iv: 0.01870245812396806\n",
      "reached maximun splits\n",
      "No better adjustment found: bins: 13, iv: 0.01870245812396806\n",
      "starting binning for column: zip_code\n",
      "bins: 6, iv: 0.02547472236269482\n",
      "bins: 7, iv: 0.02566964618974989\n",
      "bins: 7, iv: 0.02566964618974989\n",
      "bins: 8, iv: 0.029780238301253176\n",
      "bins: 8, iv: 0.029780238301253176\n",
      "bins: 9, iv: 0.03211712281438883\n",
      "bins: 9, iv: 0.03211712281438883\n",
      "bins: 10, iv: 0.032411612885305835\n",
      "bins: 10, iv: 0.032411612885305835\n",
      "bins: 11, iv: 0.033194240999325234\n",
      "bins: 11, iv: 0.033194240999325234\n",
      "bins: 12, iv: 0.03393382337964154\n",
      "bins: 12, iv: 0.03393382337964154\n",
      "bins: 13, iv: 0.03429124716539064\n",
      "bins: 13, iv: 0.03429124716539064\n",
      "bins: 14, iv: 0.034468976953757724\n",
      "bins: 14, iv: 0.034468976953757724\n",
      "bins: 15, iv: 0.034460217448150095\n",
      "No better adjustment found: bins: 14, iv: 0.034468976953757724\n",
      "starting binning for column: addr_state\n",
      "bins: 6, iv: 0.01434307303703675\n",
      "bins: 7, iv: 0.014285766533877248\n",
      "No better adjustment found: bins: 6, iv: 0.01434307303703675\n",
      "starting binning for column: dti\n",
      "bins: 5, iv: 0.06940561755840835\n",
      "bins: 6, iv: 0.07049087920895994\n",
      "bins: 6, iv: 0.07049087920895994\n",
      "bins: 7, iv: 0.07188180223190163\n",
      "bins: 7, iv: 0.07188180223190163\n",
      "bins: 8, iv: 0.07236379248631636\n",
      "bins: 8, iv: 0.07236379248631636\n",
      "bins: 9, iv: 0.07255056572517357\n",
      "bins: 9, iv: 0.07255056572517357\n",
      "bins: 10, iv: 0.07269080237051564\n",
      "bins: 10, iv: 0.07269080237051564\n",
      "bins: 11, iv: 0.07285665185988913\n",
      "bins: 11, iv: 0.07285665185988913\n",
      "bins: 12, iv: 0.07291602872713707\n",
      "bins: 12, iv: 0.07291602872713707\n",
      "bins: 13, iv: 0.072940388366019\n",
      "bins: 13 has no monotonic behaviour\n",
      "No better adjustment found: bins: 12, iv: 0.07291602872713707\n",
      "starting binning for column: delinq_2yrs\n",
      "bins: 5, iv: 0.00263556284379449\n",
      "bins: 6, iv: 0.0026425386011337875\n",
      "bins: 6 has no monotonic behaviour\n",
      "bins: 5, iv: 0.00263556284379449\n",
      "bins: 4, iv: 0.0026271450524484857\n",
      "No better adjustment found: bins: 4, iv: 0.0026271450524484857\n",
      "starting binning for column: earliest_cr_line\n",
      "bins: 6, iv: 0.016111118767286835\n",
      "bins: 7, iv: 0.017554920680469547\n",
      "bins: 7, iv: 0.017554920680469547\n",
      "bins: 8, iv: 0.01987989439610198\n",
      "bins: 8, iv: 0.01987989439610198\n",
      "bins: 9, iv: 0.02075894695367963\n",
      "bins: 9, iv: 0.02075894695367963\n",
      "bins: 10, iv: 0.020819113453789763\n",
      "bins: 10, iv: 0.020819113453789763\n",
      "bins: 11, iv: 0.02187820295382723\n",
      "bins: 11, iv: 0.02187820295382723\n",
      "bins: 12, iv: 0.02217559908474177\n",
      "bins: 12, iv: 0.02217559908474177\n",
      "bins: 13, iv: 0.022791793163063387\n",
      "bins: 13, iv: 0.022791793163063387\n",
      "bins: 14, iv: 0.022779658818649462\n",
      "No better adjustment found: bins: 13, iv: 0.022791793163063387\n",
      "starting binning for column: fico_range_low\n",
      "bins: 5, iv: 0.11650992228251364\n",
      "bins: 6, iv: 0.12038442707369316\n",
      "bins: 6, iv: 0.12038442707369316\n",
      "bins: 7, iv: 0.12203968863158682\n",
      "bins: 7, iv: 0.12203968863158682\n",
      "bins: 8, iv: 0.12272532324816948\n",
      "bins: 8, iv: 0.12272532324816948\n",
      "bins: 9, iv: 0.12302975706075177\n",
      "bins: 9, iv: 0.12302975706075177\n",
      "bins: 10, iv: 0.12336393189639952\n",
      "bins: 10, iv: 0.12336393189639952\n",
      "bins: 11, iv: 0.12386369692359468\n",
      "bins: 11, iv: 0.12386369692359468\n",
      "bins: 12, iv: 0.1247084032735372\n",
      "bins: 12, iv: 0.1247084032735372\n",
      "bins: 13, iv: 0.12489149207469719\n",
      "bins: 13, iv: 0.12489149207469719\n",
      "bins: 14, iv: 0.12502102487060163\n",
      "bins: 14, iv: 0.12502102487060163\n",
      "bins: 15, iv: 0.12516347393849306\n",
      "bins: 15, iv: 0.12516347393849306\n",
      "bins: 16, iv: 0.12521851300411835\n",
      "bins: 16, iv: 0.12521851300411835\n",
      "bins: 17, iv: 0.12528895667413462\n",
      "bins: 17, iv: 0.12528895667413462\n",
      "bins: 18, iv: 0.1255891041379127\n",
      "bins: 18, iv: 0.1255891041379127\n",
      "bins: 19, iv: 0.12563193997384983\n",
      "bins: 19, iv: 0.12563193997384983\n",
      "bins: 20, iv: 0.12567221445544258\n",
      "bins: 20, iv: 0.12567221445544258\n",
      "bins: 21, iv: 0.1256986120735955\n",
      "bins: 21, iv: 0.1256986120735955\n",
      "bins: 22, iv: 0.1257232124026202\n",
      "bins: 22, iv: 0.1257232124026202\n",
      "bins: 23, iv: 0.12575489020027872\n",
      "No better adjustment found: bins: 23, iv: 0.12575489020027872\n",
      "starting binning for column: fico_range_high\n",
      "bins: 5, iv: 0.11650992228251364\n",
      "bins: 6, iv: 0.12038442707369316\n",
      "bins: 6, iv: 0.12038442707369316\n",
      "bins: 7, iv: 0.12203968863158682\n",
      "bins: 7, iv: 0.12203968863158682\n",
      "bins: 8, iv: 0.12272532324816948\n",
      "bins: 8, iv: 0.12272532324816948\n",
      "bins: 9, iv: 0.12302975706075177\n",
      "bins: 9, iv: 0.12302975706075177\n",
      "bins: 10, iv: 0.12336393189639952\n",
      "bins: 10, iv: 0.12336393189639952\n",
      "bins: 11, iv: 0.12386369692359468\n",
      "bins: 11, iv: 0.12386369692359468\n",
      "bins: 12, iv: 0.1247084032735372\n",
      "bins: 12, iv: 0.1247084032735372\n",
      "bins: 13, iv: 0.12489149207469719\n",
      "bins: 13, iv: 0.12489149207469719\n",
      "bins: 14, iv: 0.12502102487060163\n",
      "bins: 14, iv: 0.12502102487060163\n",
      "bins: 15, iv: 0.12516347393849306\n",
      "bins: 15, iv: 0.12516347393849306\n",
      "bins: 16, iv: 0.12521851300411835\n",
      "bins: 16, iv: 0.12521851300411835\n",
      "bins: 17, iv: 0.12528895667413462\n",
      "bins: 17, iv: 0.12528895667413462\n",
      "bins: 18, iv: 0.1255891041379127\n",
      "bins: 18, iv: 0.1255891041379127\n",
      "bins: 19, iv: 0.12563193997384983\n",
      "bins: 19, iv: 0.12563193997384983\n",
      "bins: 20, iv: 0.12567221445544258\n",
      "bins: 20, iv: 0.12567221445544258\n",
      "bins: 21, iv: 0.1256986120735955\n",
      "bins: 21, iv: 0.1256986120735955\n",
      "bins: 22, iv: 0.1257232124026202\n",
      "bins: 22, iv: 0.1257232124026202\n",
      "bins: 23, iv: 0.12575489020027872\n",
      "No better adjustment found: bins: 23, iv: 0.12575489020027872\n",
      "starting binning for column: inq_last_6mths\n",
      "bins: 5, iv: 0.027206072885114976\n",
      "bins: 6, iv: 0.02725936372551685\n",
      "bins: 6, iv: 0.02725936372551685\n",
      "bins: 7, iv: 0.027275452841754592\n",
      "bins: 7 has no monotonic behaviour\n",
      "No better adjustment found: bins: 6, iv: 0.02725936372551685\n",
      "starting binning for column: open_acc\n",
      "bins: 5, iv: 0.005012675475491314\n",
      "bins: 6, iv: 0.005164570061057741\n",
      "bins: 6 has no monotonic behaviour\n",
      "No better adjustment found: bins: 5, iv: 0.005012675475491314\n",
      "starting binning for column: pub_rec\n",
      "bins: 5, iv: 0.00658048372639834\n",
      "bins: 6, iv: 0.006599986297645022\n",
      "bins: 6 has no monotonic behaviour\n",
      "bins: 5, iv: 0.00658048372639834\n",
      "bins: 4, iv: 0.006535399144085437\n",
      "No better adjustment found: bins: 4, iv: 0.006535399144085437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting binning for column: revol_bal\n",
      "bins: 5, iv: 0.004802218668901178\n",
      "bins: 6, iv: 0.004872605307916261\n",
      "bins: 6 has no monotonic behaviour\n",
      "bins: 5, iv: 0.004802218668901178\n",
      "bins: 4, iv: 0.004385318747533952\n",
      "bins: 4, iv: 0.004385318747533952\n",
      "bins: 3, iv: 0.004055603435665715\n",
      "No better adjustment found: bins: 3, iv: 0.004055603435665715\n",
      "starting binning for column: revol_util\n",
      "bins: 5, iv: 0.023882724808317324\n",
      "bins: 6, iv: 0.024115016129633713\n",
      "bins: 6, iv: 0.024115016129633713\n",
      "bins: 7, iv: 0.024380463821418823\n",
      "bins: 7, iv: 0.024380463821418823\n",
      "bins: 8, iv: 0.024726460678946377\n",
      "bins: 8, iv: 0.024726460678946377\n",
      "bins: 9, iv: 0.02506119278922905\n",
      "bins: 9 has no monotonic behaviour\n",
      "No better adjustment found: bins: 8, iv: 0.024726460678946377\n",
      "starting binning for column: total_acc\n",
      "bins: 5, iv: 0.0021333433056417352\n",
      "bins: 6, iv: 0.002249639182916852\n",
      "bins: 6 has no monotonic behaviour\n",
      "bins: 5, iv: 0.0021333433056417352\n",
      "bins: 4, iv: 0.001979371938445959\n",
      "bins: 4, iv: 0.001979371938445959\n",
      "bins: 3, iv: 0.0018187504955549253\n",
      "reached minimum split\n",
      "No better adjustment found: bins: 2, iv: 0.0016430161254371318\n",
      "No better adjustment found: bins: 2, iv: 0.0003785756242985945\n",
      "No better adjustment found: bins: 2, iv: 0.0015932961467764184\n",
      "selected cols: 18\n",
      "removed cols: 10\n"
     ]
    }
   ],
   "source": [
    "#Columns removed to prevent leakage\n",
    "\n",
    "cols_to_bin = [\n",
    " 'loan_amnt',\n",
    " 'funded_amnt',\n",
    " 'funded_amnt_inv',\n",
    " 'term',\n",
    " 'int_rate',\n",
    " 'installment',\n",
    " 'grade',\n",
    " 'sub_grade',\n",
    " 'home_ownership',\n",
    " 'annual_inc',\n",
    " 'verification_status',\n",
    "#  'issue_d',\n",
    " 'pymnt_plan',\n",
    " 'purpose',\n",
    "#  'title',\n",
    " 'zip_code',\n",
    " 'addr_state',\n",
    " 'dti',\n",
    " 'delinq_2yrs',\n",
    " 'earliest_cr_line',\n",
    " 'fico_range_low',\n",
    " 'fico_range_high',\n",
    " 'inq_last_6mths',\n",
    " 'open_acc',\n",
    " 'pub_rec',\n",
    " 'revol_bal',\n",
    " 'revol_util',\n",
    " 'total_acc',\n",
    " 'initial_list_status',\n",
    "#  'out_prncp',\n",
    "#  'out_prncp_inv',\n",
    "#  'total_pymnt',\n",
    "#  'total_pymnt_inv',\n",
    "#  'total_rec_prncp',\n",
    "#  'total_rec_int',\n",
    "#  'total_rec_late_fee',\n",
    "#  'recoveries',\n",
    "#  'collection_recovery_fee',\n",
    "#  'last_pymnt_d',\n",
    "#  'last_pymnt_amnt',\n",
    "#  'last_credit_pull_d',\n",
    "#  'last_fico_range_high',\n",
    "#  'last_fico_range_low',\n",
    "#  'collections_12_mths_ex_med',\n",
    "#  'policy_code',\n",
    " 'application_type',\n",
    "#  'acc_now_delinq',\n",
    "#  'acc_open_past_24mths',\n",
    "#  'bc_open_to_buy',\n",
    "#  'bc_util',\n",
    "#  'chargeoff_within_12_mths',\n",
    "#  'delinq_amnt',\n",
    "#  'mort_acc',\n",
    "#  'mths_since_recent_bc',\n",
    "#  'num_bc_sats',\n",
    "#  'num_sats',\n",
    "#  'percent_bc_gt_75',\n",
    "#  'pub_rec_bankruptcies',\n",
    "#  'tax_liens',\n",
    "#  'total_bal_ex_mort',\n",
    "#  'total_bc_limit',\n",
    "#  'hardship_flag',\n",
    "#  'disbursement_method',\n",
    "#  'debt_settlement_flag'\n",
    "]\n",
    "\n",
    "binner = binnerSelector().fit(train_df, \n",
    "                              cols_to_bin = cols_to_bin, \n",
    "                              target_col = \"default_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ddbfd6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 691:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 6 cols due to high corr\n",
      "selected cols: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "binner.removeHighlyCorrelatedFeatures(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fda36192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>variable_1</th>\n",
       "      <th>value</th>\n",
       "      <th>iv_1</th>\n",
       "      <th>variable_2</th>\n",
       "      <th>iv_2</th>\n",
       "      <th>dropped_variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>installment</td>\n",
       "      <td>0.845180</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>installment</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.845180</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>installment</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>installment</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.847421</td>\n",
       "      <td>0.034774</td>\n",
       "      <td>installment</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>installment</td>\n",
       "      <td>0.847421</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.034774</td>\n",
       "      <td>installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>installment</td>\n",
       "      <td>0.847886</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.034808</td>\n",
       "      <td>installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>installment</td>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.847886</td>\n",
       "      <td>0.034808</td>\n",
       "      <td>installment</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grade</td>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.944790</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>grade</td>\n",
       "      <td>0.458221</td>\n",
       "      <td>grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>grade</td>\n",
       "      <td>0.944790</td>\n",
       "      <td>0.458221</td>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>grade</td>\n",
       "      <td>0.948082</td>\n",
       "      <td>0.458221</td>\n",
       "      <td>sub_grade</td>\n",
       "      <td>0.472036</td>\n",
       "      <td>grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grade</td>\n",
       "      <td>sub_grade</td>\n",
       "      <td>0.948082</td>\n",
       "      <td>0.472036</td>\n",
       "      <td>grade</td>\n",
       "      <td>0.458221</td>\n",
       "      <td>grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>sub_grade</td>\n",
       "      <td>0.968326</td>\n",
       "      <td>0.472036</td>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>int_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.968326</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>sub_grade</td>\n",
       "      <td>0.472036</td>\n",
       "      <td>int_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.997950</td>\n",
       "      <td>0.034774</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.997950</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.034774</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.998373</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.034808</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.998373</td>\n",
       "      <td>0.034808</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>funded_amnt_inv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.034808</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.034774</td>\n",
       "      <td>loan_amnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.034774</td>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.034808</td>\n",
       "      <td>loan_amnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fico_range_high</td>\n",
       "      <td>fico_range_low</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125755</td>\n",
       "      <td>fico_range_high</td>\n",
       "      <td>0.125755</td>\n",
       "      <td>fico_range_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fico_range_low</td>\n",
       "      <td>fico_range_high</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125755</td>\n",
       "      <td>fico_range_low</td>\n",
       "      <td>0.125755</td>\n",
       "      <td>fico_range_low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index       variable_1     value      iv_1       variable_2  \\\n",
       "13  funded_amnt_inv      installment  0.845180  0.028334  funded_amnt_inv   \n",
       "8       installment  funded_amnt_inv  0.845180  0.034355      installment   \n",
       "2       installment        loan_amnt  0.847421  0.034774      installment   \n",
       "11        loan_amnt      installment  0.847421  0.028334        loan_amnt   \n",
       "12      funded_amnt      installment  0.847886  0.028334      funded_amnt   \n",
       "5       installment      funded_amnt  0.847886  0.034808      installment   \n",
       "9             grade         int_rate  0.944790  0.459442            grade   \n",
       "14         int_rate            grade  0.944790  0.458221         int_rate   \n",
       "15        sub_grade            grade  0.948082  0.458221        sub_grade   \n",
       "17            grade        sub_grade  0.948082  0.472036            grade   \n",
       "16         int_rate        sub_grade  0.968326  0.472036         int_rate   \n",
       "10        sub_grade         int_rate  0.968326  0.459442        sub_grade   \n",
       "1   funded_amnt_inv        loan_amnt  0.997950  0.034774  funded_amnt_inv   \n",
       "6         loan_amnt  funded_amnt_inv  0.997950  0.034355        loan_amnt   \n",
       "7       funded_amnt  funded_amnt_inv  0.998373  0.034355      funded_amnt   \n",
       "4   funded_amnt_inv      funded_amnt  0.998373  0.034808  funded_amnt_inv   \n",
       "3         loan_amnt      funded_amnt  0.999623  0.034808        loan_amnt   \n",
       "0       funded_amnt        loan_amnt  0.999623  0.034774      funded_amnt   \n",
       "18  fico_range_high   fico_range_low  1.000000  0.125755  fico_range_high   \n",
       "19   fico_range_low  fico_range_high  1.000000  0.125755   fico_range_low   \n",
       "\n",
       "        iv_2 dropped_variables  \n",
       "13  0.034355       installment  \n",
       "8   0.028334       installment  \n",
       "2   0.028334       installment  \n",
       "11  0.034774       installment  \n",
       "12  0.034808       installment  \n",
       "5   0.028334       installment  \n",
       "9   0.458221             grade  \n",
       "14  0.459442             grade  \n",
       "15  0.472036             grade  \n",
       "17  0.458221             grade  \n",
       "16  0.459442          int_rate  \n",
       "10  0.472036          int_rate  \n",
       "1   0.034355   funded_amnt_inv  \n",
       "6   0.034774   funded_amnt_inv  \n",
       "7   0.034808   funded_amnt_inv  \n",
       "4   0.034355   funded_amnt_inv  \n",
       "3   0.034774         loan_amnt  \n",
       "0   0.034808         loan_amnt  \n",
       "18  0.125755   fico_range_high  \n",
       "19  0.125755    fico_range_low  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner.highCorr.sort_values(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f5e1f4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>iv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>0.472036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.459442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grade</td>\n",
       "      <td>0.458221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>term</td>\n",
       "      <td>0.175357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fico_range_low</td>\n",
       "      <td>0.125755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fico_range_high</td>\n",
       "      <td>0.125755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dti</td>\n",
       "      <td>0.072916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>0.053488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>0.034808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.034774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zip_code</td>\n",
       "      <td>0.034469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>home_ownership</td>\n",
       "      <td>0.030767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.029186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>installment</td>\n",
       "      <td>0.028334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>inq_last_6mths</td>\n",
       "      <td>0.027259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>revol_util</td>\n",
       "      <td>0.024726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>earliest_cr_line</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>purpose</td>\n",
       "      <td>0.018702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>addr_state</td>\n",
       "      <td>0.014343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pub_rec</td>\n",
       "      <td>0.006535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>open_acc</td>\n",
       "      <td>0.005013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>revol_bal</td>\n",
       "      <td>0.004056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>delinq_2yrs</td>\n",
       "      <td>0.002627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>total_acc</td>\n",
       "      <td>0.001643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>application_type</td>\n",
       "      <td>0.001593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>initial_list_status</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               variable        iv\n",
       "7             sub_grade  0.472036\n",
       "4              int_rate  0.459442\n",
       "6                 grade  0.458221\n",
       "3                  term  0.175357\n",
       "17       fico_range_low  0.125755\n",
       "18      fico_range_high  0.125755\n",
       "14                  dti  0.072916\n",
       "10  verification_status  0.053488\n",
       "1           funded_amnt  0.034808\n",
       "0             loan_amnt  0.034774\n",
       "12             zip_code  0.034469\n",
       "2       funded_amnt_inv  0.034355\n",
       "8        home_ownership  0.030767\n",
       "9            annual_inc  0.029186\n",
       "5           installment  0.028334\n",
       "19       inq_last_6mths  0.027259\n",
       "23           revol_util  0.024726\n",
       "16     earliest_cr_line  0.022792\n",
       "11              purpose  0.018702\n",
       "13           addr_state  0.014343\n",
       "21              pub_rec  0.006535\n",
       "20             open_acc  0.005013\n",
       "22            revol_bal  0.004056\n",
       "15          delinq_2yrs  0.002627\n",
       "24            total_acc  0.001643\n",
       "26     application_type  0.001593\n",
       "25  initial_list_status  0.000379"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner.ivs.sort_values(\"iv\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c6f2cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'funded_amnt': <__main__.TreeClassifierBinner at 0x776aa72d6ad0>,\n",
       " 'term': <__main__.categoryClassifierBinner at 0x776aa4371b40>,\n",
       " 'sub_grade': <__main__.categoryClassifierBinner at 0x776a9c735540>,\n",
       " 'home_ownership': <__main__.categoryClassifierBinner at 0x776a99bea380>,\n",
       " 'annual_inc': <__main__.TreeClassifierBinner at 0x776aa73bf8b0>,\n",
       " 'verification_status': <__main__.categoryClassifierBinner at 0x776aa581f580>,\n",
       " 'zip_code': <__main__.categoryClassifierBinner at 0x776aa4a7d300>,\n",
       " 'dti': <__main__.TreeClassifierBinner at 0x776a7e71a440>,\n",
       " 'earliest_cr_line': <__main__.categoryClassifierBinner at 0x776aa7e68b20>,\n",
       " 'fico_range_low': <__main__.TreeClassifierBinner at 0x776aa5814a90>,\n",
       " 'inq_last_6mths': <__main__.TreeClassifierBinner at 0x776a9f83fc40>,\n",
       " 'revol_util': <__main__.TreeClassifierBinner at 0x776a80fb4610>}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner.selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311173a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binner.woe\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(f\"{output_path}woe_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05521e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_df = spark.read.parquet(f\"{output_path}woe_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800936a",
   "metadata": {},
   "source": [
    "### WoE calculator from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd42105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class addWoeFromSavedDF:\n",
    "    def __init__(self,\n",
    "                 woe_df: DataFrame):\n",
    "        self.woe_df = woe_df\n",
    "        self.col_types = woe_df\\\n",
    "                        .select(\"column\", \"type\")\\\n",
    "                        .distinct()\\\n",
    "                        .toPandas()\\\n",
    "                        .set_index(\"column\")\\\n",
    "                        .to_dict()['type']\n",
    "        self.cols_to_bin = list(self.col_types.keys())\n",
    "    \n",
    "    def transform(self, df: DataFrame):\n",
    "        result = df\n",
    "        for col_i in self.cols_to_bin:\n",
    "            result = result.transform(self.addWoeColumn, col_i)\n",
    "        return result\n",
    "\n",
    "    def addWoeColumn(self, df: DataFrame, col_to_bin: str):\n",
    "        spark_woe = self.woe_df\\\n",
    "                    .where(F.col(\"column\") == col_to_bin)\\\n",
    "                    .withColumnRenamed(\"bands\", f\"{col_to_bin}_bands\")\n",
    "        \n",
    "        column_type = self.col_types[col_to_bin]\n",
    "        if column_type == \"numeric\":\n",
    "            result = df.transform(self.addWoeNumericColumn, spark_woe, col_to_bin)\n",
    "        else:\n",
    "            result = df.transform(self.addWoeCategoryColumn, spark_woe, col_to_bin)\n",
    "        return result\n",
    "\n",
    "    def addWoeNumericColumn(self, df: DataFrame, spark_woe: DataFrame, col_to_bin: str):  \n",
    "        avg_woe = spark_woe.select(collect_list(\"null_coalesce_woe\")).first()[0][0]\n",
    "        result = df\\\n",
    "                .alias(\"a\")\\\n",
    "                .join(broadcast(spark_woe.alias(\"b\")), \n",
    "                      on=((F.col(f\"a.{col_to_bin}\")  > F.col(\"b.min\"))\n",
    "                         &(F.col(f\"a.{col_to_bin}\") <= F.col(\"b.max\"))), \n",
    "                      how=\"left\")\\\n",
    "                .withColumn(f\"{col_to_bin}_woe\", coalesce(F.col(\"woe\"), lit(avg_woe)))\\\n",
    "                .drop(\"iv\", \n",
    "                      'splits', \n",
    "                      'min', \n",
    "                      'max', \n",
    "                      'clusters',\n",
    "                      'type',\n",
    "                      'woe', \n",
    "                      'column', \n",
    "                      'null_coalesce_col_value', \n",
    "                      'null_coalesce_woe')\n",
    "        return result\n",
    "    \n",
    "    def addWoeCategoryColumn(self, df: DataFrame, spark_woe: DataFrame, col_to_bin: str):\n",
    "        spark_woe = spark_woe\\\n",
    "                    .withColumn(col_to_bin, explode(F.col(\"clusters\")))\n",
    "\n",
    "        avg_woe = spark_woe.select(collect_list(\"null_coalesce_woe\")).first()[0][0]\n",
    "\n",
    "        result = df\\\n",
    "                .withColumn(col_to_bin, coalesce(F.col(col_to_bin), lit(\"None\")))\\\n",
    "                .join(broadcast(spark_woe), [f\"{col_to_bin}\"], \"left\")\\\n",
    "                .withColumn(f\"{col_to_bin}_woe\", coalesce(F.col(\"woe\"), lit(avg_woe)))\\\n",
    "                .drop(\"iv\", 'clusters', 'min', 'max', 'splits', 'type', 'woe', 'column', 'null_coalesce_col_value', 'null_coalesce_woe')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dece8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "woe_calculator = addWoeFromSavedDF(woe_df)\n",
    "\n",
    "train_df\\\n",
    ".transform(woe_calculator.transform)\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(f\"{output_path}train_df_woe.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b5a16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 15:16:00 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 4634953 ms exceeds timeout 120000 ms\n",
      "25/06/09 15:16:00 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "25/06/09 15:16:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:16:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:17:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:18:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:19:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:20:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:21:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 15:22:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 15:22:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:22:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:23:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 15:23:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 15:23:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:23:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:23:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:23:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:23:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 15:23:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:44 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 17:04:44 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 17:04:54 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:04:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:05:04 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:05:04 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:05:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 17:05:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/06/09 17:05:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:05:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:05:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/09 17:05:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.4:32973\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "test_df\\\n",
    ".transform(woe_calculator.transform)\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(f\"{output_path}test_df_woe.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
